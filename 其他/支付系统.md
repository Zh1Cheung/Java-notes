## 和朋友圈设计区别。

读和写的问题，[微博]()粉丝多，大V更新动态时不需去关注者写信息，因为写的代价大，而应该让关注者上线后自行去读信息；对于微信朋友圈，因为好友上限少，发朋友圈时可以直接在他的所有好友的朋友圈进行写操作。



## 微信朋友圈的设计分析

https://www.jianshu.com/p/977404888afd



## 朋友圈数据库表具体实现

https://blog.csdn.net/u011035407/article/details/78592787

## 微信朋友圈数据库设计

微信的服务器端规模肯定很庞大,一个简单的lock逻辑可能都无法实现. 所以微信的功能绝大部分功能都是在客户端实现的.

   据我所知,微信的通讯协议是非常古老的邮箱通讯协议.服务器接收到消息,直接存磁盘.磁盘写满了,就会覆盖旧数据,这个写满的周期,就是微信的"保鲜期".消息写前[或写后],就会把消息ID写到待处理消息列表中.消息中应该是明确包含了需要分发到的微信账户,各个账户就会在自己的消息列表中新增一条未读的记录.

  微信的客户端和邮件系统一样进行了数据缓存. 在群里面[和好友聊天也当作是一个群]聊天,我们看到的数据都是后台缓存到本地,然后再显示出来的.

  微信的消息推送系统,是它把邮箱系统应用到实时聊天工具中的关键.它的前辈QQ是在TCP , UDP之类的通讯协议上实现的,自定义二进制协议,坑巨多,网络各种不稳定,而邮箱通讯协议+异步消息同步+长连接实时通知系统构建的微信,在可靠性,稳定性,吞吐量等各方面,都有很强的优势.

  其背后的逻辑并不复杂,客户端在需要的时候[每10分钟,或用户的登录,点击,唤醒等动作,或消息通知等]自动去服务器取上述的 "未读消息列表",如果有未读的消息,那么,首先下载所有的未读消息 ID,然后依照ID,去服务器下载消息内容缓存到本地.缓存一般会按照消息所在的群预先分组.打开一个群后,直接渲染文件内容.

   上面说的是微信的主框架,那么微信消息在我们看来是不肯能10分钟才看到消息的.微信的信息几乎就是"实时"的,这个是怎么回事呢? 这个就需要"长连接"技术了. 这个技术是微信实时性的重要支撑,也是微信和电信公司打官司的重要原因.这个技术出来很多年了,在微信出来前,我们在项目中也都用过,总觉得这个技术太重,太消耗资源,没想过像微信这样的系统,这么大规模地使用,当然首先大规模使用这个技术不是微信,而是其他公司,但现在的实时通讯系统,同时支持PC,移动应用的通知系统,几乎都是使用长连接技术了.

​    微信客户端会一直维持一个socket连接,如果有人发消息,该socket会立刻返回数据,客户端就收到了通知.这里和传统的socket通讯不同,长连接是客户端发送请求,在一定时间内有返回就立刻返回,没有返回会由服务器端在一定时间后断开连接;客户端收到断开消息后又立刻发起连接请求.而传统的通知,往往是接收消息的一方建立socket侦听,由推送方发送连接请求,连接成功后,再发送消息.从理论上讲,长连接可以比传统的socket通讯更快.客户端收到消息后,立刻执行前一段中描述的消息同步工作,最终显示消息内容给用户.

  最后再说说朋友圈. 微信刚开始是没有朋友圈的. 微信推出时,微博如日中天,从"微信"的名称来看,微信的朋友圈很可能就是对付微博的社交化尝试,当然最后结果是朋友圈很成功.朋友圈的的社交化逻辑,与微信的主框架有很大不同,简单而言,它就是一套独立的社交系统,微信提供了一个接口能够访问这套系统而已.比起微博的功能,微信的朋友圈功能其实很弱.但它太适合移动化的应用环境了.



====================================================



我谈一下我想到的 nosql 数据存储方式：

  1： 朋友圈里每个账户都已一个 “朋友圈ID”,该ID对应服务器端的一个 无限长 key-list 列表，这个key就是“朋友圈ID”.,  list逻辑上是长度无限制，但它应该是一段一段分配的。这里会存在一个列数据库，一个“朋友圈ID”，对应列数据库上的一行数据。 至于“逻辑上长度无限制”，可以有好几种实现方法，这个可自行选择，效果差别也不大。



 2： 朋友圈内容存储服务。 这个应该包括多部分： 文件存储服务【支持CDN加速】，短小消息存储服务。 其核心是短小消息服务，文件存储的地址最后都需要在这个服务中使用。 每条消息需要有一个ID来标识，这个ID可以简单实用 服务器ID+文件偏移量 来简单组合生成，这个ID可以叫做 “消息内容ID”；    
“消息内容ID” 在微信是 简单按照“联系人列表”，发送给各个联系人。使用的是推送方式，推送到上述第一个列数据库。【是按时间排序，而拉消息，在局部会明显按照用户分组。】

3：评论系统。 楼主的问题主要就是针对这个“评论内容分发”的问题。评论服务于第2条的"短小消息存储服务"是类似的。他们应该是同一个服务。其差别就在于有没有一个“发送目标”。考虑发送一个消息包含：　ContentID,SourceUserID,TargetContentID,TargetUserID。 在第2部分 的 “发送消息到朋友圈”，不会有“TargetContentID”，说明这个消息是开放的，会通知到所有联系人。 但在发表评论时，就会有明确的评论内容目标，以及发表该内容的用户，这时候，内容分发就是推送到 目标用户ID的联系人，和发送者的联系人，两个集合的交集。这个和刚好和微博的无限制刚好相反。



从设计上看，就一个简单的列数据库，和一个基于文件偏移量访问的日志系统，就可以支持起微信的朋友圈。 这种系统的难度，不是在业务逻辑上而是在于大规模分布式服务的管理上，还有安全性处理，CDN或其他加速技术的处理问题，等等。



## 支付系统中，有哪些技术问题可能会引发资金损失

- 资金损失
  - 上游商户系统，下游银行或第三方支付，并发场景下重复支付。
- 程序逻辑错误导致资金损失
  - 网络异常
    - 接口网络异常，会走exception的catch处理，但是默认的catch一般会走失败分支，但是银行的最终状态可能是成功、失败或者处理中，如果这时银行是最终成功了，但商户侧却被告知是失败的话，商户侧就有可能再次发起重试，造成重复出款。
    - 系统间的调用异常、网络异常通常包括了connection restart、connection refused、connection timeout等、代码遇到异常处理， 把订单设置为处理中状态，然后等待定时查询或者通知来返回最终状态即可。
  - 查询和通知问题
    - 支付接口一般包括了交易接口、主动查询接口和异步通知接口
      - 查询接口一般用来查询非实时返回最终状态的订单结果、二是用于核对查询。
    - 对于查询类请求和通知类请求的报文解析常见问题
      - 查询失败或者查询异常
        - 并不代表这笔订单最终是失败的，特别是第三方查询交易接口和返回的响应码状态就是失败，其实这时候开发人员一定要明确这是订单查询操作本身失败，并不是交易本身失败，如果以为是订单失败，有可能引发重复付款风险。
      - 查询频率过快
        - 系统交易请求之后，比如说快速的15s内发起主动查询，结果第三方系统返回无此订单，同时支付平台返回也无此清单给商户，这时候导致商户系统又发生了第二次的支付请求。这种情况的原因是第三方系统处理请求的链路比较长，只是说不允许那么快发起交易，建议在两分钟之后再来查询。
      - 被查询接口幂等性问题
        - T日的订单已经有了最终状态，因为某种特殊的业务场景，运营人员手工操作，把这笔订单从最终状态修改成了处理中，实际情况是银行系统T+1日返回了与T日不一样的订单状态，结果导致这个商户接受这个错误状态之后又发起了重新支付，这个问题的本质原因不一定说是银行接口幂等性的问题，是因为银行在跨日之后，这笔交易发生了撤回失败，返回的状态是想告诉大家这是撤回失败的状态。但是在运营侧来看银行的接口应该每次查询都返回同样的状态才对。建议多方案：保证我们系统的健壮性/
      - 上游或者下游重复通知的问题，或者前后两次通知不一致的问题
        - 如果第一次通知的结果是准确的 ，一定以第一次为准，如果第二次通知发生结果不一致的情况， 还是以第一次为准。那这个状态进行预警，人为干预。否则的话就容易发生重复发起的情况。
      - 接口幂等性问题
        - 只要做到了幂等，才可进行重试，后端系统对于处理重复提交的情况常用的办法是上游订单流水号作为唯一索引做幂等，比如说遇到数据库重复的抛出异常，再去查询这个支付结果，除此之外还可以在请求的入口处用redis做防重
      - 状态同步问题
        - 支付系统依赖于下游系统，支付订单的最终状态也完全依赖于下游系统。但是每家系统返回的报文都有差异，特别是在特别复杂的响应吗和相应信息。根据订单解析响应码是一件非常重要的事情。
        - 最佳实践
          - 对于查询接口返回订单不存在的情况，需要单独设置响应码，做特殊报警处理。付款类的交易不可以设置为失败状态。 这样才可以避免资金的重复支付。
          - 资金类的交易订单设置的状态是根据第三方响应码来设置的，订单的状态需要采用保守的策略，对于不确定的状态不可以直接就认为他是失败，这样也可以避免资金的重复支付。
      - 服务器重启有可能重复支付
      - 重复提交问题
        - 并发导致的重复支付
          - 多线程，集群环境，定时器、手工操作
        - 表单重复提交
          - 用户连续两次提交，或者网速过慢又点击了一次提交
        - 定时器重复执行
          - 防重：控制定时任务的执行状态，或者通过状态机来控制任务所处理的订单状态
        - 重试机制导致的重复执行
          - dubbo、nginx、mq
          - 后端防重：数据库乐观锁和有限状态机再加白名单
            - 表示有限个状态在这些状态之间的转移和动作等行为的数学模型，重点是状态的管理和状态的驱动
            - 在支付系统中状态的流转的控制可以避免订单被错误地执行或重复执行，比如成功的状态不允许被变更为处理中，比如已发送的订单状态不允许被再捡起支付等。
            - 具体的解决方案：如果当前的订单处于路由成功的状态，我们为了避免定时和人工并发执行，只需要数据库乐观锁来控制并发，白名单来控制状态机的流转。必须是白名单机制，白名单告诉我们只有15的状态才被允许更新为63或者60，否则更新失败则预警。





## 总体业务架构

第二部分介绍现在正在**运行的总体架构设计，包括总体业务架构设计，总体系统架构设计，总体技术架构设计，关键子域的架构设计；**

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/71f5d508acc64ebeba8bd7e5a0d88bff.jpeg)

**首先是总体业务架构设计**，主要包括4个部分：第1部分是我们服务的渠道和场景，包括SDK,WAP,PC各端，线上线下门店；以及电商体系的应用，金融APP的应用等；第2是我们的合作方银行：包括中农工建交等以前的直连银行，以后后来的网联，银联等；第3是贯穿整个全流程的风险控制体系与运营支撑体系，包括欺诈风险，信用风险，以及配置产品，运营的各种支撑系统；第4是**云支付平台本身**。**在云支付平台中包含三大子架构域**：一是开放平台，把我们内部的服务统一开放出去给各个渠道、各个服务去使用；二是对这个平台进行层层抽象，将c端业务平台当中线上线下的公用逻辑抽取到c端业务平台，b端业务平台当中各个行业支付的公用逻辑，我们会抽取到b端公用平台。第3作为支付核心，会统一整合内外部支付工具以及账户核心操作指令统一提供给上层使用

## 总体系统架构

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/dbf96c1bfd124650b14289d12fc62517.jpeg)

业务架构决定了系统架构，**从纵向来看**，我们分为应用层、数据层、技术服务层、基础设施层，以及贯穿整个全流程的决策支持与运营支撑层。

**从横向来看**，分成面向用户和商户的服务交付层（通过开放平台交付给我们的合作伙伴，通过这个平台前置服务于苏宁易购生态圈的各个应用场景）；应用服务层（包括业务处理类、管理类、数据类）；通用服务层（即平时常见的支付收银台、风控、合同计费等）；核心服务层（包括会员、账务核心、清结算等）；网关服务层（因为我们需要集成外部的一些服务，包括金融服务，通过金融交换服务去做；沟通网关，面向运营商的；业务网关，面向和我们合作的商户的）；

**整体架构的设计，我们采用了插件式的架构设计思想**，比如服务交付层，我们基于标准的平台业务进行服务交付，这样可以使各应用域独立并行的研发；对网关服务层，我们基于标准的外部服务引入，使平台具有快速可扩展性。

## 总体技术架构

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/6ecb4e338bd34cb89704ce1bef34a6b5.jpeg)

业务架构和系统架构决定了我们的技术架构，技术架构包括三大部分：

**持续交付层**，以及支撑我们持续交付的**中间件层**以及**基础设施部分**：持续交付重点有两个，**第一是快：**开发快，所以我们有开发的插件、模板生成工具；测试快，从自动化测试到持续集成，到一键建站的统一拉起；发布快，有现成的发布流程支持；**业务验收快，这个是我们支付平台独有的一项，上线之后要做业务匹配分析和还原**，这个有两点好处：（1）对业务来说，可以快速地知道需求有没有按照业务预期的开发；（2）对研发人员来说，可以快速地知道研发的需求是否获得了业务收益。这样，业务和研发人员有了统一的视图。

**第二是稳：**开发的过程会做这一些非功能性的设计，如：伸缩型设计，监控设计，资损防控设计；资损防控设计有三层：第1层是开发与测试，第2次是监控与核对，第3层是止损与追款。平稳，发布时支持灰度发布和预案回滚。比如做升级，收单要从单库切到多库，不能一刀切，需要按业务场景、用户、访问链路灰度，可以保证整个系统的平稳；如果一个系统上线后，如果出问题能马上实现回滚，对用户没有影响；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/6f33918e595a48fb90d750c78c7eb2e8.jpeg)

接下来讲关键子域架构设计，从收银台到交易，从支付服务到支付引擎，我们如何实现标准化和插件化。首先收银台分为三层：第1层是业务产品层；第2层是业务接入层，会做一些异常的适配，如不同的errorCode可以展示不同的异常界面，对用户体验比较好。第3是核心层，用户偏好与习惯、额度控制等。收银台是从一个简单的收银门面，千人一面，逐渐发展到千人千面，再到一人千面；就是不同的用户在不同场景下看到的收银台是不一样的，到同一个用户在不同场景下也可以进行个性化的适配；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/9a6f1ce0c84d4362b36fc92092cad3d4.jpeg)

这个是收单系统和交易系统的介绍：分成两部分，一个是公用的部分，就是我通用的上下层的依赖，比如支付引擎、会员、收银台、收费计费等，中间的收单服务层可以通过设计模式去封装，根据不同的业务请求，然后统一做收单路由到不同的插件去处理。这样即使有几十条产品线同时请求，我们也能同时响应，因为只需要改动一个插件即可；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/f694678d0c4142078673922e491ad0f3.jpeg)

基于这种插件式架构的设计思想，接下来的支付服务也是这样设计的。

支付引擎会整合银行相关的**外部支付工具**，以及零钱宝、任性付、信用支付、现金贷等**内部支付工具**，同时进行**账户操作指令的封装**（主账务核心和各个业务的微账务核心）。对我们来说，账务核心、支付工具群和支付外转接中心都是一个个插件，开发速度快；**在具体的一个支付工具内部，也是插件式的**；这样就完全可以实现大规模的并行研发；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/06fb4610a6c4470ba32875b9f1b41159.jpeg)

最后是网关层，面向接入银行渠道和接入合作大商户：第1层是报文组装解析层，第2层是适配器层，第3层是路由引擎；由图可见，每家银行的公用逻辑相同，可以通过设计模式封装。不同的是**输入参数的获取策略**以及**输出参数的不同阐释策略**。具体实践时代码结构上可以用设计模式来封装；实现代码上每个银行的输入报文的不同，可以用velocity模板来做。在返回报文时，每个银行的错误码和异常处理机制也不一样，可以通过groovy脚本来解析，这样对于接入新银行和商户不用做系统发布，直接配置即可，实现插件化可配置；2015年前公司一个月接一家银行，2016年后可以实现一个月接几十家。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/ba112a0742c0416dbe56b333b7da223f.jpeg)

基于前面的整体架构，我们思考，如果每个系统中的模块、调用链都能可视化，不管新的优化、重构、还是做业务需求都能快速实现。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/0bae1b5acc3e4b319d23b8eb21d1a74e.jpeg)

基于这个理念，我们做了一个可视化作战指挥系统。包括三大部分： **研发的可视化**： 聚焦统一目标下的交付全链路、全资源可视化；统一目标是指公司的战略目标，从上图可见，战略目标KPI一定极简指标，要定北极星指标，一般我们会定三项，战略目标分解到事业部，事业部分解到研发中心对应具体需求，而需求的整个研发周期已经可视化了，可以清楚知道每一个需求、每天做的事情是不是帮助整个集团在完成战略目标。

**运行的可视化**：系统上线之后可以看到从机房，到整个调用链，到每个架构域，再到每个具体的系统；以至系统里面的每个模块，都能清楚他们的状态。

**管控的可视化：**组件自治，资源弹性调度；每逢大促尤其是洪峰时候，需要执行应急预案，我们就需要知道，执行应急预案之后影响的用户场景，以及各个硬件执行过程当中的操作的步骤。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/27c44772855d4fe49baacce248f223f9.jpeg)

可视化作战系统架构设计，这里面除了平时的一些常用的技术设计之外，还有三点核心的设计思想：第1点，对研发来说，体现在可视化整个研发生命周期，但是起点与平常不同，**平常的起点可能就是一个需求，但这个的起点是战略。**第2点，**运行时关注不稳定性的因素，去主动分析、依赖分析和变更感知**。分析变更感知的前提，是需要对每个系统做SLA；

第3点，为了进行平滑的管控，需要做几个工作：制定应急预案后进行线下，线上的演练，除了线下测试环境的演练外，生产环境也需要实际的演练，比如划拨一定量的生产用户，调度一定量的业务场景，也会**自动注入一些故障，尽量让演练流量的结构构成接近真实流量，以保证演练的真实性**。如果故障没有经过演练，真实发生是不可控的；**故障能否快速恢复，能否自愈，对用户来说是不是感到平滑**。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/a3633e45693546789c55cbdea371e982.jpeg)

苏宁金融集团年交易量已过万亿，日均资金流水几十亿，需要保证每一笔交易资金的安全；**对这样业务需求极速变化的高并发金融资金系统进行重构，就犹如对发射出去的导弹进行二次加速，任何一个小失误，都可能导致上亿的资损，影响上亿的用户体验；**那么在重构过程中，如何保证优雅就非常重要？首先需要确定我们的目标是什么？基于这个目标我们的困难是什么？解决方案是什么？怎么去实施？怎么去演进？怎么去验证？

## 目标架构愿景

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/e8b0f3e9b19b4248822050c2357122ba.jpeg)

基于这个愿景，就要满足两点：**快和稳**； 第一个是快：我们需要对业务敏捷、快速响应业务的变化；这个也是研发中心的核心使命，能对集团业务能够很好的支撑，甚至是驱动业务的发展；第2个是稳，性能要高（20万TPS、高可用），平时会考核MTTR，出现故障后多久能恢复，10秒、20秒还是一分钟？通过这个指标去牵动其它所有的工作的优化，避免指标太多，工作没有重点，不能聚焦；**比如定10个指标，每个指标权重10%，看似面面俱到，其实没有重点；**但是系统指标有很多，有成功率，耗时，异常率，各个硬件的使用率等等，**作为负责人要找到北极星指标**；我们的北极星指标就是MTTR；以及这么多系统能否实现**弹性治理**。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/31e5c8ae80cf4552bf482b0d74a36392.jpeg)

基于目标，然后识别关键问题，那我们的关键问题有四类：

1、交付速度：基于标准的复用，并行、分布研发；2、高可用：需要分析故障点，建设DB单点/热点防护、自动化运维、服务自愈、应用级灾备能力3、可伸缩：从应用到IDC、服务器、网络，做到全网伸缩；4、低成本：不仅要节流，还要开源，重点是公司盈利和控制资损，通过技术对业务驱动力产生的正向价值，产品运营效果评估，这个也是对团队负责人的一个要求：要善于做技术产品化和技术品牌的运营；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/e2d20c830ee2459d8ea375fa19dedf67.jpeg)

高可用的重点是故障识别与应对，即对故障源的实时感知和可视化的治理。**故障源来源：**按照我们的服务模型分三方面：提供的服务、服务本身、依赖服务。**提供的服务：**故障来源在于请求，比较经常出故障的是：重复请求、并发请求、超量请求。针对每一个请求我们用不同的策略进行处理。**外部服务：**首先检测通信是否正常，通信正常后服务是否可用，服务可用后响应是否超时，这些都没问题后功能契约SLA是否满足，这样就形成了一个体系化的处理方法，就不会遗漏，也便于团队的知识传承（不论是代码结构设计，还是团队设计思想的统一，都是比较好的）

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/b368dd4abace411085cc9ab0186916c1.jpeg)

从PAAS平台进入到IAAS实现全网可伸缩。 

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/3829cb32895a4d6b9bc57620e427794b.jpeg)

**对外提供的服务**是吞吐量、单资源存储量的上限、响应时间；**内部服务**：关注DB、数据库总连接数、单数据库每秒事务数、慢SQL；**依赖服务：**银行实时清算能力、关键服务访问量等；这个是个可伸缩的框架，接下来我们详讲一些关键系统的可伸缩设计，具体是怎么做到的，交易系统、支付引擎系统和账务核心，如何实现可伸缩的。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/9f48cf10871d4fb0b2f75482863a900b.jpeg)

首先是交易系统的设计，大维度上，将B、C端拆封开，然后进行读写分离，再对写进行分库分表。这里要特别讲一下分库分表中间件有两个特别的功能：灰度支持和影子库表支持。灰度支持即用来对不同用户，对不同场景，不同功能进行灰度，影子库表主要便于生产压测使用，后面在生产压测部分会详讲；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/15883d762be34198a0337681a80afbb2.jpeg)

支付引擎即支付服务的分库分表策略，按照用户维度进行拆分。这里面有个核心设计，我们设计了一个**逻辑数据源**，而不是物理数据源，便于迁库，减少DBA的运营成本。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/c91543835c2c4139b27c37dfd41d66b3.jpeg)

**账务系统可伸缩，难点在于热点账户**；热点账户即资金处理频繁、时间点密集、基于等待的数据库排它锁的大账户；正常情况下，我们开发人员首先是切入系统进行优化，但实际**以业务为切入点会更好**。**首先对业务场景进行优化**，实现进出资金隔日，业务错峰，拆分收支账户，收款方到账准时实化，收款方到账准时实化；**接下来才是系统上的优化**，系统上做到分布式锁、资金资源池机制、缓冲记账、并发控制、异步化；**最后是账户层面的优化**，不同账户制定不同的策略。比如中间账户：只登记账户明细流水，不更新余额，日终进行汇总轧差，一次性更新；待清算账户：采用单边记账方式，待清算账户不做余额更新并且不登记账户明细流水，待日终进行单边汇总；特定业务收费账户：异步分段补账等，这样的优化才是最有效的，而且扩展性好，后期的维护成本也低；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/328fe9f9ecc14ea887b92bd54c7b8d36.jpeg)

在解决了核心瓶颈点之后，设计架构的演进路线也很重要；因为我们做系统重构是不能停业务的，就好比飞机在飞行的过程中，进行换引擎的操作；基于以上目标，我们进行架构重构，就需要注意这三点：

1、**要与业务发展路线合拍，顺势而为**：切忌沉浸在自己的技术世界里面，因为公司首先是盈利组织，研发首先要服务好业务，才能驱动业务，才能长远发展，阻碍业务发展的研发团队和研发技术方案是不长久的，特别是对于一个竞争对手激烈的高度发展的公司；

2、**专注主线、边界优先，步步为营**：就像装修房子一样，可以先把墙装好，但内部的每一个房间不一定马上装修，因为我们内部是可控的，内部系统重构可以放后，但是要先做提供给边界系统的接口，这样既可以很好的控制风险，也便于多团队之间的协同作战；

**3、定期可视化投入产出比**：因为金融系统不可能把业务停下来，他一定是在业务发展的过程当中，需要实时评估，重构和业务发展是否合拍，这样便于获得集团的大力支持，减少各方的不理解，减少很多不必要的阻碍，消除部门壁垒，消除决策层的顾虑；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/be85a89da3ce4f2f91f64536e1fd35f8.jpeg)

实施的过程当中，如何管控项目？

**项目前：**需要消除风险，获得支持，确定项目价值与范围，明确业务影响 ，获得相关干系人支持，用架构概念验证原型；

**项目中：**做到短、平、快，严格控制项目范围扩张，Rebase不可避免的业务需求；

**项目发布时：**做到稳定，用户体验连续，基于场景的立体化监控与报警，比如有时监控时我们常发现单个系统的耗时、成功率等指标都正常，但实际影响了整个调用链，影响了某个场景，所以一定需要基于场景做立体化监控。其次，**每个核心链路上的系统一定要经过应急预案的演练。**然后，**要保持悲观主义的心态**和**终结者的思维**，潜意识里面要假设重构时一定会有问题， 所以一定要守好上线的最后底线，要做到快速回滚和降级。

最后是阶段性复盘，聚焦目标，防止做的需求偏离业务目标，这一点对于研发管理者特别重要，通过这些回顾逐渐拿到和业务方的平等话语权；慢慢就会建立起和业务的一个很好的沟通渠道；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/8a53644b7b1e41da8cd20c15fcd5eb0f.jpeg)

架构重构后需要验证合理性，怎么知道我们设计的架构就是满足我们的预期呢？主要包括3个方面：

1、新老场景的推演：新老场景是否都能支持；

2、核心服务的推演：上下游系统需要演练是否稳定 ，

3、非功能性的推演：系统是否可隔离、可配置、可监控、可回滚、无单点、无状态等；

**主要推演架构的高可用、可伸缩、研发成本，运维成本和迁移成本5个指标，通过这5个指标在上述3个方面的推演，基本上就可以验证架构合理性了，**其实做架构决策也可以参考这个依据，比如多个研发中心，让你做架构决策，这个系统谁做合适？特别是架构师怎么让自己的架构决策做到大家都认可，这种思维方式都可以参考；

## 实战

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/a7a988a6a2704ad9861234f7b88fbb45.jpeg)

接下来介绍重构过程中的一些经典案例，便于大家可以在自己的公司里面快速的复制和集成；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/c9fdaabd1215415c98b44631ffad342d.jpeg)

**第一个：两周建成立体化监控体系**：为什么需要监控？因为重构过程中，随时有可能出问题，所以需要一个监控系统能随时看到重构的链路情况。**为什么是两周？**

**因为重构对时间其实是有要求的，需要快速完成**。如何在两周之内建成一个立体化监控体系呢？有几点：1、指标要极简2、可视化3、管控全网化（规则报警、一键定位、洪峰控制、业务降级），4、统一日志模型，针对重构系统的变化，要保证监控是准确的，所以需要对服务模型抽象出三层（服务的使用者、服务的提供者、服务的集成者）打日志，每个服务的接口都会打digest摘要日志。**实现过程中**，会请架构师或资深的技术经理搭好插件化框架，完成日志模型搭建、异常体系的建设，领域模型，对输出结果的阐释策略。后面程序开发时，开发人员只需要开发对应的插件即可，这样就将**程序的设计和重构变成了工厂的批量化生产**，所以这个维度上其实就能减少很多风险。做到以上几点，才能保证重构过程比较平稳和风险可视化。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/1d023ac312a441feaf7733d193d003b1.jpeg)

**第2个案例，全链路压测**：我们的压测系统经历这么几个阶段：

**首先是线下压测阶段**，这会面临3个问题：**测试环境和生产环境配置不一致**（比如生产环境配了10个数据库，而测试环境只有5个）；**测试环境和生产环境数据结构不一致**，（生产环境有些用户可能有50个订单行，而测试环境基本上为了测试方便，可能只构建了1个订单行）；**用户访问路径不一致**（比如生产环境用户从4级页到收银台有4步，而测试环境直接从金融App进来1步完成。漏斗不同决定了大促的流量比例不同，比如前面有1万TPS，经过3层漏斗只剩1000TPS如果只有2层漏斗，可能剩下5000TPS，对资源的消耗是不一样的）

第二个阶段，我们开始做**生产憋单压测**（比如代扣的订单憋在一起，从收单到支付服务到银行，都可以进行压测，但是对用户进入收银台前面的路径获取不到，基于这个缺点），这样可以实现部分链路的生产环境的真实流量压测；

第三节阶段就是，我们目前正在使用的**全链路生产压测**，就是把全链路串起来；当时我们做的时候，需要解决3个问题：（1）服务的用户商户怎么办？（2）银行不配合压测（3）如何保证支付链路系统的配置准确。解决方案是：在易购建立测试商户和账户，配置虚拟银行，配置影子库影子表，改造中间件，增加影子库单号判断，做到与生产用户数据隔离。 

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/5008e622984340f28a46438e466c1da0.jpeg)

在多活这个方面，我们也经历了几个阶段的发展，初期因为业务量小，我们做了一个**稻草系统，它是一个最小可用支付系统**，只关注80%主要的银行和支付工具，即便出问题时，能保证核心链路仍可用。这个系统在交易量大肯定是有问题的，下一阶段就开始做**支付核心链路failover**，但是仍不能解决机房出问题（如停电问题，网络设备问题等）。

所以**后来做了多活**，要做多活就要解决几个核心问题：跨机房耗时问题、依赖服务部署与治理、研发体系配套改造、故障切换的工具支持。我们的解决方案主要是：

1、支付链路单元化;

2、消息同步服务化;

3、依赖服务做多活部署;

4、研发体系的配套支持，支持发布到金丝雀环境等；

5、机房选址，跨机房调用其实存在很大延迟；

6、容灾能力，支持机房级容灾，按系统、按链路容灾；

从单机房到多机房，在架构演进过程中，也要支持容灾，因为演进过程中要做系统发布，逐步切流量。比如整体流量先切换白名单用户，再切换1/256,1/8,1/4，到1/2等，也可以针对单个系统的切换，单个模块的切换等（包括数据源的动态切换，以及切换过程中，安全等问题），便于在建设过程中控制风险；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/1f86704d8bdc40d1839dcbd3549c6629.jpeg)

热点防护包括3部分：

1、**发现热点，**感知热点源，通过埋点，关注请求，关注整个链路依赖的资源；

2、**热点诊断**，主要通过实时分析，离线分析，上下游分析;

3、**热点治理**，最粗暴的直接限流，这个是有损服务，是一刀切。

可以稍微再优雅一点，进行故障隔离，比如由于场景1导致的问题，可以直接把场景1切调，对其它场景没有影响，这样可以做到稍微精细化一点；**业务场景优化，比如账务核心收支账户分离；热点结构优化，**比如说我们在收银台上有一个活动，只取前1000名满100减50，其他人没有资格，其实是通过**优化缓存结构实现的**；**热点拆分，对数据库进行分库，对数据表进行分表， 对记录进行缓存机制处理**。基础服务包括统一日志，服务的SLA，决策支持；应用工具包括系统级的紫金大盘、产品级的地动仪，这两个其实是可视化工具，用来观测治理之后的效果。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/058bc4279c924e2ca5d7b97d8060ffed.jpeg)

接下来讲从100tps到20万tps的实践过程。

横向看，从总体架构优化到应用程序优化到数据程序优化到技术组件的优化；纵向看，深入到每一个架构，从收银台到收单到支付服务到渠道到账务核心到清结算进行优化。**应用层**看链路能否缩短，再看内部服务能否治理，再到线程池调优，去事务。

**数据层优化，需要考虑收益**，比如SQL优化排第一位，因为比分库分表的收益来的快。原则是能用单库尽量用单库，不能用单库时，才考虑分库分表。DB配置参数优化，可以优化引擎参数。因为优化过程会产生资源消耗，所以仍然**要考虑业务目标**。**基础技术平台优化**，要遵循体系，服务的本身，依赖方，服务方。

**中间是RSF分布式服务框架**（内部通过这种服务来进行路由和调度。数据方面，分库分表组件和缓存；通信方面，调度组件）的优化。**接下来是存储**，如SSD，以前是普通硬盘，那么IOPS会比较低，如果本次优化目标是提升2倍，那么只要更换SSD，速度快，成本低，对用户也没有损伤。

**闭环验证中心，这个是我们做性能优化的一个亮点：**特别是重构、性能优化的时候，需要快速知道结果是不是我们想要的，下面的服务日志、调用日志都是比较通用的。

**监控决策中心**：提供灰度方案，移动端应急管控。虽然日常有规则管控，做SLA、流控，但是关键的核心系统出问题（如支付服务），仍需人工介入确认是否执行降级和流控，因为一旦流控，会对用户产生影响。我们最近也在建设大促机器人，实现自动巡检，和智能的治理，这个后面会讲到；

**然后需要验证大屏，**可以直接看到对门店或交易是否有影响。 其实在做系统性能优化的时候，也会伴随研发组织与研发过程的“性能优化”；

**这个我有一篇文章《业务需求极速变化的高并发金融系统性能优化实践》，里面有详细的分享，**主要介绍苏宁金融对业务需求极速变化的高并发金融系统进行性能优化的实践经验，主要包括：智能监控系统，瓶颈点驱动的性能优化，全局规划驱动的性能优化以及研发组织与研发过程的“性能优化”；核心技术点包括瓶颈点的可视化诊断，瓶颈点治理，热插拔架构设计，链路failover设计，应用N+X设计，异步化，数据库单点与热点账户防护；也包括从网络，中间件，应用层，数据层，DB的横向优化方案；以及从架构，代码，会话，缓存，线程与队列，事务，堆内存与GC的纵向优化方案，横纵向结合的体系化解决方案实践；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/5e87f3bc1e744af480459e4fbd6cd469.jpeg)

以上讲述了指导思想和方法论，具体需要怎么操作呢？

可视化瓶颈点很重要，这里我举一个示例，比如银行网关系统，调用22次，透明化每一次调用，调用依赖系统多少次，每个系统的SQL有多少，这样可以清晰的可视化链路，保证快速知道哪里出了问题。然后就可以进行庖丁解牛式的优化了；

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/1d2bfdbde46a4f208a1dea9e6f6fa73b.jpeg)

为什么要做故障自愈？因为出问题时，老板一定会问：影响多少用户？影响多少场景？什么时候恢复？那个时候会非常紧张，也忙于处理生产问题，无法快速回复老板问题，也无法快速想出优化方案，所以需要有个地方可以看到问题影响面、执行什么操作可以恢复。实现这样的系统需要三部分：故障源感知、智能诊断引擎、故障治理。

故障源感知：指标分为3类，业务指标、系统指标、基础指标。观测指标发现，几类容易出问题的地方：

（1）系统变更，出故障时首先问，昨天有没有发布？系统变更占权重很大；

（2）突发业务量，可能某个商品突然很火爆，大促前估不准业务量；

（3）操作失误，拓扑获取和链路追踪，知道调用链出了什么问题；

（4）单点追踪；

（5）安全攻击；

**通过诊断业务系统暴露的问题，可以将其指标化，才能便于工作的落地与执行：**比如高可用时，不能定9999多少个9的目标，可以定MTTR=1分钟的目标。以前会定A完成日志模型，B完成SQL优化，C完成异常治理等，但一段时间后发现并没有解决问题，**后来我们定了“北极星”指标MTTR=1分钟**，这样技术经理自然的知道需要完成日志模型优化等工作。通过这一个指标去牵动其他指标的达成。

故障治理有3方面：场景修复、链路修复、服务修复；服务修复需要提前定义执行引擎，WAF防火墙到负载均衡到RSF到SCM到TCC到DB，形成一个体系。出不同问题，会执行不同的应急预案。**最后针对不同的业务流和资金流，做异常数据比对**。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/a111c0653f3442e1aaf75d9a7c8f424a.jpeg)

最后介绍我们正在做的事情和对未来的展望。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/8ab6c4eaf8ba43ce96e8a225485117b9.jpeg)

机器人巡检，因为大促对我们来说是很重要的节点，每次也耗费很多人力物力，可以实现宏观上系统的整体化构造，微观上看到每个系统的状态。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/6268153f25964ead965949cc966f4ced.jpeg)

全网可视化作战沙盘，上面的几个指标被称为“北极星指标”，不是开发部门自定的，是根据集团战略目标分解的。战略目标分解到研发中心，研发中心分解到每一个项目。右上角的+号有3个功能：1、全景的产品视图，2、系统的治理，3、考核，每做一个优化需要考核，这样每个团队能形成同一个目标去做事。

![img](http://5b0988e595225.cdn.sohucs.com/images/20190516/db396cc7969a426390f229a0ced675d7.jpeg)

这样就将人，事全部链接成了一个整体，所有的工作都将形成闭环，同时所有决策层都能可视化的看到；便于所有工作的快速推进和落地；



## 支付系统设计

### **概述**

　　支付系统是连接消费者、商家（或平台）和金融机构的桥梁，管理支付数据，调用第三方支付平台接口，记录支付信息（对应订单号，支付金额等），金额对账等功能，根据不同公司对于支付业务的定位不同大概有几个阶段：

- 第一阶段：支付作为一个（封闭）的、独立的应用系统，为各系统提供支付功能支持。一般来说，这个系统仅限于为公司内部的业务提供支付支持，并且和业务紧密耦合。
- 第二阶段：**支付作为一个开发的系统，为公司内外部系统、各种业务提供支付服务，支付服务本身应该是和具体的业务解耦合。**

------

### 1.支付系统

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224202924438-239435086.png)

我们先来看一下用户完成一次购物需要进行那些操作：

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224202941616-461745266.png)

通常消费者在手机APP或者网站都会涉及到支付相关的业务场景，用户只需要简单点击支付按钮输入支付密码，就可以完成整个支付过程，那么我就和大家一起来看看一个完整的支付系统有什么功能组成和设计时需要考虑那些问题。

- **1.1 支付系统的作用**

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224203007625-1181965707.png)

从上图中我们可以看出真实的资金流向。首先当用户产生支付行为时，资金从用户端流向支付系统，退款时则相反，从支付系统回流至用户端。因此在整个交易过程中用户端与支付系统是双向资金的流动方式。对于支付系统而言，资金有进有出。从支付系统到商户端就比较简单了，在清算完成后支付系统负责将代收的资金结算给商户，通常结算的操作可以在线上来完成（采用支付公司代付接口或者银企直连接口来完成），也可以由公司财务通过线下手工转账的方式来完成，因此这种资金流动的方式是单向的。出于资金安全考虑，大多数公司通常这部分采用线下方式实现。

真实的资金流由支付公司按照约定期限（通常 T+1 ）结算到平台公司对公账户中，然后再由平台公司再按照交易明细进行二次清算后结算给对应的商户。

- **1.2 支付系统功能模块组成**

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224203021541-1459599714.png)

- **1.3 完整的支付系统包括如下功能：**

1.  应用管理: 同时支持公司多个业务系统对接。
2.  商户管理: 支持商户入驻，商户需要向平台方提供相关的资料备案。
3.  渠道管理: 支持微信、支付宝、银联、京东支付等多种渠道。
4.  账户管理: 渠道账户管理，支持共享账户（个人商户）及自有账户。
5.  支付交易: 生成预支付订单、提供退款服务。
6.  对账管理: 实现支付系统的交易数据与第三方支付渠道交易明细的自动核对（通常T+1），确保交易数据的准确性和一致性。
7.  清算管理: 计算收款交易中商户的应收与支付系统收益。
8.  结算管理: 根据清算结果，将资金划拨至商户对应的资金帐户中。

------

### 2.核心流程

支付系统有几个关键的核心流程：**支付流程、对账流程、结算流程**

***\*2.1 支付流程\****

- **2.1.1 支付流程图**

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224203038905-241620870.png)

- **2.1.2 支付流程说明**

1. 1.  用户在商城选购商品并发起支付请求；
   2.  商城将支付订单通过B2C网关收款接口传送至支付网关；
   3.  用户选择网银支付及银行，支付平台将订单转送至指定银行网关界面；
   4.  用户支付完成，银行处理结果并向平台返回处理结果；
   5.  支付平台接收处理结果，落地处理并向商户返回结果；
   6.  商城接收到支付公司返回结果，落地处理（更改订单状态）并通知用户。

一般而言支付系统会给商户设置有“可用余额”账户、“待结算”账户；系统在接收到银行返回支付成功信息会进行落地处理，一方面更改对应订单状态，另一方面在商户待结算账户记入一笔金额；该笔金额，系统会根据结算周期从待结算账户--->“可用余额”账户。

- **2.1.3 退款流程说明**

1.  用户在商户平台发起退款申请，商户核实退款信息及申请；
2.  商户登录支付平台账户/或者通过支付公司提供的退款接口向支付平台发起退款；
3.  支付系统会对退款信息校验（退款订单对应的原订单是否支付成功？退款金额是否少于等于原订单金额？），校验商户账户余额是否充足等；校验不通过，则无法退款；
4.  支付系统在商户可用余额账户扣除金额，并将退款订单发送至银行，银行完成退款操作。注意：对于网关收款的订单退款，各银行要求不一，有些银行提供的退款接口要求原订单有效期在90或180天，有些银行不提供退款接口；针对超期或者不支持接口退款的订单，支付公司通过代付通道完成退款操作。

对于收单金额未结算，还在“待结算”账户的订单，如果出现退款情况，业务流程和上述流程差不多，只是从待结算账户进行扣款。

 **2.2 对账流程**

- **2.2.1 对账流程图**

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224203055855-992250434.png)

**说明**

对账，我们一般称为勾兑，支付系统的对账，包含着两个层面：

1. 支付系统内部间的对账，支付系统一般是分布式的，整个支付系统被拆分成了多个子系统，如交易系统、账户系统、会计系统、账户系统，每个子系统在处理各自的业务，系统间的对账，就是以上系统的核对，用于修正内部系统的数据不一致。
2. 支付系统与渠道的对账，这里的渠道泛指所有为支付系统提供代收付业务的渠道，如：第三方支付公司、银行、清算中心、网联、银联等。

**支付系统与渠道间的对账**

系统间的对账比较好理解，这里主要讲支付系统与渠道间的对账。支付系统与渠道间的对账，又包含2个维度：

1. 信息流勾对：即业务对账／交易对账，主要是就收单交易的支付信息与银行提供的信息流文件进行勾兑。信息流的勾地能发现支付系统与银行系统间的掉单、两边由于系统间的原因导致的同一笔交易支付金额不一致（可能性很小）或者支付状态不一致。信息流勾兑一般用来恢复掉单数据，可通过补单或者具体系统问题排查解决。
2. 资金流勾对：即资金对账，主要就收单交易的支付信息与银行提供的资金流信息进行勾兑。资金流的勾兑能发现支付系统在银行的帐户资金实际发生的变动与应该发生的变动的差异，比如长款（银行多结算给支付系统）和短款（银行少结算给支付系统）。

　　说了这么多，就出现来4个对账文件，**支付系统信息流文件、支付系统资金流文件、银行信息流文件、银行资金流文件**。业务对账（勾兑）就是支付系统的信息流文件与银行的信息流文件勾兑，资金对账即支付系统的资金流文件与银行的资金流文件勾兑。

**核对的差异处理**

1、信息流勾对的差异处理

- 支付系统信息流没有，而银行有的差异，可能是因为支付系统交易数据的丢失、银行的掉单，如果是银行的掉单，由支付公司的运营登录银行网银确认后，做补单处理，并将差异表中该记录核销。
- 支付系统信息流有，而银行没有的差异，此种情况一般不会发生，因为支付系统所有的交易数据都是取银行返回状态的数据。

2、资金流勾对对差异处理

- 支付系统资金流没有，而银行有的差异。可能原因如下：1、银行日切晚与支付系统核心账务系统；2、支付系统账务核心系统与其他系统间的掉单。一旦出现，则会出现长款（即银行不应该结算而实际结算）的现象，对于因日切导致的差异，在第二天的对账中系统会对平，其他原因的，需要技术排查。
- 支付系统资金流有，而银行没有的差异，可能是因为银行日切早于支付系统的核心账务系统，一旦出现，会出现短款（银行应结算而实际未结算）的现象，银行日切导致段差异，会在下一天与银行的勾对中，将此笔差异勾对上，如果是非日切导致的原因，就需要找银行追款了。

总结就是，业务对账，即信息流对账，支付系统的交易流水与银行的交易流水间核对，保障支付交易完整入账。资金对账，即资金流对账，支付系统的入账流水与银行的结算流水间核对，保障银行入账流水与实际入账资金的匹配。

**2.3 结算流程**

- **2.3.1 结算流程图**

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224203110438-1940495209.png)

**说明**

\1. 在清结算部分，系统按照设定好的清结算规则自动将钱款结算给商户。

\2. 完善的运营会计体系帮助财务进行精细化核算，提高财务效率。

\3. 与支付渠道自动进行对账，确保账务正确，在异常情况下能及时定位问题并处理。

\4. 系统更是能对商户进行个性化的费率配置或账期配置，方便灵活。

\5. 系统的价值不仅体现在支付清结算方面，同时更是提升了运营管理效率。

\6. 支付清结算系统可以有效帮助运营、财务、开发以及管理人员。对于运营人员，系统可帮助处理平台的运营工作，包括各类支付管理，商户、会员管理，营销活动的数据统计等，全面提高运营效率。针对财务人员，可以协助完成资金对账、会计处理，出入款管理，账务差错处理等，大部分工作由系统自动处理，减少人工处理，提高资金处理效率。

\7. 一套灵活便捷的配置后台供开发人员快速调整系统以适应新的业务，并能方便对系统进行维护，如渠道接入、费率配置、账期调整等，提高开发效率。

\8. 系统提供资金流转过程中各个环节的数据，能够从各个维度进行核算和分析，形成对管理人员的决策支持，从而提高决策效率。

- **2.3.2 关键表设计**

![img](https://img2018.cnblogs.com/i-beta/1645656/201912/1645656-20191224203123264-2142191563.png)

- **2.3.3 支付系统要点**

　　在支付系统中，支付网关和支付渠道的对接是最繁琐重要的功能之一，其中支付网关是对外提供服务的接口，所有需要渠道支持的资金操作都需要通过网关分发到对应的渠道模块上。一旦定型，后续就很少，也很难调整。而支付渠道模块是接收网关的请求，调用渠道接口执行真正的资金操作。每个渠道的接口，传输方式都不尽相同，所以在这里，支付网关相对于支付渠道模块的作用，类似设计模式中的wrapper，封装各个渠道的差异，对网关呈现统一的接口。而网关的功能是为业务提供通用接口，一些和渠道交互的公共操作，也会放置到网关中。

　　支付系统对其他系统，特别是交易系统，提供的支付服务包括签约，支付，退款，充值，转帐，解约等。有些地方还会额外提供签约并支付的接口，用于支持在支付过程中绑卡。 每个服务实现的流程也是基本类似，包括下单，取消订单，退单，查单等操作。每个操作实现，都包括参数校验，支付路由，生成订单，风险评估，调用渠道服务，更新订单和发送消息这7步，对于一些比较复杂的渠道服务，还会涉及到异步同通知处理的步骤。

**01 网关前置**

　　支付网关前置是对接业务系统，为其提供支付服务的模块。它是所有支付服务接口的集成前置，将不同支付渠道提供的接口通过统一的方式呈现给业务方。这样接入方就只需要对接支付网关，增加和调整支付渠道对业务方是透明的。 支付网关前置的设计对整个支付系统的稳定性、功能、性能以及其他非功能性需求有着直接的影响。

在支付网关中需要完成大量的操作，为了保证性能，这些操作都尽量异步化来处理。支付网关前置应保持稳定，尽量减少系统重启等操作对业务方的影响。支付网关也避免不了升级和重启。这可通过基于Nginx的LBS(Load Balance System)网关来解决。LBS在这里有两个作用： 一个是实现负载均衡，一个是隔离支付网关重启对调用的影响。 支付网关也采用多台机器分布式部署，重启时，每个服务器逐个启动。某台服务器重启时，首先从LBS系统中取消注册，重启完成后，再重新注册到LBS上。这个过程对调用方是无感知的。

为了避免接口受攻击，在安全上，还得要求业务方通过HTTPS来访问接口，并提供防篡改机制。防篡改则通过接口参数签名来处理。现在主流的签名是对接口参数按照参数名称排序后，做加密和散列，参考微信的签名规范。

**02 参数校验**

- 所有的支付操作，都需要对输入执行参数校验，避免接口受到攻击。
- 验证输入参数中各字段的有效性验证，比如用户ID,商户ID,价格，返回地址等参数。
- 验证账户状态。交易主体、交易对手等账户的状态是处于可交易的状态。
- 验证订单：如果涉及到预单，还需要验证订单号的有效性，订单状态是未支付。为了避免用户缓存某个URL地址，还需要校验下单时间和支付时间是否超过预定的间隔。
- 验证签名。签名也是为了防止支付接口被伪造。 一般签名是使用分发给商户的key来对输入参数拼接成的字符串做MD5 Hash或者RSA加密，然后作为一个参数随其他参数一起提交到服务器端。

**03 路由选择**

　　根据用户选择的支付方式确定用来完成该操作的合适的支付渠道。用户指定的支付方式不一定是最终的执行支付的渠道。比如用户选择通过工行信用卡来执行支付，但是我们没有实现和工行的对接，而是可以通过第三方支付，比如支付宝、微信支付、易宝支付，或者银联来完成。那如何选择合适的支付渠道，就通过支付路由来实现。支付路由会综合考虑收费、渠道的可用性等因素来选择最优方案

**04 风险评估**

检查本次交易是否有风险。风控接口返回三种结果：阻断交易、增强验证和放行交易。

- 阻断交易，说明该交易是高风险的，需要终止，不执行第5个步骤；
- 增强验证，说明该交易有一定的风险，需要确认下是不是用户本人在操作。这可以通过发送短信验证码或者其他可以验证用户身份的方式来做校验，验证通过后，可以继续执行该交易。
- 放行交易，即本次交易是安全的，可以继续往下走。

**05 发送消息**

通过消息来通知相关系统关于订单的变更。风控，信用BI等，都需要依赖这数据做准实时计算。

**06 更新订单**

对于同步返回的结果，需要在主线程中更新订单的状态，标记是支付成功还是失败。对于异步返回的渠道，需要在异步程序中处理。

**07 异步通知**

其中涉及到调用远程接口，其延迟不可控。如果调用方一直阻塞等待，很容易超时。引入异步通知机制，可以让调用方在主线程中尽快返回，通过异步线程来得到支付结果。对于通过异步来获取支付结果的渠道接口，也需要对应的在异步通知中将结果返回给调用方。 异步通知需要调用方提供一个回调地址，一般以http或者https的方式。这就有技术风险，如果调用失败，还需要重试。而重试不能过于频繁，需要逐步拉大每一次重试的时间间隔。 在异步处理程序中，订单根据处理结果变更状态后，也要发消息通知相关系统。

**08 生成交易订单**

将订单信息持久化到数据库中。当访问压力大的时候，数据库写入会成为一个瓶颈。

**09 交易流水和记账**

每一笔交易都需要记录流水，并登记到个人和机构的分户账户上，统计和分析也需要根据交易流水来更新相关数据。 而个人和机构账户总额更新、交易流水记录以及库存的处理，更是需要事务处理机制的支持。 从性能角度， 可以弱化了事务处理的要求，采用消息机制来异步化和交易相关的数据处理。

- 在支付网关前置的主流程中，仅记录交易流水，即将当前的请求保存到数据库中。
- 完成数据记录后，发送MQ出来，记账、统计、分析，都是接收MQ来完成数据处理。
- 涉及到本地资金支付，比如钱包支付，会需要分布式事务处理，扣减账号余额，记账，扣减库存等，每个操作失败，都要回滚。阿里有很不错的分享，这里不详细描述。
- 当交易量上来后，需要考虑交易表的分表分库的事情。分表分库有两个策略，按照流水号或者交易主体id来走。后者可以支持按用户来获取交易记录。我们用的是前者。后者可以走elastic，确保数据库专用。风控，信用和统计所需要的数据，通过MQ同步到历史库里面。作为支付系统最有价值的数据，在存储上做到专库专用，无可厚非，毕竟存储成本还是廉价的。

**10 支付路由**

支付路由是一个复杂的话题。对支付系统来说，能支持的支付方式越多越好，不能由于支付方式的不支持断了财路。现实中的支付方式多得难以置信。用户随时甩出一张你听都没听说过的卡。如果一个银行卡只有几个用户在用，那针对这个卡开发个对接有点得不尝失。现在第三方支付的爆发，确实给开发支付系统省了不少事。但是公司不可能只对接一个第三方支付，如果这个渠道出问题了，或者闹矛盾了，把链接给掐了，老板还不欲哭无泪。总之，得对接多个渠道。对于交易量大的银行，还得考虑直联。

**11 渠道接入**

对于支付渠道，首先考虑的是接入哪些渠道。要对接的渠道按优先级有：

- 第三方支付，对大部分应用来说，支付宝和微信支付都是必须的，一般来说，这两者可以占到90%以上的交易量。用户不需要绑卡，授权后直接支付就行。各种平台都支持，性能和稳定性都不错。对于一些特殊业务，比如游戏，企业支付，可以查看一些专用的第三方支付平台。
- 银联，它的存在，极大方便了和银行的对接。和第三方支付主要不同在两个地方一是需要绑卡，也就是用户先把卡号，手机，身份证号提供出来。这一步会折损不少用户。绑卡后，以后的支付操作就简单了，用户只需要输入密码就行。手机客户端不需要像第三方支付那样安装SDK，都在服务器端完成。当然，这是针对快捷支付。网银支付还是挺麻烦的。银联接入也需要ADSS认证。
- 银行：2018年2月9日银监会公布了最新权威数字：一共【4549家】开发性金融机构1家：国家开发银行；政策性银行2家：进出口银行、农业发展银行；5大国有银行：工、建、农、中、交；邮储银行1家；全国性股份制商业银行12家：招行、中信、兴业、民生、浦发、光大、广发、华夏、平安、浙商、渤海、恒丰；金融资产管理公司4家：信达、华融、长城、东方四大AMC；城商行134家；住房储蓄银行1家；民营银行17家，如网商银行；农商行1262家；农村合作银行33家；农村信用社965家；村镇银行1562家；贷款公司13家；农村资金互助社48家；外资法人银行39家；信托公司68家；金融租赁公司69家；企业集团财务公司247家；汽车金融公司25家；消费金融公司22家；货币经纪公司5家；其他金融机构14家。一般对接一个银行预计有3周左右的工作量，大部分银行需要专线接入，费用和带宽有关，一年也得几万费用。不同银行对接入环境有不同要求，这也是成本。
- 手机支付：比如苹果的In-App支付， 三星支付、华为支付等， 这些支付仅针对特定的手机型号， 支持NFC等，根据业务需要也可以接入。

------

###  总结

　　支付系统是一个繁杂的系统，其中涉及了各种错综复杂的业务流程，以上只是简单介绍了支付系统我们能看见的一些问题和设计，还有后续的系统保障没有写出来，没写出来的才是关键部分，比如：支付系统监控（业务监控分类、渠道监控、商户监控、账户监控）文章只是引子， 架构不是静态的，而是动态演化的。只有能够不断应对环境变化的系统，才是有生命力的系统。所以即使你掌握了以上所有的业务细节，仍然需要演化式思维，在设计的同时，借助反馈和进化的力量推动架构的持续演进。



## 支付系统高可用架构设计实战



#### 一、背景

对于互联网应用和企业大型应用而言，多数都尽可能地要求做到7*24小时不间断运行，而要做到完全不间断运行可以说“难于上青天”。为此，对应用可用性程度的衡量标准一般有3个9到5个9。

| 可用性指标 | 计算方式         | 不可用时间（分钟） |
| ---------- | ---------------- | ------------------ |
| 99.9%      | 0.1%*365*24*60   | 525.6              |
| 99.99%     | 0.01%*365*24*60  | 52.56              |
| 99.999%    | 0.001%*365*24*60 | 5.256              |

对于一个功能和数据量不断增加的应用，要保持比较高的可用性并非易事。为了实现高可用，宜信支付系统从避免单点故障、保证应用自身的高可用、解决交易量增长等方面做了许多探索和实践。

在不考虑外部依赖系统突发故障，如网络问题、三方支付和银行的大面积不可用等情况下，宜信支付系统的服务能力可以达到99.999%。

本文重点讨论如何提高应用自身的可用性，关于如何避免单点故障和解决交易量增长问题会在其他系列讨论。

为了提高应用的可用性，首先要做的就是尽可能避免应用出现故障，但要完全做到不出故障是不可能的。互联网是个容易产生“蝴蝶效应”的地方，任何一个看似很小的、发生概率为0的事故都可能出现，然后被无限放大。

大家都知道RabbitMQ本身是非常稳定可靠的，宜信支付系统最开始也一直在使用单点RabbitMQ，并且从未出现运行故障，所以大家在心理上都认为这个东西不太可能出问题。

直到某天，这台节点所在的物理主机硬件因为年久失修坏掉了，当时这台RabbitMQ就无法提供服务，导致系统服务瞬间不可用。

故障发生了也不可怕，最重要的是及时发现并解决故障。宜信支付系统对自身系统的要求是，秒级发现故障，快速诊断和解决故障，从而降低故障带来的负面影响。

#### 二、问题

以史为鉴。首先我们简单的回顾一下，宜信支付系统曾经碰到的一些问题：

(1) 新来的开发同事在处理新接入的三方通道时，由于经验不足忽视了设置超时时间的重要性。就是这样一个小小的细节，导致这个三方队列所在的交易全部堵塞，同时影响到其他通道的交易；

(2) 宜信支付系统是分布式部署的，并且支持灰度发布，所以环境和部署模块非常多而且复杂。某次增加了一个新模块，由于存在多个环境，且每个环境都是双节点，新模块上线后导致数据库的连接数不够用，从而影响其他模块功能；

(3) 同样是超时问题，一个三方的超时，导致耗尽了当前所配置的所有worker threads, 以至于其他交易没有可处理的线程；

(4) A三方同时提供鉴权，支付等接口，其中一个接口因为宜信支付系统交易量突增，从而触发A三方在网络运营商那边的DDoS限制。通常机房的出口IP都是固定的，从而被网络运营商误认为是来自这个出口IP的交易是流量攻击，最终导致A三方鉴权和支付接口同时不可用。

(5) 再说一个数据库的问题，同样是因为宜信支付系统交易量突增引发的。建立序列的同事给某个序列的上限是999，999，999，但数据库存的这个字段长度是32位，当交易量小的时候，系统产生的值和字段32位是匹配的，序列不会升位。可是随着交易量的增加，序列不知不觉的升位数了，结果导致32位就不够存放。

类似这样的问题对于互联网系统非常常见，并且具有隐蔽性，所以如何避免就显得非常重要了。

#### 三、解决方案

下面我们从三个方面来看宜信支付系统所做的改变。

###### 3.1 尽可能避免故障

**3.1.1 设计可容错的系统**

![img](https://segmentfault.com/img/remote/1460000019452656?w=546&h=385)

比如重路由，对于用户支付来说，用户并不关心自己的钱具体是从哪个通道支付出去的，用户只关心成功与否。宜信支付系统连接30多个通道，有可能A通道支付不成功，这个时候就需要动态重路由到B或者C通道，这样就可以通过系统重路由避免用户支付失败，实现支付容错。

还有针对OOM做容错，像Tomcat一样。系统内存总有发生用尽的情况，如果一开始就对应用本身预留一些内存，当系统发生OOM的时候，就可以catch住这个异常，从而避免这次OOM。

**3.1.2 某些环节快速失败“fail fast原则”**

Fail fast原则是当主流程的任何一步出现问题的时候，应该快速合理地结束整个流程，而不是等到出现负面影响才处理。

举个几个例子：

(1)支付系统启动的时候需要加载一些队列信息和配置信息到缓存，如果加载失败或者队列配置不正确，会造成请求处理过程的失败，对此最佳的处理方式是加载数据失败，JVM直接退出，避免后续启动不可用；

(2)支付系统的实时类交易处理响应时间最长是40s，如果超过40s前置系统就不再等待，释放线程，告知商户正在处理中，后续有处理结果会以通知的方式或者业务线主动查询的方式得到结果；

(3)宜信支付系统使用了redis做缓存数据库，用到的地方有实时报警埋点和验重等功能。如果连接redis超过50ms，那么这笔redis操作会自动放弃，在最坏的情况下这个操作带给支付的影响也就是50ms，控制在系统允许的范围内。

**3.1.3 设计具备自我保护能力的系统**

系统一般都有第三方依赖，比如数据库，三方接口等。系统开发的时候，需要对第三方保持怀疑，避免第三方出现问题时候的连锁反应，导致宕机。

（1）拆分消息队列

宜信支付系统提供各种各样的支付接口给商户，常用的就有快捷，个人网银，企业网银，退款，撤销，批量代付，批量代扣，单笔代付，单笔代扣，语音支付，余额查询，身份证鉴权，银行卡鉴权，卡密鉴权等。与其对应的支付通道有微信支付，ApplePay，支付宝等30多家支付通道，并且接入了几百家商户。在这三个维度下，如何确保不同业务、三方、商户、以及支付类型互不影响，宜信支付系统所做的就是拆分消息队列。下图是部分业务消息队列拆分图：

![img](https://segmentfault.com/img/remote/1460000019452657)

（2）限制资源的使用

对于资源使用的限制设计是高可用系统最重要的一点，也是容易被忽略的一点，资源相对有限，用的过多了，自然会导致应用宕机。为此宜信支付系统做了以下功课：

- 限制连接数

随着分布式的横向扩展，需要考虑数据库连接数，而不是无休止的最大化。数据库的连接数是有限制的，需要全局考量所有的模块，特别是横向扩展带来的增加。

- 限制内存的使用

内存使用过大，会导致频繁的GC和OOM，内存的使用主要来自以下两个方面：

A:集合容量过大；

B:未释放已经不再引用的对象，比如放入ThreadLocal的对象一直会等到线程退出的时候回收。

- 限制线程创建

线程的无限制创建，最终导致其不可控，特别是隐藏在代码中的创建线程方法。

当系统的SY值过高时，表示linux需要花费更多的时间进行线程切换。Java造成这种现象的主要原因是创建的线程比较多，且这些线程都处于不断的阻塞（锁等待，IO等待）和执行状态的变化过程中，这就产生了大量的上下文切换。

除此之外，Java应用在创建线程时会操作JVM堆外的物理内存，太多的线程也会使用过多的物理内存。

对于线程的创建，最好通过线程池来实现，避免线程过多产生上下文切换。

- 限制并发

做过支付系统的应该清楚，部分三方支付公司是对商户的并发有要求的。三方给开放几个并发是根据实际交易量来评估的，所以如果不控制并发，所有的交易都发给三方，那么三方只会回复“请降低提交频率”。

所以在系统设计阶段和代码review阶段都需要特别注意，将并发限制在三方允许的范围内。

我们讲到宜信支付系统为了实现系统的可用性做了三点改变，其一是尽可能避免故障，接下来讲后面两点。

###### 3.2 及时发现故障

故障就像鬼子进村，来的猝不及防。当预防的防线被冲破，如何及时拉起第二道防线，发现故障保证可用性，这时候报警监控系统的开始发挥作用了。一辆没有仪表盘的汽车，是无法知道车速和油量，转向灯是否亮，就算“老司机”水平再高也是相当危险的。同样，系统也是需要监控的，最好是出现危险的时候提前报警，这样可以在故障真正引发风险前解决。

**3.2.1 实时报警系统**

如果没有实时报警，系统运行状态的不确定性会造成无法量化的灾难。宜信支付系统的监控系统指标如下：

- 实时性-实现秒级监控；
- 全面性-覆盖所有系统业务，确保无死角覆盖；
- 实用性-预警分为多个级别，监控人员可以方便实用地根据预警严重程度做出精确的决策；
- 多样性-预警方式提供推拉模式，包括短信，邮件，可视化界面，方便监控人员及时发现问题。

报警主要分为单机报警和集群报警，而宜信支付系统属于集群部署。实时预警主要依靠各个业务系统实时埋点数据统计分析实现，因此难度主要在数据埋点和分析系统上。

**3.2.2 埋点数据**

要做到实时分析，又不影响交易系统的响应时间，宜信支付系统在各个模块中通过redis实时做数据埋点，然后将埋点数据汇总到分析系统，分析系统根据规则进行分析报警。

**3.2.3 分析系统**

分析系统最难做的是业务报警点，例如哪些报警只要一出来就必须出警，哪些报警一出来只需要关注。下面我们对分析系统做一个详细介绍：

(1)系统运行架构

![img](https://segmentfault.com/img/remote/1460000019452658)

(2)系统运行流程

![img](https://segmentfault.com/img/remote/1460000019452659)

(3)系统业务监控点

宜信支付系统的业务监控点都是在日常运行过程中一点一滴总结出来的，分为出警类和关注类两大块。

A:出警类

- 网络异常预警；
- 单笔订单超时未完成预警；
- 实时交易成功率预警；
- 异常状态预警；
- 未回盘预警；
- 失败通知预警；
- 异常失败预警；
- 响应码频发预警；
- 核对不一致预警；
- 特殊状态预警；

B:关注类

- 交易量异常预警；
- 交易额超过500W预警；
- 短信回填超时预警；
- 非法IP预警；

**3.2.4 非业务监控点**

非业务监控点主要是指从运维角度的监控，包括网络，主机，存储，日志等。具体如下：

(1)服务可用性监控

使用JVM采集Young GC/Full GC次数及时间、堆内存、耗时Top 10线程堆栈等信息，包括缓存buffer的长度。

(2)流量监控

通过Agent监控代理部署在各个服务器上，实时采集流量情况。

(3)外部系统监控

通过间隙性探测来观察三方或者网络是否稳定。

(4)中间件监控

- 针对MQ消费队列，通过RabbitMQ脚本探测，实时分析队列深度；
- 针对数据库部分，通过安装插件xdb，实时监控数据库性能.

(5)实时日志监控

通过rsyslog完成分布式日志的归集，然后通过系统分析处理，完成日志实时监控和分析。最后，通过开发可视化页面展示给使用者。

(6)系统资源监控

通过Zabbix监控主机的CPU负载、内存使用率、各网卡的上下行流量、各磁盘读写速率、各磁盘读写次数(IOPS)、各磁盘空间使用率等。

以上就是宜信支付系统的实时监控系统所做的，主要分为业务点监控和运维监控两方面，虽然系统是分布式部署，但是每个预警点都是秒级响应。除此之外，业务系统的报警点也有一个难点，那就是有些报警是少量报出来不一定有问题，大量报警就会有问题，也就是所谓的量变引起质变。

举一个例子，拿网络异常来说，发生一笔可能是网络抖动，但是多笔发生就需要重视网络是否真的有问题，针对网络异常宜信支付系统的报警样例如下：

- 单通道网络异常预警：1分钟内A通道网络异常连续发生了12笔，触发了预警阀值；
- 多通道网络异常预警1: 10分钟内，连续每分钟内网络异常发生了3笔，涉及3个通道，触发了预警阀值；
- 多通道网络异常预警2： 10分钟内，总共发生网络异常25笔，涉及3个通道， 触发了预警阀值.

**3.2.5 日志记录和分析系统**

对于一个大型系统而言，每天记录大量的日志和分析日志是有一定的难度的。宜信支付系统每天平均有200W笔订单量，一笔交易经过十几个模块流转，假设一笔订单记录30条日志，可想而知每天会有多么巨大的日志量。

宜信支付系统日志的分析有两个作用，一个是实时日志异常预警，另外一个是提供订单轨迹给运营人员使用。

(1)实时日志预警

实时日志预警是针对所有实时交易日志，实时抓取带有Exception或者Error的关键字然后报警。这样的好处是，如果代码中有任何运行异常，都会第一时间发现。宜信支付系统针对实时日志预警的处理方式是，首先采用rsyslog完成日志归集，然后通过分析系统实时抓取，再做实时预警。

(2)订单轨迹

对于交易系统，非常有必要实时了解一笔订单的状态流转。宜信支付系统最初的做法是通过数据库来记录订单轨迹，但是运行一段时间后，发现订单量剧增导致数据库表过大不利于维护。

宜信支付系统现在的做法是，每个模块通过打印日志轨迹，日志轨迹打印的格式按照数据库表结构的方式打印，打印好所有日志后，rsyslog来完成日志归集，分析系统会实时抓取打印的规范日志，进行解析然后按天存放到数据库中，并展示给运营人员可视化界面。

日志打印规范如下：

2016-07-22 18:15:00.512||pool-73-thread-4||通道适配器||通道适配器-发三方后||CEX16XXXXXXX5751||16201XXXX337||||||04||9000||【结算平台消息】处理中||0000105||98XX543210||GHT||03||11||2016-07-22 18:15:00.512||张张||||01||tunnelQuery||true||||Pending||||10.100.140.101||8cff785d-0d01-4ed4-b771-cb0b1faa7f95||10.999.140.101||O001||||0.01||||||||[http://10.100.444.59](http://10.100.444.59/):8080/regression/notice||||240||2016-07-20 19:06:13.000xxxxxxx

||2016-07-22 18:15:00.170||2016-07-22 18:15:00.496xxxxxxxxxxxxxxxxxxxx

||2016-07-2019:06:13.000||||||||01||0103||111xxxxxxxxxxxxxxxxxxxxxxxxx

||8fb64154bbea060afec5cd2bb0c36a752be734f3e9424ba7xxxxxxxxxxxxxxxxxxxx

||622xxxxxxxxxxxxxxxx||9bc195a59dd35a47||f2ba5254f9e22914824881c242d211

||||||||||||||||||||6xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx010||||||||||

简要日志可视化轨迹如下：

![img](https://segmentfault.com/img/remote/1460000019452660)

日志记录和分析系统除了以上两点，也提供了交易和响应报文的下载和查看。

**3.2.6 7*24小时监控室**

宜信支付系统以上的报警项目给操作人员提供推拉两种方式，一种是短信和邮件推送，一种是报表展示。除此之外，由于支付系统相比互联网其他系统本身的重要性，宜信支付系统采用7*24小时的监控室保证系统的安全稳定。

###### **3.3 及时处理故障**

在故障发生之后，特别是生产环境，第一时间要做的不是寻找故障发生的原因，而是以最快速度处理故障，保障系统的可用性。宜信支付系统常见的故障和处理措施如下：

**3.3.1 自动修复**

针对自动修复部分，宜信支付系统常见的故障都是三方不稳定造成的，针对这种情况，就是上面说的系统会自动进行重路由。

**3.3.2 服务降级**

服务降级指在出现故障的情况下又无法快速修复的情况下，把某些功能关闭，以保证核心功能的使用。宜信支付系统针对商户促销的时候，如果某个商户交易量过大，会实时的调整这个商户的流量，使此商户服务降级，从而不会影响到其他商户，类似这样的场景还有很多，具体的服务降级功能会在后续系列介绍。

#### 四、Q&A

Q1: 能讲讲当年那台RabbitMQ宕掉的具体细节和处理方案吗？

A1: RabbitMQ宕机时间引发了对系统可用性的思考，当时我们的RabbitMQ本身并没有宕机（RabbitMQ还是很稳定的），宕机的是RabbitMQ所在的硬件机器，但是问题就出在当时RabbiMQ的部署是单点部署，并且大家惯性思维认为RabbitMQ不会宕机，从而忽略了它所在的容器，所以这个问题的产生对于我们的思考就是所有的业务不可以有单点，包括应用服务器、中间件、网络设备等。单点不仅仅需要从单点本身考虑，比如整个服务做双份，然后AB测试，当然也有双机房的。

Q2: 贵公司的开发运维是在一起的吗？

A2: 我们开发运维是分开的，今天的分享主要是站在整个系统可用性层面来考虑的，开发偏多，有一部分运维的东西。这些宜信支付系统的走过的路，是我一路见证过的。

Q3: 你们的后台全部使用的Java吗？有没有考虑其他语言？

A3: 我们目前系统多数是java，有少数的python、php、C++，这个取决于业务类型，目前java这个阶段最适合我们，可能随着业务的扩展，会考虑其他语言。

Q4: 对第三方依赖保持怀疑，能否举个具体的例子说明下怎么样做？万一第三方完全不了用了怎么办

A4: 系统一般都有第三方依赖，比如数据库，三方接口等。系统开发的时候，需要对第三方保持怀疑，避免第三方出现问题时候的连锁反应，导致宕机。大家都知道系统一旦发生问题都是滚雪球的，越来越大。比如说我们扫码通道，如果只有一家扫码通道，当这家扫码通道发生问题的时候是没有任何办法的，所以一开始就对它表示怀疑，通过接入多家通道，如果一旦发生异常，实时监控系统触发报警后就自动进行路由通道切换，保证服务的可用性；其二，针对不同的支付类型、商户、交易类型做异步消息拆分，确保如果一旦有一种类型的交易发生不可预估的异常后，从而不会影响到其他通道，这个就好比高速公路多车道一样，快车和慢车道互不影响。其实总体思路就是容错+拆分+隔离，这个具体问题具体对待。

**Q5: 支付超时后，会出现网络问题，会不会存在钱已付，订单丢失，如何做容灾及数据一致性，又有没重放日志，修过数据？**

A5：做支付最重要的就是安全，所以针对订单状态我们都是保守处理策略，因此对于网络异常的订单我们都是设置处理中状态，然后最终通过主动查询或者被动接受通知来完成和银行或者三方的最终一致性。支付系统中，除了订单状态还有响应码问题，大家都知道银行或者三方都是通过响应码来响应的，响应码和订单状态的翻译也是一定要保守策略，确保不会出现资金多付少付等问题。总之这个点的总体思路是，资金安全第一，所有的策略都是白名单原则。

**Q6: 刚才提到过，若某支付通道超时，路由策略会分发至另一通道，根据那个通道图可看出，都是不同的支付方式，比如支付宝或微信支付，那如果我只想通过微信支付，为啥不是重试，而要换到另一通道呢？还是通道本身意思是请求节点？**

A6：首先针对超时不可以做重路由，因为socket timeout是不能确定这笔交易是否发送到了三方，是否已经成功或者失败，如果是成功了，再重试一遍如果成功，针对付款就是多付，这种情况的资金损失对公司来说不可以的； 其次，针对路由功能，需要分业务类型，如果是单笔代收付交易，用户是不关心钱是哪个通道出去的，是可以路由的，如果是扫码通道，用户如果用微信扫码，肯定最终是走微信，但是我们有好多中间渠道，微信是通过中间渠道出去的，这里我们可以路由不同的中间渠道，这样最终对于用户来说还是微信支付。

**Q7: 能否举例说下自动修复的过程？如何发现不稳定到重路由的细节？**

A7: 自动修复也就是通过重路由做容错处理，这个问题非常好，如果发现不稳定然后去决策重路由。重路由一定是明确当前被重路由的交易没有成功才可以路由，否则就会造成多付多收的资金问题。我们系统目前重路由主要是通过事后和事中两种方式来决策的，针对事后比如5分钟之内通过实时预警系统发现某个通道不稳定，那么就会把当期之后的交易路由到别的通道；针对事中的，主要是通过分析每笔订单返回的失败响应码，响应码做状态梳理，明确可以重发的才做重路由。这里我指列举这两点，其他的业务点还非常多，鉴于篇幅原因，不做详述，但是总体思路是必须有一个内存实时分析系统，秒级决策，这个系统必须快，然后结合实时分析和离线分析做决策支撑，我们的实时秒级预警系统就做这个事情。

**Q8: 商户促销有规律吗？促销时峰值与平时相比会有多少差别？有技术演练么？降级的优先级是怎样的？**

A8： 商户促销一般我们会事先经常和商户保持沟通，事先了解促销的时间点和促销量，然后针对性做一些事情；促销峰值和平时差距非常大，促销一般都是2个小时之内的比较多，比如有的卖理财产品，促销也就集中在1个小时之内，所以峰值非常高；技术演练是我们在了解商户的促销量，然后预估系统的处理能力，然后提前做演练；降级的优先级主要是针对商户的，由于接入我们的商户支付场景比较多的，有理财，有代收付，有快捷，有扫码等等，所以我们整体原则就是不同的商户之间一定不可以相互影响，因为不能因为你家做促销影响了其他商家。

Q9：rsyslog归集日志怎么存储的？

A9: 这个是好问题，刚开始我们的日志也就是订单轨迹log是记录在数据库表中的，结果发现一笔订单流转需要好多模块，这样一笔订单的日志轨迹就是10笔左右，如果一天400w笔交易的话，这张数据库表就有问题了，就算拆分也是会影响数据库性能的，并且这个属于辅助业务，不应该这样做。然后，我们发现写日志比写数据库好，所以把实时日志打印成表格的形式，打印到硬盘上，这块由于只是实时日志所以日志量不大，就是在日志服务器的一个固定目录下。由于日志都是在分布式机器上，然后通过归集日志到一个集中的地方，这块是通过挂载存储的，然后有专门运维团队写的程序去实时解析这些表格形式的日志，最终通过可视化页面展示到运营操作页面，这样运营人员看到的订单轨迹几乎是实时的，您关心的怎么存储实际上不是啥问题，因为我们分了实时日志和离线日志，然后超过一定时间的离线日志会切割，最终被删除。

Q10: 系统监控和性能监控如何配合的？

A10：我理解的系统监控包括了系统性能监控，系统性能监控是系统整体监控的一部分，不存在配合问题，系统性能监控有多个维度，比如应用层面，中间件，容器等。系统的非业务监控可以查看文章分享。



## 二维码支付原理分析



**2  二维码简介**

先来简单说说二维码：二维码是用一定规则排布的点阵的图像来编码信息的方式。与二维码对应的是传统的“条码”（一维码）。

和“条码”一样，二维码具有如下特点：

1. 容易生成
2. 容易被机器识别

但是“二维码”具有更多的优点：

1. 高容错性
2. 搞污损能力
3. 高密度的信息承载能力

二维码曾被腾讯公司总裁 **马化腾** 誉为：连接线上和线下的通道。

随着支付宝，微信，微博等厂商的大力支持和推广，二维码的应用已经逐渐成为生活中随处可见的应用图案了。

当然，大家最熟悉的使用场景肯定是：移动支付。也是本期重点讨论的领域。

**3  支付场景**

- 身份二维码
- 收款二维码
- 付款二维码

大家可以使用第三方应用扫描微信或者支付宝提供的二维码，可以获取其中代表的含义。比如：两种应用互扫二维码。

**3.1  身份二维码**

**微信** 身份二维码：

```javascript
http://weixin.qq.com/r/L-rg_G-EbIITrZub0097
```

**支付宝** 身份二维码：

```javascript
https://qr.alipay.com/apa2uu7j3tpjyxlr00
```

不难看出，身份二维码实际上有用的信息就是指定的URL后面的一个串号。这个串号具有如下特点：

1. 一直固定不变
2. 无法通过此串号获取用户信息
3. 仅能被自己的app识别其深层含义（自家app查询自家数据库），客户app扫码后，将弹出相应的显示详细身份或者加好友的界面

这样很好地兼顾了 **隐私性** 和 **开放性** 。

**3.2  收款二维码**

使用UC来扫微信和支付宝的收款二维码。

微信：

```javascript
https://wx.tenpay.com/f2f?t=AQAAAEBfhXKNRIQUrs6fy4XO8p879
```

支付宝：

```javascript
https://d.alipay.com/i/index.htm?b=RECEIVE_AC&u=mGnPJ/rNBfKKKKKDcQlNGn1mthWAVDa7vw00ow5sM4o=
```

明显看出，换了一个API，同时后面带上一串和用户账号无关串号。此串号具有如下特点：

1. 一直固定不变
2. 无法通过此串号获取用户信息
3. 仅能被自己的app识别其深层含义（自家app查询自家数据库），被客户app扫描后，客户app直接调出向对方账号付款的界面

**3.3  付款二维码**

在付款二维码上，**微信** 和 **支付宝** 是差不多的，都是一串每分钟就会变一次的一串数字：

```javascript
284308793673642130
```

此二维码信息具有如下特点：

1. 是一串不带API的纯数字串
2. 每分钟变一次
3. 通过指定的SDK以此数字为参数进行接口调用可以完成扣款

其实上本质上就是一个付款账号。然后扫码时自动输入这个串号，通过第三方客户端调用支付平台SDK即可以完成扣款。

此扣款场景及规则如下：支付平台默认只要用户主动出具了二维码，就表明进行了授权扣款，这有点类似于在校园卡在食堂的作用一样，小额交易免除了繁琐的授权流程了。

关于付款二维码和之前的二维码的区别如下：

1. **永久不变** 和 **每分钟必变**
2. **API+参数** 和 **纯参数**

关于第一点的解释，笔者在此插播一个现实生活中的小故事：

在某早餐店， 笔者问店主：为何不做个二维码放墙上？ 店主说：那玩意经常变，我们就不知道怎么弄了。 笔者笑：你说的是付款二维码，那东西如果不变，传播出去后，你的钱会被人随便扣，但是你的收款二维码，你是不是还会担心别人随便给你转钱呢？ 店主立刻明白了，笑：当然不会，别人不断给我汇款，我高兴都还来不及呢。

所以，用户只是担心自己的钱可能被不知情的情况下被划走，但是肯定是不会担心别人给自己汇款的，这就很好的解释了 “不变” 和 “变” 的区别了。

关于第二点的解释：

1. app上的扫码所对应的场景众多，必须要做一定的区分，所以带上API名称
2. 条码枪对应的场景单一，仅仅只是扣款，所以二维码只需要对参数进行编码即可

## 付款码背后的原理

以支付宝为例，具体用户端支付流程如下：



![img](http://i9.taou.com/maimai/p/23364/9489_21_9ic9mjCeayiejwL)



付款码支付后台调用流程如下：



![img](http://i9.taou.com/maimai/p/23364/9492_21_26WIg9GHyxKzvQZH)



## 付款码支付详细版流程

微信/支付宝付款码支付调用流程大同小异，官网写的都比较清楚，这里直接用支付宝的官网的流程。



![img](http://i9.taou.com/maimai/p/23364/9497_21_78AsItyRtO3Wy37l)



从上面的流程可以看到，付款码支付可以说是一个同步的接口，即接口同步返回扣款结果，无需通过另外异步通知获取结果。

不过这里我们需要注意，由于涉及安全风控等问题，付款码支付过程用户端可能需要输入密码确认支付，此时付款码接口将会返回等待用户支付。

接入时务必这正确判断返回信息，若返回以下结果，代表此时用户正在输入密码。

- 微信支付: err_code=USERPAYING 或 err_code=SYSTEMERROR
- 支付宝： code=10003 或 code=20000

微信付款码支付在以下情况需要输入密码二次确认。



![img](http://i9.taou.com/maimai/p/23364/9494_21_4jNCEceLwOttkXrV)



支付宝官方文档暂未找到相关规则，经过测试当支付金额大于 2000 ，需要输入密码。如果有熟悉其他验密规则的同学，可以在评论区留言一下。

另外一点需要注意的是，微信/支付宝其他支付接口，支付成功之后，微信/支付宝服务端将会发送消息通知支付结果。但是付款码不一样，该接口是不会有消息通知。

所以如果付款码支付若返回等待用户输入密码，商家后台服务必须定时调用调用微信支付/支付宝查询接口，获取支付结果。



![img](http://i9.taou.com/maimai/p/23364/9499_21_eiYw2FCo30eeDhx)



## 撤销支付

如果在一段时间内比如 30s，轮询查询支付结果返回都是等待用户支付，或者支付交易过程返回失败或支付系统超时，这两种情况官方文档都是建议立刻调用撤销接口撤销交易。

如果此订单用户支付失败，撤销接口将会订单关闭；如果用户支付成功，撤销接口将会订单资金退还给用户。

也就是说撤销支付接口功能上等同与关闭订单加上退款。虽然撤销也具有退款功能，但是两者存在比较大的区别：

支付类型限制

微信/支付宝撤销支付仅能撤销付款码支付类型的订单，而退款可以支持多种支付类型的订单。

退款金额

撤销接口只能是全额退款，而退款接口支持传入金额，可以全额退款，也可以部分退款。

时间限制

撤销接口时间限制比较短，比如微信支付撤销支持 7 天内的订单，而支付宝撤销接口仅支持当天的订单。

但是退款接口可以支持较长时间订单退款，比如微信支付退款支持一年内的订单，而支付宝仅支持 3 个月内订单。

基于以上区别，其他正常支付的单如需实现相同功能请调用退款接口，官方文档建议仅在异常的情况下才建议调用撤销支付接口。

另外再说一点，有些地方这个功能接口称为冲正接口，如下面工商二维码支付。



![img](http://i9.taou.com/maimai/p/23364/9490_21_1o4tTu8uB815Hrxp)



实际上提供的功能与微信/支付宝撤销类似，这里需要各家支付公司提供文档具体研究。

## 撤销支付相关问题

由于撤销支付，可能导致退款，也可能关闭订单，接入之前一直有些问题弄不清楚，在官方文档处也没有查询到任何资料，没办法只好实测验证相关问题。

> 由于规定，支付机构不能直连微信/支付宝,所以以下测试基于银联微信/支付宝通道。
> 银联提供的接口与直连微信/支付宝存在些许差别，但是主要功能一样。

## 重复撤销

通过实测，微信/支付宝撤销接口幂等实现，重复撤销返回结果一致。

不过需要注意需要正确判断撤销的返回结果。

比如微信撤销接口成功判断还需要结合 recall 字段，支付宝也有类似字段。



![img](http://i9.taou.com/maimai/p/23364/9496_21_61FQ1RMZudcY8nTd)



## 订单状态

微信/支付宝订单状态处理不太一致，微信订单状态比较复杂：



![img](http://i9.taou.com/maimai/p/23364/9493_21_3dRiX1swxeBDV3dh)



也就是说，付款码订单一旦被撤销成功，再次查询订单，状态将会返回为已撤销（REVOKED）。

另外微信对于付款码支付订单有限制，是无法调用关闭订单接口关闭订单，所以在付款码的场景中，是不存在订单状态为 CLOSED—已关闭。

接下来说下支付宝的状态，支付宝文档没要给出类似的订单状态机，我根据官方一些文档，以及一些测试结果总结出下方订单状态图。



![img](http://i9.taou.com/maimai/p/23364/9495_21_5nJnlX06vjkPJLF1)



所以支付宝的付款码订单一旦撤销成功，再次查询原单状态将会返回 TRADE_CLOSED。

## 对账文件数据

当天产生交易之后，次日我们需要拉取微信/支付宝对账文件，逐一核对数据，防止少账，多账问题。

对账设计流程可以参考之前写过的文章：

聊聊对账系统的设计方案

微信/支付宝对账文件只会记录交易成功的订单，所以未支付的订单被撤销是不会出现在对账文件中。但是如果支付成功了，然后又被撤销成功，将会在对账文件中产生两笔记录，一笔正交易，一笔反向退款记录。

正交易与普通的退款的记录都比较好识别，一般可以使用我们上送给微信支付宝订单号。但是撤销导致退款记录，我们无法仅用一个单号识别，我们需要结合另外的字段区分判断。

微信对账文件撤销产生那笔退款，交易状态为 REVOKED，所以我们可以采用商户订单号加交易状态识别出一条记录是否为撤销产生退款记录。



![img](http://i9.taou.com/maimai/p/23364/9500_21_1pa6ExOcAHUhDuR7)



> 上面银联订单号可以当做是微信支付宝内部产生订单号

支付宝对账文件比较麻烦，撤销产生的退款记录不能跟微信根据交易状态区分。从对账文件上看支付宝撤销产生退款与普通退款接口产生退款记录是一样的。



![img](http://i9.taou.com/maimai/p/23364/9498_21_8bwWpNkzrjUbX4lX)



仔细研究对账文件可以发现一些区别，撤销导致退款记录退款批次与正交易支付宝内部订单号是一致的。而正常退款记录，退款批次号是由商户自己上送的。所以我们可以以此筛选出撤销产生的退款记录。

## 撤销失败

极端情况下，有可能产生多次撤销都失败的奇葩情况，那怎么办？

这种情况下就不用往系统自动处理方向考虑了，通过线下人工介入处理吧，毕竟这种概率太低了。引用知乎**@天顺**的文章中一句话：

> 很多时候人工保障比你动脑筋想异常中的异常如何系统自动处理来得反而高效和低成本

这句话大家仔细品，越品越有道理！

## Reference

异步通知如何判断对应哪笔退款交易

对账与退款

撤销接口retry_flag字段说明

退款接口、关闭接口和撤销接口的区别

## 最后说一句（求点赞）

付款码支付接入其实比较简单，主要难点在于撤销接口引入之后对于现有的系统的改造，比如撤销成功的订单之后，是直接修改原单的成功状态到撤销状态，还是说再创建一条撤销记录？还有对账系统核对时，对端记录可能比本端多，如何核对？这些问题大家在接入之后一定结合现有系统好好思考一下。