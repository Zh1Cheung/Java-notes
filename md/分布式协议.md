## CAP理论

- 一致性（Consistency）
  - 一致性说的是客户端的每次读操作，不管访问哪个节点，要么读到的都是同一份最新的数据，要么读取失败。
  - 你可以把一致性看作是分布式系统对访问本系统的客户端的一种承诺：不管你访问哪个节点，要么我给你返回的都是绝对一致的数据，要么你都读取失败。你可以看到，一致性强调的不是数据完整，而是各节点间的数据一致。
- 可用性（Availability）
  - 可用性说的是任何来自客户端的请求，不管访问哪个节点，都能得到响应数据，但不保证是同一份最新数据。你也可以把可用性看作是分布式系统对访问本系统的客户端的另外一种承诺：我尽力给你返回数据，不会不响应你，但是我不保证每个节点给你的数据都是最新的。这个指标强调的是服务可用，但不保证数据的一致
- 分区容错性（Partition Tolerance）
  - 当节点间出现任意数量的消息丢失或高延迟的时候，系统仍然可以继续提供服务。也就是说，分布式系统在告诉访问本系统的客户端：不管我的内部出现什么样的数据同步问题，我会一直运行，提供服务。这个指标，强调的是集群对分区故障的容错能力。
  - 当节点 1 和节点 2 通信出问题的时候，如果系统仍能提供服务，那么，2个节点是满足分区容错性的。
  - 因为分布式系统与单机系统不同，它涉及到多节点间的通讯和交互，节点间的分区故障是必然发生的，所以我要提醒你，在分布式系统中分区容错性是必须要考虑的。

## **CAP权衡**

通过CAP理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？

> CA without P：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但其实分区不是你想不想的问题，而是始终会存在，因此CA的系统更多的是允许分区后各子系统依然保持CA。
> CP without A：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。
> AP wihtout C：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。





## ACID理论

- ACID 理论是对事务特性的抽象和总结，方便我们实现事务。你可以理解成：如果实现了操作的 ACID 特性，那么就实现了事务。而大多数人觉得比较难，是因为分布式系统涉及多个节点间的操作。加锁、时间序列等机制，只能保证单个节点上操作的 ACID 特性，无法保证节点间操作的 ACID 特性。
- TCC
  - 不管是原始的二阶段提交协议，还是 XA 协议，都存在一些问题
    - 在提交请求阶段，需要预留资源，在资源预留期间，其他人不能操作（比如，XA 在第一阶段会将相关资源锁定）；
    - 在第一个阶段，每个参与者投票表决事务是放弃还是提交。一旦参与者投票要求提交事务，那么就不允许放弃事务。也就是说，在一个参与者投票要求提交事务之前，它必须保证能够执行提交协议中它自己那一部分，即使参与者出现故障或者中途被替换掉。这个特性，是我们需要在代码实现时保障的。
    - 崩溃后协调者会不断的重试，这时候这个事务使用到的相关资源都会被锁住，没办法使用，直到节点恢复。
    - 参与者在阶段二挂掉，重启之后是没办法知道别的节点是否commit还是rollback，事务处于悬挂状态，必须等待协调者的异步重试消息，来执行commit或rollback操作
  - 在TCC中，数据是最终一致。
    - 2PC用在集群间一致性数据同步，所有参与者完成的是同一件事，可以理解为它们在一个start transaction--commit里面，具有强一致性
    - TCC是对业务过程的拆分，一致性弱于2PC

- Paxos、Raft 等强一致性算法，也采用了二阶段提交操作，在“提交请求阶段”，只要大多数节点确认就可以，而具有 ACID 特性的事务，则要求全部节点确认可以。所以可以将具有 ACID 特性的操作，理解为最强的一致性。
- 事务系统缺乏节点故障容错能力，性能也是痛点。
  - 需要将提交相关信息保存到持久存储上，用于故障后恢复，超时，也要不断重试







## BASE理论

- BASE 理论是 CAP 理论中的 AP 的延伸，是对互联网大规模分布式系统的实践总结，强调可用性。
  - 几乎所有的互联网后台分布式系统都有 BASE 的支持，这个理论很重要，地位也很高。一旦掌握它，你就能掌握绝大部分场景的分布式系统的架构技巧，设计出适合业务场景特点的、高可用性的分布式系统。
  - 而它的核心就是基本可用（Basically Available）和最终一致性（Eventually consistent）。也有人会提到软状态（Soft state），在我看来，软状态描述的是实现服务可用性的时候系统数据的一种过渡状态，也就是说不同节点间，数据副本存在短暂的不一致。你只需要知道软状态是一种过渡状态就可以了，我们不多说。
- 实现基本可用的 4 板斧
  - 基本可用是说，当分布式系统在出现不可预知的故障时，允许损失部分功能的可用性，保障核心功能的可用性。
  - 在春运期间，深圳出发的火车票在 8 点开售，北京出发的火车票在 9 点开售。这就是我们常说的**流量削峰**。
  - 在春运期间，自己提交的购票请求，往往会在队列中排队等待处理，可能几分钟或十几分钟后，系统才开始处理，然后响应处理结果，这就是你熟悉的**延迟响应**。
  - **体验降级**， 比如用小图片来替代原始图片，通过降低图片的清晰度和大小，提升系统的处理能力。
  - 然后你还能想到**过载保护**， 比如把接收到的请求放在指定的队列中排队处理，如果请求等待时间超时了（假设是 100ms），这个时候直接拒绝超时请求；再比如队列满了之后，就清除队列中一定数量的排队请求，保护系统不过载，实现系统的基本可用。

- 最终的一致
  - 在数据一致性上，存在一个短暂的延迟
  - 一般来说，在实际工程实践中有这样几种方式：
    - 以最新写入的数据为准，比如 AP 模型的 KV 存储采用的就是这种方式；
    - 以第一次写入的数据为准，如果你不希望存储的数据被更改，可以以它为准。
  - 在这里，我想强调的是因为写时修复（在写入数据，检测数据的不一致时，进行修复）不需要做数据一致性对比，性能消耗比较低，对系统运行影响也不大，所以我推荐你在实现最终一致性时优先实现这种方式。而读时修复（在读取数据时，检测数据的不一致，进行修复）和异步修复（这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复）因为需要做数据的一致性对比，性能消耗比较多，在开发实际系统时，你要尽量优化一致性对比的算法，降低性能消耗，避免对系统运行造成影响。
  - 在实现最终一致性的时候，我推荐同时实现自定义写一致性级别（All、Quorum、One、Any）， 让用户可以自主选择相应的一致性级别，比如可以通过设置一致性级别为 All，来实现强一致性。
- 从微服务的角度来考虑, 有这些方式能够尽可能地保证系统的基本可用:
  - 使用消息队列, 对偶然的高并发写操作进行削峰填谷;
    - MQ能很好的弥补ES写性能差、支持突发流量能力弱的痛点。
  2. 对进程间的服务调用做好熔断保护;
  3. 在系统能力无法支撑高并发访问时, 对非核心业务降级;
  4. 对关键服务做好限流.



## Paxos算法

- Paxos 算法包含 2 个部分
  - 一个是 Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；
  - 另一个是 Multi-Paxos 思想，描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。
  - 在我看来，Basic Paxos 是 Multi-Paxos 思想的核心，说白了，Multi-Paxos 就是多执行几次 Basic Paxos
- 你需要了解的三种角色
  - 提议者（Proposer）：提议一个值，用于投票表决。在绝大多数场景中，集群中收到客户端请求的节点，才是提议者
  - 接受者（Acceptor）：对每个提议的值进行投票，并存储接受的值，比如 A、B、C 三个节点。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受和存储数据
  - 学习者（Learner）：被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，被动地接受数据，容灾备份
- 如何达成共识
  - 为了方便演示，我使用[n, v]表示一个提案，其中 n 为提案编号，v 为提议值。
  - 准备（Prepare）阶段
    - 你要注意，在准备请求中是不需要指定提议的值的，只需要携带提案编号就可以了
    - 先来看第一个阶段，首先客户端 1、2 作为提议者，分别向所有接受者发送包含提案编号的准备请求
      - 接着，当节点 A、B 收到提案编号为 1 的准备请求，节点 C 收到提案编号为 5 的准备请求后，将进行下面这样的处理
      - 由于之前没有通过任何提案，所以节点 A、B 将返回一个 “尚无提案”的响应。也就是说节点 A 和 B 在告诉提议者，我之前没有通过任何提案呢，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案。
      - 节点 C 也是如此，它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案
  - 接受（Accept）阶段
    - 第二个阶段也就是接受阶段，首先客户端 1、2 在收到大多数节点的准备响应之后，会分别发送接受请求
    - 当节点 A、B、C 收到接受请求[1, 3]的时候，由于提案的提案编号 1 小于三个节点承诺能通过的提案的最小提案编号 5，所以提案[1, 3]将被拒绝。
    - 当节点 A、B、C 收到接受请求[5, 7]的时候，由于提案的提案编号 5 不小于三个节点承诺能通过的提案的最小提案编号 5，所以就通过提案[5, 7]，也就是接受了值 7，三个节点就 X 值为 7 达成了共识。
  - 通过上面的演示过程，你可以看到，最终各节点就 X 的值达成了共识。那么在这里我还想强调一下，Basic Paxos 的容错能力，源自“大多数”的约定，你可以这么理解：当少于一半的节点出现故障的时候，共识协商仍然在正常工作。
  - 如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中，包含已经通过的最大编号的提案信息
- 注意到这么两点
  - 首先在Basic Paxos的准备阶段，是会发现之前通过的提案的值；
  - 另外，Basic Paxos是一个共识算法，就一个值达成共识后，共识的这个值，就不会再变了。
- 如果我们直接通过多次执行 Basic Paxos 实例，来实现一系列值的共识，就会存在这样几个问题：
  - 如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，一个 5 节点的集群，如果 3个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商
  - 2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。你要知道，分布式系统的运行是建立在 RPC 通讯的基础之上的，因此，延迟一直是分布式系统的痛点，是需要我们在开发分布式系统时认真考虑和优化的。
- 我们可以通过引入领导者节点，也就是说，领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了
  - 我们可以采用“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，优化 Basic Paxos 执行。也就是说，领导者节点上，序列中的命令是最新的，不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段
- Chubby 的 Multi-Paxos 实现
  - 在 Chubby 中，主节点是通过执行 Basic Paxos 算法，进行投票选举产生的，并且在运行过程中，主节点会通过不断续租的方式来延长租期（Lease）。比如在实际场景中，几天内都是同一个节点作为主节点。如果主节点故障了，那么其他的节点又会投票选举出新的主节点，也就是说主节点是一直存在的，而且是唯一的。
  - Chubby 只能在主节点上执行读操作，这个设计有什么局限呢？
    - 只能在主节点进行读操作，效果相当于单机，对吞吐量和性能有所影响
    - 写也是在主节点进行，性能也有问题







## Raft算法

- 概述

  - Raft 算法属于 Multi-Paxos 算法，它是在兰伯特 Multi-Paxos 思想的基础上，做了一些简化和限制，比如增加了日志必须是连续的，只支持领导者、跟随者和候选人三种状态，在理解和算法实现上都相对容易许多。
  - 除此之外，Raft 算法是现在分布式系统开发首选的共识算法。
  - 掌握这个算法，可以得心应手地处理绝大部分场景的容错和一致性需求，比如分布式配置系统、分布式 NoSQL 存储等等，轻松突破系统的单机限制。
  - 如果要用一句话概括 Raft 算法：从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致。

- 选举领导者的过程

  - Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。假设节点 A 的等待超时时间最小（150ms），它会最先因为没有等到领导者的心跳信息，发生超时。
    - 这个时候，节点 A 就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者。
    - 如果其他节点接收到候选人 A 的请求投票 RPC 消息，在编号为 1 的这届任期内，也还没有进行过投票，那么它将把选票投给节点 A，并增加自己的任期编号
    - 如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

- 节点间是如何通讯的

  - 在 Raft 算法中，服务器节点间的沟通联络采用的是远程过程调用（RPC），在领导者选举中，需要用到这样两类的 RPC：
    - 请求投票（RequestVote）RPC，是由候选人在选举期间发起，通知各节点进行投票；
    - 日志复制（AppendEntries）RPC，是由领导者发起，用来复制日志和提供心跳消息。

- 任期

  - 我想强调的是，与现实议会选举中的领导者的任期不同，Raft 算法中的任期不只是时间段，而且任期编号的大小，会影响领导者选举和请求的处理。
    - 在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小，那么它会立即恢复成跟随者状态。比如分区错误恢复后，任期编号为 3 的领导者节点B，收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟随者状态
    - 还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那么它将拒绝这个消息

- 选举有哪些规则

  - 在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。
  - 其实在选举中，除了选举规则外，我们还需要避免一些会导致选举失败的情况，比如同一任期内，多个候选人同时发起选举，导致选票被瓜分，选举失败。那么在 Raft 算法中，如何避免这个问题呢？答案就是随机超时时间。
  - 随机超时时间
    - 其实，Raft 算法巧妙地使用随机选举超时时间的方法，把超时时间都分散开来，在大多数情况下只有一个服务器节点先发起选举，而不是同时发起选举，这样就能减少因选票瓜分导致选举失败的情况。
    - 在 Raft 算法中，随机超时时间是有 2 种含义的
      - 跟随者等待领导者心跳信息超时的时间间隔，是随机的；
      - 当没有候选人赢得过半票数，选举无效了，这时需要等待一个随机时间间隔，也就是说，等待选举超时的时间间隔，是随机的。

- 日志

  - 在 Raft 算法中，副本数据是以日志的形式存在的，领导者接收到来自客户端写请求后，处理写请求的过程就是一个复制和提交日志项的过程。

    - 日志项是一种数据格式，它主要包含用户指定的数据，也就是指令，还包含一些附加信息，比如索引值、任期编号。

  - 如何复制日志

    - 首先，领导者进入第一阶段，通过日志复制（AppendEntries）RPC 消息，将日志项复制到集群其他节点上。
    - 接着，如果领导者接收到大多数的“复制成功”响应后，它将日志项提交到它的状态机，并返回成功给客户端。如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给客户端
      - 领导者将日志项提交到它的状态机，怎么没通知跟随者提交日志项呢？
      - 这是 Raft 中的一个优化，领导者不直接发送消息通知其他节点提交指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交的日志项索引值。所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。
      - 因此，当其他节点接受领导者的心跳消息，或者新的日志复制 RPC 消息后，如果跟随者发现领导者已经提交了某条日志项，而它还没提交，就会将这条日志项提交到它的状态机。而这个优化，降低了处理客户端请求的延迟，将二阶段提交优化为了一段提交，降低了一半的消息延迟。

  - 如何实现日志的一致

    - 具体有 2 个步骤

      - 首先，领导者通过日志复制 RPC 的一致性检查，找到跟随者节点上，与自己相同日志项的最大索引值。也就是说，这个索引值之前的日志，领导者和跟随者是一致的，之后的日志是不一致的了。
      - 然后，领导者强制跟随者更新覆盖的不一致日志项，实现日志的一致。

    - > PrevLogEntry：表示当前要复制的日志项，前面一条日志项的索引值。
      > PrevLogTerm：表示当前要复制的日志项，前面一条日志项的任期编号
      >
      > 
      >
      > 领导者通过日志复制 RPC 消息，发送当前最新日志项到跟随者（为了演示方便，假设当前需要复制的日志项是最新的），这个消息的 PrevLogEntry 值为 7，PrevLogTerm 值为 4。
      > 如果跟随者在它的日志中，找不到与 PrevLogEntry 值为 7、PrevLogTerm 值为 4 的日志项，也就是说它的日志和领导者的不一致了，那么跟随者就会拒绝接收新的日志项，并返回失败信息给领导者。
      > 这时，领导者会递减要复制的日志项的索引值，并发送新的日志项到跟随者，这个消息的 PrevLogEntry 值为 6，PrevLogTerm 值为 3。

- 如何通过单节点变更解决成员变更的问题

  - 单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。比如将 3 节点集群扩容为 5 节点集群，这时你需要执行 2 次单节点变更，先将 3 节点集群变更为 4 节点集群，然后再将 4 节点集群变更为 5 节点集群

- 有什么办法能突破 Raft 集群的写性能瓶颈

  - leader可以合并请求
  - leader提交日志和日志复制RPC两个步骤可以并行和批量处理
  - 每个节点启动多个raft实例，对请求进行hash或者range后，让每个raft实例负责部分请求







## 一致哈希算法

- 一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而一致哈希算法是对 2^32 进行取模运算。你可以想象下，一致哈希算法，将整个哈希值空间组织成一个虚拟的圆环，也就是哈希环
  - 首先，将 key 作为参数执行hash() 计算哈希值，并确定此 key 在环上的位置；
  - 然后，从这个位置沿着哈希环顺时针“行走”，遇到的第一节点就是 key 对应的节点。
- 在一致哈希中，如果节点太少，容易因为节点分布不均匀造成数据访问的冷热不均，也就是说大多数访问请求都会集中少量几个节点上
  - 其实，就是对每一个服务器节点计算多个哈希值，在每个计算结果位置上，都放置一个虚拟节点，并将虚拟节点映射到实际节点。比如，可以在主机名的后面增加编号，分别计算“Node-A-01”“Node-A-02”“Node-B-01”“Node-B-02”“Node-C-01”“Node-C-02”的哈希值，于是形成 6 个虚拟节点
  - 这时，如果有访问请求寻址到“Node-A-01”这个虚拟节点，将被重定位到节点 A。
- 一致哈希是一种特殊的哈希算法，在使用一致哈希算法后，节点增减变化时只影响到部分数据的路由寻址，也就是说我们只要迁移部分数据，就能实现集群的稳定了。
- 为什么一个节点可以算出多个hash值
  - 引入虚拟节点，比如，将包含主机名和虚拟节点编号的字符串，作为参数，来计算哈希值。

作者：2020Labs

链接：https://zhuanlan.zhihu.com/p/135248487

来源：知乎

著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 一、哈希表概述

哈希表的哈希函数输入一个键，并向返回一个哈希表的索引。可能的键的集合很大，但是哈希函数值的集合只是表的大小。

哈希函数的其他用途包括密码系统、消息摘要系统、数字签名系统，为了使这些应用程序按预期工作，冲突的概率必须非常低，因此需要一个具有非常大的可能值集合的散列函数。

密码系统:给定用户密码，操作系统计算其散列，并将其与存储在文件中的该用户的散列进行比较。(不要让密码很容易被猜出散列到相同的值)。

消息摘要系统:给定重要消息，计算其散列，并将其与消息本身分开发布。希望检查消息有效性的读者也可以使用相同的算法计算其散列，并与发布的散列进行比较。(不要希望伪造消息很容易，仍然得到相同的散列)。

这些应用的流行哈希函数算法有:

- **md5 :** 2^128个值（找一个冲突键，需要哈希大约2 ^ 64个值）
- **sha-1：**2^160个值（找一个冲突键，需要大约2^80个值）

## 二、哈希冲突

来看一个简单的实例吧，假设采用hash函数：H(K) = K mod M，插入这些值：217、701、19、30、145

> H(K) = 217 % 7 = 0
> H(K) = 701 % 7 = 1
> H(K) = 19 % 7 = 2
> H(K) = 30 % 7 = 2
> H(K) = 145 % 7 = 5

![img](https://pic2.zhimg.com/v2-bc641b055188ed91ab20cab1ace8acbf_b.jpg)

上面实例很明显 19 和 30 就发生冲突了。

## 三、冲突解决策略

除非您要进行“完美的散列”，否则必须具有冲突解决策略，才能处理表中的冲突。
同时，该策略必须允许查找，插入和删除正确运行的操作！

冲突解决技术可以分为两类：开散列方法( open hashing，也称为拉链法，separate chaining )和闭散列方法( closed hashing，也称为开地址方法，open addressing )。这两种方法的不同之处在于：开散列法把发生冲突的关键码存储在散列表主表之外，而闭散列法把发生冲突的关键码存储在表中另一个槽内。

下面介绍业内比较流行的hash冲突解决策略：

- **线性探测(Linear probing)**
- **双重哈希(Double hashing)**
- **随机散列(Random hashing)**
- **分离链接(Separate chaining)**

上面线性探测、双重哈希、随机散列都是闭散列法，而分离链接则是开散列法。

### **1、线性探测(Linear probing)**

**插入一个值**

使用散列函数H（K）在大小为M的表中插入密钥K时：

1. 设置 indx = H（K）
2. 如果表位置indx已经包含密钥，则无需插入它。Over
3. 否则，如果表位置indx为空，则在其中插入键。Over
4. 其他碰撞。设置 indx =（indx + 1）mod M.
5. 如果 indx == H（K），则表已满！就只能做哈希表的扩容了

因此，线性探测基本上是在发生碰撞时对空槽进行线性搜索。

**优点：**易于实施；总是找到一个位置（如果有）；当表不是很满时，平均情况下的性能非常好。

**缺点：**表的相邻插槽中会形成“集群”或“集群”键；当这些簇填满整个阵列的大部分时，性能会严重下降，因为探针序列执行的工作实际上是对大部分阵列的穷举搜索。

**简单例子**

如哈希表大小M = 7, 哈希函数：H(K) = K mod M。插入这些值：701, 145, 217, 19, 13, 749

> H(K) = 701 % 7 = 1
> H(K) = 145 % 7 = 5
> H(K) = 217 % 7 = 0
> H(K) = 19 % 7 = 2
> H(K) = 13 % 7 = 1(冲突) --> 2(已经有值) --> 3(插入位置3)
> H(K) = 749 % 7 = 2(冲突) --> 3(已经有值) --> 4(插入位置4)

可见，如果哈希表如果不是很大，随着数据插入，冲突也会组件发生，探针遍历次数将会逐渐变低，检索过程也就成为穷举。

**检索一个值**

如果使用线性探测将键插入表中，则线性探测将找到它们！

当使用散列函数 H（K）在大小为N的表中搜索键K时：

1. 设置 indx = H（K）
2. 如果表位置indx包含键，则返回FOUND。
3. 否则，如果表位置 indx 为空，则返回NOT FOUND。
4. 否则设置 indx =（indx + 1）modM。
5. 如果 indx == H（K），则返回NOT FOUND。就只能做哈希表的扩容了

**问题：如何从使用线性探测的表中删除键？**

能否进行“延迟删除”，而只是将已删除密钥的插槽标记为空？ 

> 很明显，在线性探测很难做到，如果把位置置为空，那么如果后面的值也是哈希冲突，线性探测插入，则再也无法遍历这些值了。

### **2、双重哈希(Double hashing)**

线性探测冲突解决方案会导致表中出现簇，因为如果两个键发生碰撞，则探测到的下一个位置对于这两个键都是相同的。

双重哈希的思想：使偏移到下一个探测到的位置取决于键值，因此对于不同的键可以不同。

需要引入第二个哈希函数 H 2（K），用作探测序列中的偏移量（将线性探测视为 H 2（K）== 1 的双重哈希）。

对于大小为 M 的哈希表，H 2（K）的值应在 1到M-1 的范围内；如果M为质数，则一个常见选择是 H2（K）= 1 +（（K / M）mod（M-1））。

然后，用于双哈希的插入算法为：

1. 设置 indx = H（K）; offset = H 2（K）
2. 如果表位置indx已经包含密钥，则无需插入它。Over
3. 否则，如果表位置 indx 为空，则在其中插入键。Over
4. 其他碰撞。设置 indx =（indx + offset）mod M.
5. 如果 indx == H（K），则表已满！就只能做哈希表的扩容了

哈希表为质数情况，双重hash在实践中非常有效

双重 Hash 也见：[https://blog.csdn.net/chenxuegui1234/article/details/103454285](https://link.zhihu.com/?target=https%3A//blog.csdn.net/chenxuegui1234/article/details/103454285)

### **3、随机散列(Random hashing)**

与双重哈希一样，随机哈希通过使探测序列取决于密钥来避免聚类。

使用随机散列时，探测序列是由密钥播种的伪随机数生成器的输出生成的（可能与另一个种子组件一起使用，该组件对于每个键都是相同的，但是对于不同的表是不同的）。

然后，用于随机哈希的插入算法为：

1. 创建以 K 为种子的 RNG。设置indx = RNG.next() mod M。
2. 如果表位置 indx 已经包含密钥，则无需插入它。Over
3. 否则，如果表位置 indx 为空，则在其中插入键。Over
4. 其他碰撞。设置 indx = RNG.next() mod M.
5. 如果已探测所有M个位置，则放弃。就只能做哈希表的扩容了。

随机散列很容易分析，但是由于随机数生成的“费用”，它并不经常使用。双重哈希在实践中还是经常被使用。

### **4、分离链接(Separate chaining)**

在具有哈希函数 H（K）的表中插入键K时

1. 设置 indx = H（K）
2. 将关键字插入到以 indx 为标题的链接列表中。（首先搜索列表，以避免重复。）

在具有哈希函数H（K）的表中搜索键K时

1. 设置 indx = H（K）
2. 使用线性搜索在以 indx 为标题的链表中搜索关键字。

使用哈希函数 H（K）删除表中的键K时

1. 设置 indx = H（K）
2. 删除链接列表中以 indx 为标题的键

**优点：**随着条目数量的增加，平均案例性能保持良好。甚至超过M；删除比开放地址更容易实现。

**缺点：**需要动态数据，除数据外还需要存储指针，本地性较差，导致缓存性能较差。

很明显，Java7 的 HashMap 就是一种分裂链接的实现方式。

**分离链哈希分析**

请记住表的填充程度的负载系数度量：α = N / M。

其中M是表格的大小，并且 N 是表中已插入的键数。

通过单独的链接，可以使 α> 1 给定负载因子α，我们想知道在最佳，平均和最差情况下的时间成本。

**成功找到**

新键插入和查找失败（这些相同），最好的情况是O（1），最坏的情况是O（N）。让我们分析平均情况

**分裂链接的平均成本**

假设负载系数为 α = N / M 的表
在M个链接列表中总共分配了N个项目（其中一些可能为空），因此每个链接列表的平均项目数为：

- 如果查找/插入失败，则必须穷举搜索表中的链表之一，并且表中链表的平均长度为α。因此，使用单独链接进行插入或不成功查找的比较平均次数为

![img](https://pic3.zhimg.com/v2-c4ddfd281da85ae4adbae8c7fc53df42_b.jpeg)

- 成功查找后，将搜索包含目标密钥的链接列表。除目标密钥外，该列表中平均还有（N-1）/ M个密钥；在找到目标之前，将平均搜索其中一半。因此，使用单独链接成功找到的比较平均次数为

![img](https://pic1.zhimg.com/v2-9b827aaf292d3291c124fcd164830dea_b.jpeg)

当α<1时，它们分别小于1和1.5。并且即使当α超过1时，它们仍然是O（1），与N无关。

## 四、开散列方法 VS 闭散列方法

如果将键保留为哈希表本身中的条目，则可以使用线性探测，双重和随机哈希... 这样做称为“开放式寻址”，也称为“封闭式哈希”。

另一个想法：哈希表中的条目只是指向链表（“链”）头部的指针；链接列表的元素包含键... 这称为“单独链接”，也称为“开放式哈希”。

通过单独的链接，冲突解决变得容易：只要在其链表中插入一个键，就可以将其插入（为此，可以使用比链表更高级的数据结构；但是正如我们将看到的，链表在一般情况下效果很好）。

让下面我们看一下这些策略的时间成本。

**开放式地址哈希分析**

分析哈希表“查找”或“插入”性能时，一个有用的参数是负载系数 α = N / M。

其中 M 是表格的大小，并且 N 是表中已插入的键数负载系数是表满度的一种度量。

给定负载因子 α ，我们想知道在最佳，平均和最差情况下的时间成本。

**成功找到**
对所有键，最好的情况是O（1），最坏的情况是O（N），新键插入和查找失败（这些相同），所以让我们分析平均情况。
我们将给出随机哈希和线性探测的结果。实际上，双重哈希类似于随机哈希；

**平均不成功的查找/插入成本**

假定负载系数为α= N / M的表。考虑随机散列，因此聚类不是问题。每个探针位置是随机且独立生成的对于每个探针，找到空位置的可能性为（1-α）。查找空位置将停止查找或插入，这是一个伯努利过程，成功概率为（1-α）。该过程的预期一阶到达时间为 1 /（1-α）。所以：

使用随机哈希进行插入或不成功查找的探针的平均数量为

![img](https://picb.zhimg.com/v2-67092013ffc06ab2dbed6dd9f3b29dff_b.jpeg)

使用线性探测时，探头的位置不是独立的。团簇形成，当负载系数高时会导致较长的探针序列。可以证明，用于线性探测的插入或未成功发现的探针的平均数量约为

![img](https://pic2.zhimg.com/v2-49f5287f2e72b6e4fa3bba4be41c89cf_b.jpeg)

当 α 接近1时，这些平均案例时间成本很差，仅受M限制；但当 α 等于或小于7.75（与M无关）时，效果还不错（分别为4和8.5）

**平均成功查找成本**

假定负载系数为 α= N / M 的表。考虑随机散列，因此聚类不是问题。每个探针位置是随机且独立生成的。

对于表中的键，成功找到它所需的探针数等于将其插入表中时所采用的探针数。每个新密钥的插入都会增加负载系数，从0开始到α。

因此，通过随机散列成功发现的探测器的平均数量为

![img](https://pic4.zhimg.com/v2-8f2d44535fe8143d6c9881d6b6478b70_b.jpeg)

通过线性探测，会形成簇，从而导致更长的探针序列。可以证明，通过线性探测成功发现的平均探针数为

![img](https://pic2.zhimg.com/v2-652139119d5ba91008730b5b816c2f16_b.jpeg)

当α接近1时，这些平均案例时间成本很差，仅受M限制；但当α等于或小于7.75时好（分别为1.8和2.5），与M无关。



## 分布式 ID 的设计方案

- 首先，我们需要明确通常的分布式 ID 定义，基本的要求包括：

  - 全局唯一，区别于单点系统的唯一，全局是要求分布式系统内唯一。
  - 有序性，通常都需要保证生成的 ID 是有序递增的。例如，在数据库存储等场景中，有序ID 便于确定数据位置，往往更加高效。

- 目前业界的方案很多，典型方案包括：

  - > 整体长度通常是 64 （1 + 41 + 10+ 12 = 64）位，适合使用 Java 语言中的 long 类型来存储。
    > 头部是 1 位的正负标识位。
    > 紧跟着的高位部分包含 41 位时间戳，通常使用 System.currentTimeMillis()。
    > 后面是 10 位的 WorkerID，标准定义是 5 位数据中心 + 5 位机器 ID，组成了机器编号，以区分不同的集群节点。
    > 最后的 12 位就是单位毫秒内可生成的序列号数目的理论极限。

- Snowflake 是否受冬令时切换影响

  - 没有影响，你可以从 Snowflake 的具体算法实现寻找答案。我们知道 Snowflake 算法的 Java 实现，大都是依赖于 System.currentTimeMillis()，这个数值代表什么呢？从Javadoc 可以看出，它是返回当前时间和 1970 年 1 月 1 号 UTC 时间相差的毫秒数，这个数值与夏 / 冬令时并没有关系，所以并不受其影响。

- 我们到底需要一个什么样的分布式 ID

  - 除了唯一和有序，考虑到分布式系统的功能需要，通常还会额外希望分布式 ID 保证：
  - 有意义，或者说包含更多信息，例如时间、业务等信息。这一点和有序性要求存在一定关联，如果 ID 中包含时间，本身就能保证一定程度的有序，虽然并不能绝对保证。ID 中包含额外信息，在分布式数据存储等场合中，有助于进一步优化数据访问的效率。
  - 高可用性，这是分布式系统的必然要求。前面谈到的方案中，有的是真正意义上的分布式，有得还是传统主从的思路，这一点没有绝对的对错，取决于我们业务对扩展性、性能等方面的要求。
  - 紧凑性，ID 的大小可能受到实际应用的制约，例如数据库存储往往对长 ID 不友好，太长的 ID 会降低 MySQL 等数据库索引的性能；编程语言在处理时也可能受数据类型长度限制

- Snowflake 方案的好处是算法简单，依赖也非常少，生成的序列可预测，性能
  也非常好，比如 Twitter 的峰值超过 10 万 /s。

  - 但是，它也存在一定的不足，例如时钟偏斜问题（Clock Skew）。我们知道普通的计算机系统时钟并不能保证长久的一致性，可能发生时钟回拨等问题，这就会导致时间戳不准确，进而产生重复 ID

- Snowflake 这种基于时间的算法，从形式上天然地限制了 ID 的并发生成数量，如果在极端情况下，短时间需要更多 ID，有什么办法解决呢？

  - 因为snowflake的可预测性，可以提前生成好放到队列里，获取的时候直接获取。相当于做了一层缓存；理论上可以解决短时间大量获取id的需求；

- 69年的极限问题不难解决，timestamp减个常量就可以了，对于已生成的历史id，可以导表刷id，当然，这里涉及到个数据库设计原则，系统之间传递数据不应使用物理主键，这样刷id 就容易了





















