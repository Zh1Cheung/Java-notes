## HTTP是什么

- HTTP 是什么
  - 超文本传输协议
  - HTTP 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范
- HTTP 不是什么
  - 因为 HTTP 是一个协议，是一种计算机间通信的规范，所以它不存在“单独的实体”。
  - HTTP 不是一个孤立的协议。
    - 在互联网世界里，HTTP 通常跑在 TCP/IP 协议栈之上，依靠 IP 协议实现寻址和路由、TCP协议实现可靠数据传输、DNS 协议实现域名查找、SSL/TLS 协议实现安全通信。
    - 此外，还有一些协议依赖于 HTTP，例如 WebSocket、HTTPDNS 等。这些协议相互交织，构成了一个协议网，而 HTTP 则处于中心地位。
- CDN
  - CDN 也是现在互联网中的一项重要基础设施，除了基本的网络加速外，还提供负载均衡、安全防护、边缘计算、跨运营商网络等功能，能够成倍地“放大”源站服务器的服务能力，很多云服务商都把 CDN 作为产品的一部分



## 与HTTP相关的各种协议

- TCP/IP
  - TCP/IP 协议实际上是一系列网络通信协议的统称，其中最核心的两个协议是TCP和IP，其他的还有 UDP、ICMP、ARP 等等，共同构成了一个复杂但有层次的协议栈。
  - HTTP 是一个"传输协议"，但它不关心路由、寻址、数据完整性等传输细节，而要求这些工作都由下层来处理。因为互联网上最流行的是 TCP/IP 协议，而它刚好满足 HTTP 的要求，所以互联网上的 HTTP 协议就运行在了 TCP/IP 上，HTTP 也就可以更准确地称为“HTTP over TCP/IP”。
- DNS
  - 想要使用 TCP/IP 协议来通信仍然要使用 IP 地址，所以需要把域名做一个转换，“映射”到它的真实 IP，这就是所谓的“域名解析”。
  - HTTP 协议中并没有明确要求必须使用 DNS，但实际上为了方便访问互联网上的 Web 服务器，通常都会使用 DNS 来定位或标记主机名，间接地把 DNS 与 HTTP 绑在了一起。
- URI/URL
  - URI（Uniform Resource Identifier），中文名称是 统一资源标识符
  - URL（Uniform Resource Locator）， 统一资源定位符
  - URI 主要有三个基本的部分构成
    - http://nginx.org/en/download.html
    - 协议名：即访问该资源应当使用的协议，在这里是“http”
    - 主机名：即互联网上主机的标记，可以是域名或 IP 地址，在这里是“nginx.org”
    - 路径：即资源在主机上的位置，使用“/”分隔多级目录，在这里是“/en/download.html”
- HTTPS
  - “HTTP over SSL/TLS，也就是运行在 SSL/TLS 协议上的 HTTP。
  - SSL/TLS是负责加密通信的安全协议，建立在 TCP/IP 之上，所以也是个可靠的传输协议，可以被用作 HTTP 的下层。
  - SSL 的全称是“Secure Socket Layer”，由网景公司发明，当发展到 3.0 时被标准化，改名为 TLS，即“Transport Layer Security”，但由于历史的原因还是有很多人称之为SSL/TLS，或者直接简称为 SSL。
- 代理
  - 代理（Proxy）是 HTTP 协议中请求方和应答方中间的一个环节，作为“中转站”，既可以转发客户端的请求，也可以转发服务器的应答。
  - 功能
    - 负载均衡
    - 内容缓存
    - 安全防护
    - 数据处理







## 常说的“四层”和“七层”到底是什么

- TCP/IP 网络分层模型

  - 第一层：链接层
  - 第二层：网际层
  - 第三层：传输层
  - 第四层：应用层

- OSI 网络分层模型

  - OSI，全称是“开放式系统互联通信参考模型”

    - 第一层：物理层，网络的物理形式，例如电缆、光纤、网卡、集线器等等；

    2. 第二层：数据链路层
    3. 第三层：网络层
    4. 第四层：传输层
    5. 第五层：会话层
    6. 第六层：表示层
    7. 第七层：应用层

  7. TCP/IP 是一个纯软件的栈，没有网络应有的最根基的电缆、网卡等物理设备的位置。而 OSI 则补足了这个缺失，在理论层面上描述网络更加完整。

- 两个分层模型的映射关系

  - 第五、六、七层：统一对应到 TCP/IP 的应用层。
  - 所谓的“四层负载均衡”就是指工作在传输层上，基于 TCP/IP 协议的特性，例如 IP 地址、端口号等实现对后端服务器的负载均衡。
  - 所谓的“七层负载均衡”就是指工作在应用层上，看到的是 HTTP 协议，解析 HTTP 报文里的 URI、主机名、资源类型等数据，再用适当的策略转发给后端服务器。

- TCP/IP 协议栈的工作方式

  - HTTP 协议的传输过程就是这样通过协议栈逐层向下，每一层都添加本层的专有数据，层层打包，然后通过下层发送出去。

![OSI七层协议模型、TCP/IP四层模型学习笔记](https://s4.51cto.com/images/blog/201806/19/eb5a8f1811f634f1cc1fe5684ecdb7eb.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

![OSI七层协议模型、TCP/IP四层模型学习笔记](https://s4.51cto.com/images/blog/201806/19/d754f1060a2390637b4a013c1925079b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

![OSI七层协议模型、TCP/IP四层模型学习笔记](https://s4.51cto.com/images/blog/201806/19/27f893d336d5ac31f5e2495d6ec4b57f.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)



## 域名

- 域名的解析

  - DNS

    - 访问根域名服务器，它会告诉你“com”顶级域名服务器的地址；

    2. 访问“com”顶级域名服务器，它再告诉你“apple.com”域名服务器的地址；
    3. 最后访问“apple.com”域名服务器，就得到了“www.apple.com”的地址。

  - 在核心 DNS 系统之外，还有两种手段用来减轻域名解析的压力，并且能够更快地获取结果，基本思路就是“缓存”。

    - 其次，操作系统里也会对 DNS 解析结果做缓存，如果你之前访问过“www.apple.com”，那么下一次在浏览器里再输入这个网址的时候就不会再跑到DNS 那里去问了，直接在操作系统里就可以拿到 IP 地址。
    - 另外，操作系统里还有一个特殊的“主机映射”文件/hosts，通常是一个可编辑的文本，如果操作系统在缓存里找不到 DNS记录，就会找这个文件。



## 键入网址再按下回车，后面究竟发生了什么

- 使用 IP 地址访问 Web 服务器

  - 浏览器从地址栏的输入中获得服务器的 IP 地址和端口号；

  2. 浏览器用 TCP 的三次握手与服务器建立连接；
  3. 浏览器向服务器发送拼好的报文；
  4. 服务器收到报文后处理请求，同样拼好报文再发给浏览器；
  5. 浏览器解析报文，渲染输出页面。

- 使用域名访问 Web 服务器

  - Wireshark 抓包过程，你会发现，好像没有什么不同，浏览器上同样显示出了欢迎界面，抓到的包也同样是 11 个：先是三次握手，然后是两次 HTTP 传输
  - 发起域名解析动作，通过访问一系列的域名解析服务器，试图把这个域名翻译成 TCP/IP 协议里的 IP 地址。
  - 在域名解析的过程中会有多级的缓存，浏览器首先看一下自己的缓存里有没有，如果没有就向操作系统的缓存要，还没有就检查本机域名解析文件 hosts

- 真实的网络世界

  - 假设你要访问的是 Apple 网站，显然你是不知道它的真实 IP 地址的，在浏览器里只能使用
    域名“www.apple.com”访问，那么接下来要做的必然是域名解析。这就要用 DNS 协议开始从操作系统、本地 DNS、根 DNS、顶级 DNS、权威 DNS 的层层解析，当然这中间有缓存，可能不会费太多时间就能拿到结果。
  - CDN，它也会在 DNS 的解析过程中“插上一脚”。DNS 解析可能会给出 CDN 服务器的 IP 地址，这样你拿到的就会是 CDN 服务器而不是目标网站的实际地址。CDN 会缓存网站的大部分资源。
  - 由 PHP、Java 等后台服务动态生成的页面属于“动态资源”，CDN 无法缓存，只能从目标网站获取。于是你发出的 HTTP 请求就要开始在互联网上的“漫长跋涉”，经过无数的路由器、网关、代理，最后到达目的地。
  - 目标网站的服务器对外表现的是一个 IP 地址，但为了能够扛住高并发，在内部也是一套复杂的架构。通常在入口是负载均衡设备，例如四层的 LVS 或者七层的 Nginx，在后面是许多的服务器，构成一个更强更稳定的集群
  - 负载均衡设备会先访问系统里的缓存服务器，通常有 memory 级缓存 Redis 和 disk 级缓存 Varnish，它们的作用与 CDN 类似，不过是工作在内部网络里，把最频繁访问的数据缓存几秒钟或几分钟，减轻后端应用服务器的压力
  - 如果缓存服务器里也没有，那么负载均衡设备就要把请求转发给应用服务器了。这里就是各种开发框架大显神通的地方了，例如 Java 的 Tomcat/Netty/Jetty，Python 的 Django，还有 PHP、Node.js、Golang 等等。它们又会再访问后面的 MySQL、PostgreSQL、MongoDB 等数据库服务，实现用户登录、商品查询、购物下单、扣款支付等业务操作，然后把执行的结果返回给负载均衡设备，同时也可能给缓存服务器里也放一份。
  - 应用服务器的输出到了负载均衡设备这里，请求的处理就算是完成了，就要按照原路再走回去，还是要经过许多的路由器、网关、代理。如果这个资源允许缓存，那么经过 CDN 的时候它也会做缓存，这样下次同样的请求就不会到达源站了。
  - 最后网站的响应数据回到了你的设备，它可能是 HTML、JSON、图片或者其他格式的数据，需要由浏览器解析处理才能显示出来，如果数据里面还有超链接，指向别的资源，那么就又要重走一遍整个流程，直到所有的资源都下载完





## HTTP报文是什么样子的

- HTTP 协议的核心部分是它传输的报文内容

  - 它在实际要传输的数据之前附加了一个 20 字节的头部数据，存储 TCP 协议必须的额外信息，例如发送方的端口号、接收方的端口号、序号、标志位等等。

- HTTP 协议的请求报文和响应报文

  - 三大部分组成
    - 起始行（start line）：描述请求或响应的基本信息
    - 头部字段集合（header）：使用 key-value 形式更详细地说明报文；
      - HTTP 协议规定报文必须有 header，但可以没有 body，而且在 header 之后必须要有一个“空行”，也就是“CRLF”，十六进制的“0D0A”。
    - 消息正文（entity）：实际传输的数据，它不一定是纯文本，可以是图片、视频等二进制数据。
    - 这其中前两部分起始行和头部字段经常又合称为“请求头”或“响应头”，消息正文又称为“实体”（“body”）

- 请求行

  > 由三部分构成
  >
  > GET / HTTP/1.1
  >
  > “GET”是请求方法，“/”是请求目标，“HTTP/1.1”是版本号

- 状态行

  - > 响应报文里的起始行，在这里它不叫“响应行”，而是叫“状态行”（status line），意思是服务器响应的状态。
    >
    > HTTP/1.1 200 OK
    > 浏览器你好，我已经处理完了你的请求，这个报文使用的协议版本号是 1.1，状态码是 200，一切OK。”

- 常用头字段

  - 请求行或状态行再加上头部字段集合就构成了 HTTP 报文里完整的请求头或响应头
  - 基本上可以分为四大类
    - 通用字段：在请求头和响应头里都可以出现
    - 请求字段：仅能出现在请求头里，进一步说明请求信息或者额外的附加条件
    - 响应字段：仅能出现在响应头里，补充说明响应报文的信息
    - 实体字段：它实际上属于通用字段，但专门描述 body 的额外信息。

  

  

  

## 响应状态码

- RFC 标准把状态码分成了五类

  - > ​	1××：提示信息，表示目前是协议处理的中间状态，还需要后续的操作；
    > ​	2××：成功，报文已经收到并被正确处理；
    > ​	3××：重定向，资源位置发生变动，需要客户端重新发送请求；
    > ​	4××：客户端错误，请求报文有误，服务器无法处理；
    > ​	5××：服务器错误，服务器在处理请求时内部发生了错误

- 2××

  - “200 OK”
    - 如果是非 HEAD请求，通常在响应头后都会有 body 数据
  - “204 No Content”
    - 另一个很常见的成功状态码，它的含义与“200 OK”基本相同，但响应头后没有 body 数据。
  - “206 Partial Content”
  - 是 HTTP 分块下载或断点续传的基础，在客户端发送“范围请求”、要求获取资源的部分数据时出现，它与 200 一样，也是服务器成功处理了请求，但 body 里的数据不是资源的全部，而是其中的一部分。
  - 状态码 206 通常还会伴随着头字段“Content-Range”，表示响应报文里 body 数据的具体范围，供客户端确认，例如“Content-Range: bytes 0-99/2000”，意思是此次获取的是总计 2000 个字节的前 100 个字节。

- 3××

  - “301 Moved Permanently”
    - 俗称“永久重定向”，含义是此次请求的资源已经不存在了，需要改用改用新的 URI再次访问
  - “302 Moved Temporarily”
    - 俗称“临时重定向”，意思是请求的资源还在，但需要暂时用另一个 URI 来访问。
  - “304 Not Modified”
    - 表示资源未修改，用于缓存控制。它不具有通常的跳转含义，但可以理解成“重定向已到缓存的文件”（即“缓存重定向”）。
  - 303 See Other
    - 类似 302，但要求重定向后的请求改为GET 方法，访问一个结果页面，避免 POST/PUT 重复操作
  - 307 Temporary Redirect
    - 类似 302，但重定向后请求里的方法和实体不允许变动，含义比 302 更明确
  - 308 Permanent Redirect	
    - 类似 307，不允许重定向后的请求变动，但它是 301“永久重定向”的含义
    - 请求方法和主体不会被更改，`301`但有时可能会被错误地更改为[`GET`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/GET)方法。

- 4××

  - “400 Bad Request”
    - 通用的错误码，表示请求报文有错误，但具体是数据格式错误、缺少请求头还是 URI 超长它没有明确说，只是一个笼统的错误，客户端看到 400
      只会是“一头雾水”“不知所措”。所以，在开发 Web 应用时应当尽量避免给客户端返回 400，而是要用其他更有明确含义的状态码。
  - “403 Forbidden”	
    - 实际上不是客户端的请求出错，而是表示服务器禁止访问资源。原因可能多种多样，例如信息敏感、法律禁止等
  - “404 Not Found”	
    - 资源在本服务器上未找到
  - 405 Method Not Allowed
    - 不允许使用某些方法操作资源，例如不允许 POST 只能 GET
  - 406 Not Acceptable
    - 资源无法满足客户端请求的条件，例如请求中文但只有英文
  - 408 Request Timeout
    - 请求超时

- 5××

  - “501 Not Implemented”
    - 表示客户端请求的功能还不支持
  - “502 Bad Gateway”
    - 通常是服务器作为网关或者代理时返回的错误码，表示服务器自身工作正常，访问后端服务器时发生了错误，但具体的错误原因也是不知道的
  - “503 Service Unavailable”
    - 表示服务器当前很忙，暂时无法响应服务，我们上网时有时候遇到的“网络服务正忙，请稍后重试”的提示信息就是状态码 503
    - 503 是一个“临时”的状态，很可能过几秒钟后服务器就不那么忙了，可以继续提供服务，所以 503 响应报文里通常还会有一个“Retry-After”字段，指示客户端可以在多久以后再次尝试发送请求。



## HTTP有哪些特点

- 灵活可扩展
  - HTTP 协议随着互联网的发展一同成长起来了。在这个过程中，HTTP 协议逐渐增加了请求方法、版本号、状态码、头字段等特性。而 body 也不再限于文本形式的 TXT 或HTML，而是能够传输图片、音频视频等任意数据，这些都是源于它的“灵活可扩展”的特点
- 可靠传输
  - 因为 HTTP 协议是基于 TCP/IP 的，而 TCP 本身是一个“可靠”的传输协议，所以 HTTP 自然也就继承了这个特性，能够在请求方和应答方之间“可靠”地传输数据。
  - 不过我们必须正确地理解“可靠”的含义，HTTP 并不能 100% 保证数据一定能够发送到另一端，在网络繁忙、连接质量差等恶劣的环境下，也有可能收发失败。“可靠”只是向使用者提供了一个“承诺”，会在下层用多种手段“尽量”保证数据的完整送达。
- 应用层协议
  - HTTP 凭借着可携带任意头字段和实体数据的报文结构，以及连接控制、缓存代理等方便易用的特性，一出现就“技压群雄”，迅速成为了应用层里的“明星”协议。只要不太苛求性能，HTTP 几乎可以传递一切东西，满足各种需求，称得上是一个“万能”的协议。
- 请求 - 应答
  - 请求 - 应答模式也明确了 HTTP 协议里通信双方的定位，永远是请求方先发起连接和请求，是主动的，而应答方只有在收到请求后才能答复，是被动的，如果没有请求时不会有任何动作。
- 无状态
  - “状态”其实就是客户端或者服务器里保存的一些数据或者标志，记录了通信过程中的一些变化信息。
  - TCP 协议是有状态的，一开始处于 CLOSED 状态，连接成功后是ESTABLISHED 状态，断开连接后是 FIN-WAIT 状态，最后又是 CLOSED 状态。
  - HTTP，客户端和服务器永远是处在一种“无知”的状态。建立连接前两者互不知情，每次收发的报文也都是互相独立的，没有任何的联系。收发文也不会对客户端或服务器产生任何影响，连接后也不会要求保存任何信息。







## HTTP有哪些优点？又有哪些缺点

- 无状态
  - 因为服务器没有“记忆能力”，所以就不需要额外的资源来记录状态信息，不仅实现上会简单一些，而且还能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。
  - 而且，“无状态”也表示服务器都是相同的，没有“状态”的差异，所以可以很容易地组成集群，让负载均衡把请求转发到任意一台服务器
  - 既然服务器没有“记忆能力”，它就无法支持需要连续多个步骤的“事务”操作。例如电商购物，首先要登录，然后添加购物车，再下单、结算、支付，这一系列操作都需要知道用户的身份才行，但“无状态”服务器是不知道这些请求是相互关联的，每次都得问一遍身份信息，不仅麻烦，而且还增加了不必要的数据传输量。
- 明文
  - “明文”意思就是协议里的报文（准确地说是 header 部分）不使用二进制数据，而是用简单可阅读的文本形式。
- 不安全
  - 在“身份认证”和“完整性校验”这两方面 HTTP 也是欠缺的
  - 为了解决 HTTP 不安全的缺点，所以就出现了 HTTPS
- 性能
  - 现在互联网的特点是移动和高并发，不能保证稳定的连接质量，所以在 TCP 层面上 HTTP 协议有时候就会表现的不够好
  - “请求 - 应答”模式则加剧了 HTTP 的性能问题，这就是著名的“队头阻塞”（Head-of-line blocking），当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一并被阻塞，会导致客户端迟迟收不到数据
  - 不过现在已经有了终极解决方案：HTTP/2 和 HTTP/3







## HTTP的实体数据

- 数据类型与编码

  - 在 TCP/IP 协议栈里，传输数据基本上都是“header+body”的格式。但 TCP、UDP 因为是传输层的协议，它们不会关心 body 数据是什么，只要把数据发送到对方就算是完成了任务。
  - 而 HTTP 协议则不同，它是应用层的协议，数据到达之后工作只能说是完成了一半，还必须要告诉上层应用这是什么数据才行，否则上层应用就会“不知所措”。
    - “MIME type” 数据类型
    - “Encoding type” 编码类型

- 数据类型使用的头字段

  - HTTP 协议为此定义了两个 Accept 请求头字段和两个Content 实体头字段，用于客户端和服务器进行“内容协商”。也就是说，客户端用 Accept 头告诉服务器希望接收什么样的数据，而服务器用 Content 头告诉客户端实际发送了什么样的数据。

- 语言类型与字符集

  - 这实际上就是“国际化”的问题。
  - en-US 表示美式英语，en-GB 表示英式英语，而 zh-CN 就表示我们最常使用的汉语
    所以后来就出现了 Unicode 和 UTF-8，把世界上所有的语言都容纳在一种编码方案里，	
  - utf-8只是编码方案，Unicode是字符集

- 语言类型使用的头字段

  - Accept-Language字段标记了客户端可理解的自然语言，也允许用“,”做分隔符列出多个类型

  - 相应的，服务器应该在响应报文里用头字段ContentLanguage告诉客户端实体数据使用的

    实际语言类型

  - 字符集在 HTTP 里使用的请求头字段是Accept-Charset，但响应头里却没有对应的 Content-Charset，而是在Content-Type字段的数据类型后面用“charset=xxx”来表示，这点需要特别注意。

- 内容协商的结果

  - 内容协商的过程是不透明的，每个 Web 服务器使用的算法都不一样。但有的时候，服务器会在响应头里多加一个Vary字段，记录服务器在内容协商时参考的请求头字段，给出一点信息
  - Vary 字段可以认为是响应报文的一个特殊的“版本标记”。每当 Accept 等请求头变化时，Vary 也会随着响应报文一起变化。也就是说，同一个 URI 可能会有多个不同的“版本”，主要用在传输链路中间的代理服务器实现缓存服务

- 假设你要使用 POST 方法向服务器提交一些 JSON 格式的数据，里面包含有中文，请求头应该是什么样子的呢

  - > content-type: application/json; 
    >
    > content-language:zh-CN

    - content-type是实体字段，所以请求和响应里都可以用，作用是指明body数据的类型。如果要发post请求，就需要带上它
    - 在这里不能用accept字段，因为是post，所以要用content-language来指明body的语言类型，在content-type里用charset指明编码类型。

  - accept 表达的是你想要的。而你发送 post请求时，你发送的数据是给服务器的，这时候就需要像 服务器会用 content-type 标明它给你的数据类型一样，你也需要用 content- 来表明你给别人的数据的一些属性





## HTTP传输大文件的方法

- 分块传输
  - 这种“化整为零”的思路在 HTTP 协议里就
    是“chunked”分块传输编码，在响应报文里用头字段“Transfer-Encoding: chunked”来表示，意思是报文里的 body 部分不是一次性发过来的，而是分成了许多的块（chunk）逐个发送。
  - “Transfer-Encoding: chunked”和“Content-Length”这两个字段是互斥的，也就是说响应报文里这两个字段不能同时出现，一个响应报文的传输要么是长度已知，要么是长度未知（chunked），这一点你一定要记住。
- 范围请求
  - 允许客户端在请求头里使用专用字段来表示只获取文件的一部分，相当于是客户端的“化整为零”。
  - “Accept-Ranges: bytes=x-y”明确告知客户端：“我是支持范围请求的
  - 服务器收到 Range 字段后，需要做四件事。
    - 第一，它必须检查范围是否合法
    - 第二，如果范围正确，服务器就可以根据 Range 头计算偏移量，读取文件的片段了，返回状态码“206 Partial Content”
    - 第三，服务器要添加一个响应头字段Content-Range
    - 最后剩下的就是发送数据了
  - 有了范围请求之后，HTTP 处理大文件就更加轻松了，看视频时可以根据时间点计算出文件的 Range，不用下载整个文件，直接精确获取片段所在的数据内容。不仅看视频的拖拽进度需要范围请求，常用的下载工具里的多段下载、断点续传也是基于它实现的
    - 先发个 HEAD，看服务器是否支持范围请求，同时获取文件的大小；
    - 开 N 个线程，每个线程使用 Range 字段划分出各自负责下载的片段，发请求传输数据；
    - 下载意外中断也不怕，不必重头再来一遍，只要根据上次的下载记录，用 Range 请求剩下的那一部分就可以了。
- 多段数据
  - 刚才说的范围请求一次只获取一个片段，其实它还支持在Range 头里使用多个“x-y”，一次性获取多个片段数据
  - 这种情况需要使用一种特殊的 MIME 类型：“multipart/byteranges”，表示报文的 body 是由多段字节序列组成的，并且还要用一个参数“boundary=xxx”给出段之间的分隔标记。





## HTTP的连接管理

- 短连接
  - HTTP 协议底层的数据传输基于 TCP/IP，每次发送请求前需要先与服务器建立连接，收到响应报文后会立即关闭连接
- 长连接
  - 既然 TCP 的连接和关闭非常耗时间，那么就把这个时间成本由原来的一个“请求 - 应答”均摊到多个“请求 - 应答”上。
- 连接相关的头字段
  - 我们也可以在请求头里明确地要求使用长连接机制，使用的字段是Connection，值是“keep-alive”。
  - 在客户端，可以在请求头里加上“Connection: close”字段，告诉服务器：“这次通信后就关闭连接”。
- 队头阻塞
  - “队头阻塞”与短连接和长连接无关，而是由 HTTP 基本的“请求 - 应答”模型所导致的。
- 性能优化
  - 在 HTTP 里就是“并发连接”（concurrent connections），也就是同时对一个域名发起多个长连接，用数量来解决质量的问题。
  - “域名分片”（domain sharding）技术，还是用数量来解决质量的思路。
- tcp握手1个rtt，挥手2个rtt
  - 一个来回就是1rtt，三次握手准确来说是1.5个rtt，四次挥手是两个来回，所以是2rtt







## HTTP的重定向和跳转

- 重定向的过程
  - 第一个请求返回的响应报文
    - 这里出现了一个新的头字段“Location: /index.html”，它就是 301/302 重定向跳转的秘密所在。
    - “Location”字段属于响应字段，必须出现在响应报文里。但只有配合 301/302 状态码才有意义，它标记了服务器要求重定向的 URI
    - 浏览器收到 301/302 报文，会检查响应头里有没
      有“Location”。如果有，就从字段值里提取出 URI，发出新的 HTTP 请求，相当于自动替我们点击了这个链接
- 重定向状态码
  - 303 See Other
    - 类似 302，但要求重定向后的请求改为GET 方法，访问一个结果页面，避免 POST/PUT 重复操作
  - 307 Temporary Redirect
    - 类似 302，但重定向后请求里的方法和实体不允许变动，含义比 302 更明确
  - 308 Permanent Redirect	
    - 类似 307，不允许重定向后的请求变动，但它是 301“永久重定向”的含义
    - 请求方法和主体不会被更改，`301`但有时可能会被错误地更改为[`GET`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/GET)方法。
- 重定向的应用场景
  - 一个最常见的原因就是“资源不可用”，需要用另一个新的URI 来代替。
  - 另一个原因就是“避免重复”，让多个网址都跳转到一个URI，增加访问入口的同时还不会增加额外的工作量。
  - 决定要实行重定向后接下来要考虑的就是“永久”和“临时”的问题了，也就是选择 301 还是 302。
- 重定向的相关问题
  - 第一个问题是“性能损耗”。很明显，重定向的机制决定了一个跳转会有两次请求 - 应答，比正常的访问多了一次。
  - 第二个问题是“循环跳转”。所以 HTTP 协议特别规定，浏览器必须具有检测“循环跳转”的能力







## HTTP的缓存控制

- 服务器的缓存控制
  - 服务器标记资源有效期使用的头字段是“Cache-Control”，里面的值“maxage=30”就是资源的有效时间，相当于告诉浏览器，“这个页面只能缓存 30 秒，之后就算是过期，不能用
  - 这里的 max-age 是“生存时间”，时间的计算起点是响应报文的创建时刻（即 Date 字段，也就是离开服务器的时刻），而不是客户端收到报文的时刻，也就是说包含了在链路传输过程中所有节点所停留的时间。
  - “max-age”是 HTTP 缓存控制最常用的属性，此外在响应报文里还可以用其他的属性来更精确地指示浏览器应该如何使用缓存
    - no_store：不允许缓存，用于某些变化非常频繁的数据，例如秒杀页面；
    - no_cache：实际的意思并不是不允许缓存，而是可以缓存，但在使用之前必须要去服务器验证是否过期，是否有最新的版本；
    - must-revalidate：它的意思是如果缓存不过期就可以继续使用，但过期了如果还想用就必须去服务器验证
- 客户端的缓存控制
  - 其实不止服务器可以发“Cache-Control”头，浏览器也可以发“Cache-Control”，也就是说请求 - 应答的双方都可以用这个字段进行缓存控制，互相协商缓存的使用策略。
  - 当你点“刷新”按钮的时候，浏览器会在请求头里加一个“Cache-Control: maxage=0”。
  - Ctrl+F5 的“强制刷新”又是什么样的呢？
    - 它其实是发了一个“Cache-Control: no-cache”，含义和“max-age=0”基本一样，就看后台的服务器怎么理解，通常两者的效果是相同的。
  - 重定向跳转功能，也可以发现浏览器使用了缓存
  - 在“前进”“后退”“跳转”这些重定向动作中浏览器不会“夹带私货”，只用最基本的请求头，没有“Cache-Control”，所以就会检查缓存，直接利用之前的资源，不再进行网络通信
- 条件请求
  - 浏览器可以用两个连续的请求组成“验证动作”：先是一个 HEAD，获取资源的修改时间等元信息，然后与缓存数据比较，如果没有改动就使用缓存，节省网络流量，否则就再发一个 GET 请求，获取最新的版本。但这样的两个请求网络成本太高了，所以 HTTP 协议就定义了一系列“If”开头的“条件请求”字段，专门用来检查验证资源是否过期
  - 条件请求一共有 5 个头字段，我们最常用的是“if-Modified-Since”和“If-NoneMatch”这两个
    - 需要第一次的响应报文预先提供“Last-modified”和“ETag”，然后第二次请求时就可以带上缓存里的原值，验证资源是否是最新的。
    - 如果资源没有变，服务器就回应一个“304 Not Modified”，表示缓存依然有效，浏览器就可以更新一下有效期，然后放心大胆地使用缓存了。
    - 刷新页面时浏览器就会同时发送缓存控制头“max-age=0”和条件请求头“If-None-Match”，如果缓存有效服务器就会返回 304
- 即使有“Last-modified”和“ETag”，强制刷新（Ctrl+F5）也能够从服务器获取最新数据（返回 200 而不是 304）
  - 强制刷新，请求头里有Pragma: no-cache和Cache-Control: no-cache，没有If-Modified-Since/If-None-Match，这个Pragma: no-cache的意思是禁用缓存







## HTTP的代理服务

- 代理的作用

  - 代理最基本的一个功能是负载均衡。
    - 代理中常用的负载均衡算法比如轮询、一致性哈希等等，这些算法的目标都是尽量把外部的流量合理地分散到多台源服务器，提高系统的整体资源利用率和性能
  - 在负载均衡的同时，代理服务还可以执行更多的功能
    - 健康检查	
    - 安全防护：保护被代理的后端服务器，限制 IP 地址或流量，抵御网络攻击和过载；	
    - 加密卸载：对外网使用 SSL/TLS 加密通信认证，而在安全的内网不加密，消除加解密成本	
    - 数据过滤：拦截上下行的数据，任意指定策略修改请求或者响应；
    - 内容缓存：暂存、复用服务器响应
  - 代理会增加链路长度，在代理上做一些复杂的处理。会很耗费性能，增加响应时间。代理会成为性能瓶颈，有单点问题

- 代理相关头字段

  - 代理服务器需要用字段“Via”标明代理的身份
    - Via 字段只解决了客户端和源服务器判断是否存在代理的问题，还不能知道对方的真实信息。
  - 最常用的两个头字段是“X-Forwarded-For”和“X-Real-IP”
    - “X-Forwarded-For”的字面意思是“为谁而转发”，形式上和“Via”差不多，也是每经过一个代理节点就会在字段里追加一个信息。但“Via”追加的是代理主机名（或者域名），而“X-Forwarded-For”追加的是请求方的 IP 地址
    - “X-Real-IP”是另一种获取客户端真实 IP 的手段，它的作用很简单，就是记录客户端 IP地址，没有中间的代理信息，相当于是“X-Forwarded-For”的简化版。如果客户端和源服务器之间只有一个代理，那么这两个字段的值就是相同的

- 代理协议

  - 通过“X-Forwarded-For”操作代理信息必须要解析 HTTP 报文头，这对于代理来说成本比较高

  - 另一个问题是“X-Forwarded-For”头必须要修改原始报文，而有些情况下是不允许甚至不可能的（比如使用 HTTPS 通信被加密）。

  - 所以就出现了一个专门的“代理协议”（The PROXY protocol），它由知名的代理软件HAProxy 所定义，也是一个“事实标准”，被广泛采用

    - > “代理协议”有 v1 和 v2 两个版本，v1 和 HTTP 差不多，也是明文，而 v2 是二进制格式。今天只介绍比较好理解的 v1，它在 HTTP 报文前增加了一行 ASCII 码文本，相当于又多了一个头。
      >
      > 这一行文本其实非常简单，开头必须是“PROXY”五个大写字母，然后是“TCP4”或者“TCP6”，表示客户端的 IP 地址类型，再后面是请求方地址、应答方地址、请求方端口号、应答方端口号，最后用一个回车换行（\r\n）结束。
      >
      > 1 PROXY TCP4 1.1.1.1 2.2.2.2 55555 80\r\n
      > 2 GET / HTTP/1.1\r\n
      > 3 Host: www.xxx.com\r\n
      > 4 \r\n
      >
      > 服务器看到这样的报文，只要解析第一行就可以拿到客户端地址，不需要再去理会后面的HTTP 数据，省了很多事情
      >
      > 不过代理协议并不支持“X-Forwarded-For”的链式地址形式，所以拿到客户端地址后再如何处理就需要代理服务器与后端自行约定。







## HTTP的缓存代理

- 源服务器的缓存控制

  - 服务器端的“Cache-Control”属性：max-age、no_store、no_cache 和 must-revalidate这 4 种缓存属性可以约束客户端，也可以约束代理。
  - 首先，我们要区分客户端上的缓存和代理上的缓存，可以使用两个新属性“private”和“public
    - “private”表示缓存只能在客户端保存，是用户“私有”的，不能放在代理上与别人共享。而“public”的意思就是缓存完全开放，谁都可以存，谁都可以用。、
  - 其次，缓存失效后的重新验证也要区分开（即使用条件请求“Lastmodified”和“ETag”），
    - “must-revalidate”是只要过期就必须回源服务器验证，而新的“proxy-revalidate”只要求代理的缓存过期后必须验证，客户端不必回源，只验证到代理这个环节就行了。
  - 还有一个代理专用的属性“no-transform”。
    - 代理有时候会对缓存下来的数据做一些优化，比如把图片生成 png、webp 等几种格式，方便今后的请求处理，而“notransform”就会禁止这样做，不许“偷偷摸摸搞小动作”。
  - 源服务器在设置完“Cache-Control”后必须要为报文加上“Lastmodified”或“ETag”字段。否则，客户端和代理后面就无法使用条件请求来验证缓存是否有效，也就不会有 304 缓存重定向。

- 客户端的缓存控制

  - 关于缓存的生存时间，多了两个新属性“max-stale”和“min-fresh”。
  - “max-stale”的意思是如果代理上的缓存过期了也可以接受，但不能过期太多，超过 x 秒也会不要。“min-fresh”的意思是缓存必须有效，而且必须在 x 秒后依然有效。
  - 比如，草莓上贴着标签“max-age=5”，现在已经在冰柜里存了 7 天。如果有请求“max-stale=2”，意思是过期两天也能接受，所以刚好能卖出去。
  - 但要是“min-fresh=1”，这是绝对不允许过期的，就不会买走。这时如果有另外一个菠萝是“max-age=10”，那么“7+1<10”，在一天之后还是新鲜的，所以就能卖出去。
  - 有的时候客户端还会发出一个特别的“only-if-cached”属性，表示只接受代理缓存的数据，不接受源服务器的响应。如果代理上没有缓存或者缓存过期，就应该给客户端返回一个504（Gateway Timeout）


## HTTP问题

- 与服务器建立的连接是否会在一个HTTP请求后断开？什么情况下断开？

  - **在 HTTP/1.0 中，一个服务器在发送完一个 HTTP 响应后，会断开 TCP 链接。但是这样每次请求都会重新建立和断开 TCP 连接，代价过大。**所以虽然标准中没有设定，某些服务器对 Connection: keep-alive 的 Header 进行了支持。意思是说，完成这个 HTTP 请求之后，不要断开 HTTP 请求使用的 TCP 连接。这样的好处是连接可以被重新使用，之后发送 HTTP 请求的时候不需要重新建立 TCP 连接，以及如果维持连接，那么 SSL 的开销也可以避免。
  - **持久连接：**既然维持 TCP 连接好处这么多，HTTP/1.1 就把 Connection 头写进标准，并且默认开启持久连接，除非请求中写明 Connection: close，那么浏览器和服务器之间是会维持一段时间的 TCP 连接，不会一个请求结束就断掉。
  - **所以第一个问题的答案是：**默认情况下建立 TCP 连接不会断开，只有在请求报头中声明 Connection: close 才会在请求完成后关闭连接。

- 一个 TCP 连接可以对应几个 HTTP 请求

  - 如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。

- 一个 TCP 连接中 HTTP 请求发送可以一起发送么

  - **HTTP/1.1 存在一个问题：**单个 TCP 连接在同一时刻只能处理一个请求。

    **意思是说：**两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。

    虽然 HTTP/1.1 规范中规定了 [Pipelining](https://link.zhihu.com/?target=https%3A//links.jianshu.com/go%3Fto%3Dhttps%3A%2F%2Ftools.ietf.org%2Fhtml%2Frfc2616%23section-8.1.2.2) 来试图解决这个问题，但是这个功能在浏览器中默认是关闭的。

  - 但是，HTTP2 提供了 Multiplexing 多路传输特性，可以在一个 TCP 连接中同时完成多个 HTTP 请求。

- 为什么有的时候刷新页面不需要重新建立 SSL 连接

  - TCP 连接有的时候会被浏览器和服务端维持一段时间。TCP 不需要重新建立，SSL 自然也会用之前的。

- 浏览器对同一 Host 建立 TCP 连接到数量有没有限制

  - 假设我们还处在 HTTP/1.1 时代，那个时候没有多路传输，当浏览器拿到一个有几十张图片的网页该怎么办呢？
    - **Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别**
  - **收到的 HTML 如果包含几十个图片标签，这些图片是以什么方式、什么顺序、建立了多少连接、使用什么协议被下载下来的呢**
    - 如果图片都是 HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量能不能用 HTTP2，如果能的话就使用 Multiplexing 功能在这个连接上进行多路传输。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取，但是可以确定的是 Multiplexing 很可能会被用到。如果发现用不了 HTTP2 呢？或者用不了 HTTPS（现实中的
    - HTTP2 都是在 HTTPS 上实现的，所以也就是只能使用 HTTP/1.1）。那浏览器就会在一个 HOST 上建立多个 TCP 连接，连接数量的最大限制取决于浏览器设置，这些连接会在空闲的时候被浏览器用来发送新的请求，如果所有的连接都正在发送请求呢？那其他的请求就只能等等了。



## HTTPS是什么？SSL/TLS又是什么

- 为什么要有 HTTPS？
  - 由于 HTTP 天生“明文”的特点，整个传输过程完全透明，任何人都能够在链路中截获、修改或者伪造请求 / 响应报文，数据不具有可信性。
- 什么是安全？
  - 通常认为，如果通信过程具备了四个特性，就可以认为是“安全”的，这四个特性是：机密性、完整性，身份认证和不可否认。
    - 机密性是指对数据的“保密”，只能由可信的人访问
    - 完整性是指数据在传输过程中没有被窜改
    - 身份认证是指确认对方的真实身份
    - 不可否认是指不能否认已经发生过的行为
- 什么是 HTTPS？
  - 它为 HTTP 增加了刚才所说的四大安全特性
  - HTTPS 其实是一个“非常简单”的协议，新的协议名“https”，默认端口号 443，至于请求 - 应答模式、报文结构、请求方法、URI、头字段、连接管理等等都完全沿用 HTTP，没有任何新的东西。
  - 如果用 Wireshark 抓包，也会发现与HTTP 不一样，不再是简单可见的明文，多了“Client Hello”“Server Hello”等新的数据包
  - 秘密就在于 HTTPS 名字里的“S”，它把 HTTP 下层的传输协议由 TCP/IP 换成了SSL/TLS，由“HTTP over TCP/IP”变成了“HTTP over SSL/TLS”
- SSL/TLS
  - SSL 即安全套接层（Secure Sockets Layer），在 OSI 模型中处于第 5 层（会话层）
  - TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成，综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术。
  - TLS 的密码套件命名非常规范，格式很固定。
    基本的形式是“密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法”
    - “ECDHE-RSA-AES256-GCM-SHA384”。
    - “握手时使用 ECDHE 算法进行密钥交换，用 RSA 签名和身份认证，握手后的通信使用AES 对称算法，密钥长度 256 位，分组模式是 GCM，摘要算法 SHA384 用于消息认证和产生随机数。”
  - 机密性由对称加密AES保证，完整性由SHA384摘要算法保证，身份认证和不可否认由RSA非对称加密保证





## 对称加密与非对称加密

- 对称加密
  - 加密和解密时使用的密钥都是同一个
  - 对称算法还有一个“分组模式”的概念，它可以让算法用固定长度的密钥加密任意长度的明文，把小秘密（即密钥）转化为大秘密（即密文）。
    - 比如，AES128-GCM，意思是密钥长度为 128 位的 AES 算法，使用的分组模式是 GCM；
- 非对称加密
  - 对称加密看上去好像完美地实现了机密性，但其中有一个很大的问题：如何把密钥安全地传递给对方，术语叫“密钥交换”。
  - 所以，就出现了非对称加密（也叫公钥加密算法）。
  - 公钥和私钥有个特别的“单向”性，虽然都可以用来加密解密，但公钥加密后只能用私钥解密，反过来，私钥加密后也只能用公钥解密。
  - 非对称加密可以解决“密钥交换”的问题。网站秘密保管私钥，在网上任意分发公钥，你想要登录网站只要用公钥加密就行了，密文只能由私钥持有者才能解密。
- 在 TLS 里使用混合加密方式
  - 在通信刚开始的时候使用非对称算法，比如 RSA、ECDHE，首先解决密钥交换的问题。
  - 然后用随机数产生对称算法使用的“会话密钥”（session key），再用公钥加密。因为会话密钥很短，通常只有 16 字节或 32 字节，所以慢一点也无所谓。
  - 对方拿到密文后用私钥解密，取出会话密钥。这样，双方就实现了对称密钥的安全交换，后续就不再使用非对称加密，全都使用对称加密。





## 数字签名与证书

- 摘要算法
  - 实现完整性的手段主要是摘要算法（Digest Algorithm），也就是常说的散列函数、哈希函数（Hash Function）
  - 摘要算法实际上是把数据从一个“大空间”映射到了“小空间”，所以就存在“冲突”（collision，也叫碰撞）的可能性
  - 因为摘要算法对输入具有“单向性”和“雪崩效应”，输入的微小不同会导致输出的剧烈变化，所以也被 TLS 用来生成伪随机数
  - 摘要算法保证了“数字摘要”和原文是完全等价的。所以，我们只要在原文后附上它的摘要，就能够保证数据的完整性。
    - 不过摘要算法不具有机密性，如果明文传输，那么黑客可以修改消息后把摘要也一起改了，网站还是鉴别不出完整性。
    - 所以，真正的完整性必须要建立在机密性之上，在混合加密系统里用会话密钥加密消息和摘要，这样黑客无法得知明文，也就没有办法动手脚了
    - 这有个术语，叫哈希消息认证码（HMAC）
- 数字签名
  - 加密算法结合摘要算法，我们的通信过程可以说是比较安全了。但这里还有漏洞，就是通信的两个端点（endpoint）。
    - 非对称加密里的“私钥”，使用私钥再加上摘要算法，就能够实现“数字签名”，同时实现“身份认证”和“不可否认”。
  - 数字签名的原理其实很简单，就是把公钥私钥的用法反过来，之前是公钥加密、私钥解密，现在是私钥加密、公钥解密
  - 只要你和网站互相交换公钥，就可以用“签名”和“验签”来确认消息的真实性，因为私钥保密，黑客不能伪造签名，就能够保证通信双方的身份。
    - 比如，你用自己的私钥签名一个消息“我是小明”。网站收到后用你的公钥验签，确认身份
      没问题，于是也用它的私钥签名消息“我是某宝”。你收到后再用它的公钥验一下，也没问
      题，这样你和网站就都知道对方不是假冒的，后面就可以用混合加密进行安全通信了。
- 数字证书和 CA
  - 怎么来判断这个公钥就是你或者某宝的公钥呢？
  - 必须引入“外力”，找一个公认的可信第三方，让它作为“信任的起点，递归的终点”，构建起公钥的信任链。这个“第三方”就是我们常说的CA（Certificate Authority，证书认证机构）
  - CA 对公钥的签名认证也是有格式的，不是简单地把公钥绑定在持有者身份上就完事了，还要包含序列号、用途、颁发者、有效时间等等，把这些打成一个包再签名，完整地证明公钥关联的各种信息，形成“数字证书”（Certificate）。





## TLS1.2连接过程

- TLS 协议的组成

  - TLS 包含几个子协议，你也可以理解为它是由几个不同职责的模块组成
  - 记录协议（Record Protocol）规定了 TLS 收发数据的基本单位：记录（record）。
  - 警报协议（Alert Protocol）的职责是向对方发出警报信息，有点像是 HTTP 协议里的状态码。
  - 握手协议（Handshake Protocol）是 TLS 里最复杂的子协议，要比 TCP 的 SYN/ACK 复杂的多，浏览器和服务器会在握手过程中协商 TLS 版本号、随机数、密码套件等信息，然后交换证书和密钥参数，最终双方协商得到会话密钥，用于后续的混合加密系统。
  - 变更密码规范协议（Change Cipher Spec Protocol），它非常简单，就是一个“通知”，告诉对方，后续的数据都将使用加密保护。那么反过来，在它之前，数据都是明文的。

- TLS 的握手过程

  - ECDHE 握手过程
    - 在 TCP 建立连接之后，浏览器会首先发一个“Client Hello”消息
      - 里面有客户端的版本号、支持的密码套件，还有一个随机数（Client Random），用于后续生成会话密钥。
    - 服务器收到“Client Hello”后，会返回一个“Server Hello”消息
      - 把版本号对一下，也给出一个随机数（Server Random），然后从客户端的列表里选一个作为本次通信使用的密码套件
      - 然后，服务器为了证明自己的身份，就把证书也发给了客户端（Server Certificate）。
      - 因为服务器选择了 ECDHE算法，所以它会在证书后发送“Server Key Exchange”消息，里面是椭圆曲线的公钥（Server Params），用来实现密钥交换算法，再加上自己的私钥签名认证。
      - 之后是“Server Hello Done”消息，服务器说：“我的信息就是这些，打招呼完毕。”
    - 这样第一个消息往返就结束了（两个 TCP 包），结果是客户端和服务器通过明文共享了三个信息：Client Random、Server Random 和 Server Params。
    - 客户端这时也拿到了服务器的证书，开始走证书链逐级验证，确认证书的真实性，再用证书公钥验证签名，就确认了服务器的身份
    - 然后，客户端按照密码套件的要求，也生成一个椭圆曲线的公钥（Client Params），用“Client Key Exchange”消息发给服务器。
      - 现在客户端和服务器手里都拿到了密钥交换算法的两个参数（Client Params、Server Params），就用 ECDHE 算法一阵算，算出了一个新的东西，叫“Pre-Master”，其实也是一个随机数。
      - 现在客户端和服务器手里有了三个随机数：Client Random、Server Random 和 Pre-Master。用这三个作为原始材料，就可以生成用于加密会话的主密钥，叫“Master Secret”。而黑客因为拿不到“Pre-Master”，所以也就得不到主密钥。
      - 他们不信任客户端或服务器伪随机数的可靠性，为了保证真正的“完全随机”“不可预测”，把三个不可靠的随机数混合起来
    - 有了主密钥和派生的会话密钥，握手就快结束了。客户端发一个“Change Cipher Spec”，然后再发一个“Finished”消息，把之前所有发送的数据做个摘要，再加密一下，让服务器做个验证。意思就是告诉服务器：后面都改用对称算法加密通信了
    - 服务器也是同样的操作，发“Change Cipher Spec”和“Finished”消息，双方都验证加密解密 OK，握手正式结束，后面就收发被加密的 HTTP 请求和响应了。
  - RSA 握手过程
    - 这与传统的握手有两点不同
      - 第一个，使用 ECDHE 实现密钥交换，而不是 RSA，所以会在服务器端发出“Server Key Exchange”消息。
      - 第二个，因为使用了 ECDHE，客户端可以不用等到服务器发回“Finished”确认握手完毕，立即就发出 HTTP 报文，省去了一个消息往返的时间浪费。这个叫“TLS False Start”，意思就是“抢跑”
    - 大体的流程没有变，只是“Pre-Master”不再需要用算法生成，而是客户端直接生成随机数，然后用服务器的公钥加密，通过“Client Key Exchange”消息发给服务器。服务器再用私钥解密，这样双方也实现了共享三个随机数，就可以生成主密钥。

  





## TLS1.3特性

- TLS1.3 的三个主要改进目标：兼容、安全与性能
- 最大化兼容性
  - 在记录头的 Version 字段被兼容性“固定”的情况下，只要是 TLS1.3 协议，握手的“Hello”消息后面就必有“supported_versions”扩展，它标记了 TLS 的版本号，使用它就能区分新旧协议。
- 强化安全
  - TLS1.3 里只保留了 AES、ChaCha20 对称加密算法，分组模式只能用 AEAD 的 GCM、CCM 和 Poly1305，摘要算法只能用 SHA256、SHA384，密钥交换算法只有 ECDHE 和 DHE
- 提升性能
  - HTTPS 建立连接时除了要做 TCP 握手，还要做 TLS 握手，在 1.2 中会多花两个消息往返（2-RTT），可能导致几十毫秒甚至上百毫秒的延迟，在移动网络中延迟还会更严重。
  - TLS1.3压缩了以前的“Hello”协商过程，删除了“Key Exchange”消息，把握手时间减少到了“1-RTT”，效率提高了一倍
  - 其实具体的做法还是利用了扩展。客户端在“Client Hello”消息里直接用“supported_groups”带上支持的曲线，用“key_share”带上曲线对应的客户端公钥参数，用“signature_algorithms”带上签名算法。
    - 服务器收到后在这些扩展里选定一个曲线和参数，再用“key_share”扩展返回服务器这边的公钥参数，就实现了双方的密钥交换，后面的流程就和 1.2 基本一样了
    - 这时只交换了两条消息，客户端和服务器就拿到了四个共享信息：Client Random和Server Random、Client Params和Server Params，两边就可以各自用 ECDHE 算出“Pre-Master”，再用 HKDF 生成主密钥“Master Secret”
  - 在算出主密钥后，服务器立刻发出“Change Cipher Spec”消息，比 TLS1.2 提早进入加密通信，后面的证书等就都是加密的了，减少了握手时的明文信息泄露。
    - 这里 TLS1.3 还有一个安全强化措施，多了个“Certificate Verify”消息，用服务器的私钥把前面的曲线、套件、参数等握手数据加了签名，作用和“Finished”消息差不多。但由于是私钥签名，所以强化了身份认证和和防窜改。
  - 这两个“Hello”消息之后，客户端验证服务器证书，再发“Finished”消息，就正式完成了握手，开始收发 HTTP 报文

## HTTP/2特性

- HTTP 有两个主要的缺点：安全不足和性能不高。
- 兼容 HTTP/1
  - 因为必须要保持功能上的兼容，所以 HTTP/2 把 HTTP 分解成了“语义”和“语法”两个部分，“语义”层不做改动，与 HTTP/1 完全一致。比如请求方法、URI、状态码、头字段等概念都保留不变，
  - 特别要说的是，与 HTTPS 不同，HTTP/2 没有在 URI 里引入新的协议名，仍然用“http”表示明文协议，用“https”表示加密协议
  - 在“语义”保持稳定之后，HTTP/2 在“语法”层做了“天翻地覆”的改造，完全变更了HTTP 报文的传输格式。
- 头部压缩
  - 由于报文 Header 一般会携带“User Agent”“Cookie”“Accept”“Server”等许多固定的头字段，多达几百字节甚至上千字节，但 Body 却经常只有几十字节（比如 GET 请求、204/301/304 响应），成千上万的请求响应报文里有很多字段值都是重复的，非常浪费，“长尾效应”导致大量带宽消耗在了这些冗余度极高的数据上。
  - HTTP/2 并没有使用传统的压缩算法，而是开发了专门的“HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还釆用哈夫曼编码来压缩整数和字符串，可以达到 50%~90% 的高压缩率。
- 二进制格式
  - 报文不再使用肉眼可见的ASCII 码，而是向下层的 TCP/IP 协议“靠拢”，全面采用二进制格式。这样虽然对人不友好，但却大大方便了计算机的解析。
  - 二进制里只有“0”和“1”，可以严格规定字段大小、顺序、标志位等格式
  - 它把 TCP 协议的部分特性挪到了应用层，把原来的“Header+Body”的消息“打散”为数个小片的二进制“帧”（Frame），用“HEADERS”帧存放头数据、“DATA”帧存放实体数据。
- 虚拟的“流”
  - HTTP/2 为此定义了一个“流”（Stream）的概念，它是二进制帧的双向传输序列，同一个消息往返的帧会分配一个唯一的流 ID。
  - 你可以想象把它成是一个虚拟的“数据流”，在里面流动的是一串有先后顺序的数据帧，这些数据帧按照次序组装起来就是 HTTP/1 里的请求报文和响应报文。
  - 因为“流”是虚拟的，实际上并不存在，所以 HTTP/2 就可以在一个 TCP 连接上用“流”同时发送多个“碎片化”的消息，这就是常说的“多路复用”（ Multiplexing）——多个往返通信都复用一个连接来处理。
  - HTTP/1里的请求都是排队处理的，所以有队头阻塞。HTTP/2的请求是乱序的，彼此不依赖，所以没有队头阻塞。
  - HTTP/2 还在一定程度上改变了传统的“请求 - 应答”工作模式，服务器不再是完全被动地响应请求，也可以新建“流”主动向客户端发送消息
- 强化安全
  - 互联网上通常所能见到的 HTTP/2 都是使用“https”协议名，跑在 TLS 上面。
  - 为了区分“加密”和“明文”这两个不同的版本，HTTP/2 协议定义了两个字符串标识符：“h2”表示加密的 HTTP/2，“h2c”表示明文的 HTTP/2，多出的那个字母“c”的意思是“clear text”。







## HTTP/2内核

- 头部压缩
  - 因为语义上它与 HTTP/1 兼容，所以报文还是由“Header+Body”构成的，但在请求发送前，必须要用“HPACK”算法来压缩头部数据。
    - 与 gzip、zlib 等压缩算法不同，它是一个“有状态”的算法，需要客户端和服务器各自维护一份“索引表”，也可以说是“字典”，压缩和解压缩就是查表和更新表的操作。
  - 为了方便管理和压缩，HTTP/2 废除了原有的起始行概念，把起始行里面的请求方法、URI、状态码等统一转换成了头字段的形式，并且给这些“不是头字段的头字段”起了个特别的名字——“伪头字段”
    - 为了与“真头字段”区分开来，这些“伪头字段”会在名字前加一个“:”，比如“:authority”    “:method”     “:status”，分别表示的是域名、请求方法和状态码。
- 二进制帧
  - 头部数据压缩之后，HTTP/2 就要把报文拆成二进制的帧准备发送。
  - HTTP/2 的帧结构有点类似 TCP 的段或者 TLS 里的记录，但报头很小，只有 9 字节，非常节省（可以对比一下 TCP 头，它最少是 20 个字节）。
    - 帧开头是 3 个字节的长度（但不包括头的 9 个字节），默认上限是 2^14，最大是 2^24，也就是说 HTTP/2 的帧通常不超过 16K，最大是 16M。
    - 长度后面的一个字节是帧类型，大致可以分成数据帧和控制帧两类
    - 第 5 个字节是非常重要的帧标志信息，可以保存 8 个标志位，携带简单的控制信息。
    - 流标识符，也就是帧所属的“流”，接收方使用它就可以从乱序的帧里识别出具有相同流 ID 的帧序列，按顺序组装起来就实现了虚拟的“流”。
- 流与多路复用
  - 在 HTTP/2 连接上，虽然帧是乱序收发的，但只要它们都拥有相同的流 ID，就都属于一个流，而且在这个流里帧不是无序的，而是有着严格的先后顺序。
  - 流是可并发的，一个 HTTP/2 连接上可以同时发出多个流传输数据，也就是并发多请求，实现“多路复用”；
    - HTTP/2 在一个连接上使用多个流收发数据，那么它本身默认就会是长连接，所以永远不需要“Connection”头字段（keepalive 或 close）
  - 流可以设置优先级，让服务器优先处理
  - 流 ID 不能重用，只能顺序递增，客户端发起的 ID 是奇数，服务器端发起的 ID 是偶数；
  - 第 0 号流比较特殊，不能关闭，也不能发送数据帧，只能发送控制帧，用于流量控制。
  - 在流上发送“RST_STREAM”帧可以随时终止流，取消接收或发送；
- 流状态转换
  - 流很重要，也很复杂。为了更好地描述运行机制，HTTP/2 借鉴了 TCP，根据帧的标志位实现流状态转换。当然，这些状态也是虚拟的，只是为了辅助理解。
  - 对应到一个标准的 HTTP“请求 - 应答”
    - 最开始的时候流都是“空闲”（idle）状态
    - 当客户端发送 HEADERS 帧后，有了流 ID，流就进入了“打开”状态，两端都可以收发数据，然后客户端发送一个带“END_STREAM”标志位的帧，流就进入了“半关闭”状态。
      - 这个“半关闭”状态很重要，意味着客户端的请求数据已经发送完了，需要接受响应数据，而服务器端也知道请求数据接收完毕，之后就要内部处理，再发送响应数据。
    - 响应数据发完了之后，也要带上“END_STREAM”标志位，表示数据发送完毕，这样流两端就都进入了“关闭”状态，流就结束了。
    - 刚才也说过，流 ID 不能重用，所以流的生命周期就是 HTTP/1 里的一次完整的“请求 - 应答”，流关闭就是一次通信结束。
    - 下一次再发请求就要开一个新流（而不是新连接），流 ID 不断增加，直到到达上限，发送“GOAWAY”帧开一个新的 TCP 连接，流 ID 就又可以重头计数。







## HTTP/3

- HTTP/2 的“队头阻塞”
  - HTTP/2 虽然使用“帧”“流”“多路复用”，没有了“队头阻塞”，但这些手段都是在应用层里，而在下层，也就是 TCP 协议里，还是会发生“队头阻塞”。
  - 让我们从协议栈的角度来仔细看一下。在 HTTP/2 把多个“请求 - 响应”分解成流，交给TCP 后，TCP 会再拆成更小的包依次发送（其实在 TCP 里应该叫 segment，也就是“段”）。
  - TCP 为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，其他的包即使已经收到了，也只能放在缓冲区里，上层的应用拿不出来，只能“干着急”。
  - 由于这种“队头阻塞”是 TCP 协议固有的，所以 HTTP/2 即使设计出再多的“花样”也无法解决。
- QUIC协议
  - 是一个传输层的协议，和 TCP 是平级的
  - “HTTP over QUIC”就是 HTTP 协议的下一个大版本，HTTP/3。它在 HTTP/2 的基础上又实现了质的飞跃，真正“完美”地解决了“队头阻塞”问题。
  - HTTP/3 有一个关键的改变，那就是它把下层的 TCP“抽掉”了，换成了 UDP。因为 UDP 是无序的，包之间没有依赖关系，所以就从根本上解决了“队头阻塞”
  - QUIC 就选定了 UDP，在它之上把 TCP 的那一套连接管理、拥塞窗口、流量控制等“搬”了过来，“去其糟粕，取其精华”，打造出了一个全新的可靠传输协议，可以认为是“新时代的 TCP”。
- QUIC 的特点
  - QUIC 基于 UDP，而 UDP 是“无连接”的，根本就不需要“握手”和“挥手”，所以天生就要比 TCP 快。
  - 就像 TCP 在 IP 的基础上实现了可靠传输一样，QUIC 也基于 UDP 实现了可靠传输，保证数据一定能够抵达目的地。它还引入了类似 HTTP/2 的“流”和“多路复用”，单个“流”是有序的，可能会因为丢包而阻塞，但其他“流”不会受到影响。
  - QUIC 并不是建立在 TLS 之上，而是内部“包含”了 TLS。它使用自己的帧“接管”了TLS 里的“记录”，握手消息、警报消息都不使用 TLS 记录，直接封装成 QUIC 的帧发送，省掉了一次开销。
- QUIC 内部细节
  - QUIC 的基本数据传输单位是包（packet）和帧（frame），一个包由多个帧组成，包面向的是“连接”，帧面向的是“流”。
  - QUIC 的帧里有多种类型，PING、ACK 等帧用于管理连接，而 STREAM 帧专门用来实现流。
  - QUIC 使用不透明的“连接 ID”来标记通信的两个端点，客户端和服务器可以自行选择一组 ID 来标记自己，这样就解除了 TCP 里连接对“IP 地址 + 端口”（即常说的四元组）的强绑定，支持“连接迁移”
- HTTP/3 协议
  - 因为 QUIC 本身就已经支持了加密、流和多路复用，所以 HTTP/3 的工作减轻了很多，把流控制都交给 QUIC 去做。调用的不再是 TLS 的安全接口，也不是 Socket API，而是专门的 QUIC 函数。不过这个“QUIC 函数”还没有形成标准，必须要绑定到某一个具体的实现库。
  - HTTP/3 里仍然使用流来发送“请求 - 响应”，但它自身不需要像 HTTP/2 那样再去定义流，而是直接使用 QUIC 的流，相当于做了一个“概念映射”。
  - 由于流管理被“下放”到了 QUIC，所以 HTTP/3 里帧的结构也变简单了。
    - 帧头只有两个字段：类型和长度
    - HTTP/3 里的帧仍然分成数据帧和控制帧两类，HEADERS 帧和 DATA 帧传输数据，但其他一些帧因为在下层的 QUIC 里有了替代，所以在 HTTP/3 里就都消失了，比如RST_STREAM、WINDOW_UPDATE、PING 等。
    - 头部压缩算法在 HTTP/3 里升级成了“QPACK”，使用方式上也做了改变。虽然也分成静态表和动态表，但在流上发送 HEADERS 帧时不能更新字段，只能引用，索引表的更新需要在专门的单向流上发送指令来管理，解决了 HPACK 的“队头阻塞”问题。





## 协议三要素

- 语法
  - 这一段内容要符合一定的规则和格式
- 语义
  - 这一段内容要代表某种意义
- 顺序
  - 先干啥，后干啥





## MTU1500

- 最大传输单元（ Maximum Transmission Unit）

- MTU是二层MAC层的概念。MAC层有MAC的头，以太网规定MAC头带正文合起来，不允许超过1500个字节。MTU指的是IP头+IP数据部分长度

- MTU 大小是不包含二层头部和尾部的，MTU 1500表示二层MAC帧大小不超过1518. MAC 头14 字节，尾4字节。可以抓包验证

- 为什么标准以太网帧长度上限为1518字节

  - IP头total length为两个byte，理论上IP packet可以有65535 byte，加上Ethernet Frame头和尾，可以有65535 +14 + 4 = 65553 byte。如果在10Mbps以太网上，将会占用共享链路长达50ms，这将严重影响其它主机的通信，特别是对延迟敏感的应用是无法接受的。
  - 由于线路质量差而引起的丢包，发生在大包的概率也比小包概率大得多，所以大包在丢包率较高的线路上不是一个好的选择。
  - 但是如果选择一个比较小的长度，传输效率又不高，拿TCP应用来说，如果选择以太网长度为218byte，TCP payload = 218 - Ethernet Header - IP Header - TCP Header = 218 - 18 - 20 - 20 = 160 byte
  - 那有效传输效率= 160 / 218 = 73%
  - 而如果以太网长度为1518，那有效传输效率= 1460 / 1518 = 96%
  - 通过比较，选择较大的帧长度，有效传输效率更高，而更大的帧长度同时也会造成上述的问题，于是最终选择一个折衷的长度：1518 byte ! 对应的IP packet 就是 1500 byte，这就是最大传输单元MTU的由来。

  



## 无类型域间选路（CIDR）

- 32位的IP地址一分为二，前面是网络号，后面是主机号

  - > 10.100.122.2/24
    > 	后面24的意思是，32位中，前24位是网络号，后8位是主机号。





## 动态主机配置协议（DHCP）

- 他只需要配置一段共享的IP地址。每一台新接入的机器都通DHCP协议，来这个共享的IP地址里申请，然后自动配置好就可以了。
  - 如果是数据中心里面的服务器，IP一旦配置好，基本不会变，这就相当于买房自己装修。DHCP的方式就相当于租房。你不用装修，都是帮你配置好的。你暂时用一下，用完退租就可以了。





## 预启动执行环境（PXE）

- 网络管理员不仅能自动分配IP地址，还能帮你自动安装操作系统！
- 所以管理员希望的不仅仅是自动分配IP地址，还要自动安装系统。装好系统之后自动分配IP地址，直接启动就能用了，这样当然最好了！
  - 首先，启动BIOS。这是一个特别小的小系统，只能干特别小的一件事情。其实就是读取硬盘的MBR启动扇区，将GRUB启动起来；然后将权力交给GRUB，GRUB加载内核、加载作为根文件系统的initramfs文件；然后将权力交给内核；最后内核启动，初始化整个操作系统。





## 数据链路层

- ARP协议，也就是已知IP地址，求MAC地址的协议。
- 交换机怎么知道每个口的电脑的MAC地址呢？这需要交换机会学习。
  - 一台MAC1电脑将一个包发送给另一台MAC2电脑，当这个包到达交换机的时候，一开始交换机也不知道MAC2的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。
  - 交换机会记住，MAC1是来自一个明确的口。以后有包的目的地址是MAC1的，直接发送到这个口就可以了。
  - 当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的IP地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为转发表，是有一个过期时间的。
- 广播风暴
  - ARP广播时，交换机会将一个端口收到的包转发到其它所有的端口上
  - 比如数据包经过交换机A到达交换机B，交换机B又将包复制为多份广播出去。如果整个局域网存在一个环路，使得数据包又重新回到了最开始的交换机A，这个包又会被A再次复制多份广播出去。如此循环，数据包会不停得转发，而且越来越多，最终占满带宽，或者使解析协议的硬件过载，行成广播风暴。





## 交换机与 VLAN

- STP协议

  - 最小生成树，计算机网络中，生成树的算法叫作STP

  - Priority Vector，优先级向量。可以比喻为实力 （值越小越牛）

    - > [Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]
      >
      > ​	老大的ID
      > ​	我距离我的老大的距离
      > ​	我自己的ID

- 如何解决广播问题和安全问题？

  - VLAN，或者叫虚拟局域网
    - 使用VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？
    - 我们只需要在原来的二层的头上加一个TAG，里面有一个VLAN ID，一共12位。为什么是12位呢？因为12位可以划分4096个VLAN。
    - 只有相同VLAN的包，才会互相转发，不同VLAN的包，是看不到的。这样广播问题和安全问题就都能够解决了。
  - 交换机之间怎么连接呢
    - 对于支持VLAN的交换机，有一种口叫作Trunk口。可以允许多个VLAN通过,可以接收和发送多个VLAN 报文。交换机之间可以通过这种口相互连接。

  





## ICMP与ping

- ICMP协议的格式
  - ping是基于ICMP协议工作的。ICMP，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？
  - ICMP报文是封装在IP包里面的。
- 查询报文类型
  - 常用的ping就是查询报文，是一种主动请求，并且获得主动应答的ICMP协议。
- 差错报文类型
  - 举几个ICMP差错报文的例子：终点不可达为3，源抑制为4，超时为11，重定向为5







## 网关

- 在任何一台机器上，当要访问另一个IP地址的时候，都会先判断，这个目标IP地址，和当前机器的IP地址，是否在同一个网段。怎么判断同一个网段呢？需要CIDR和子网掩码
  - 如果不是同一网段，这就需要发往默认网关Gateway
  - 网关往往是一个路由器，是一个三层转发的设备
    - 把MAC头和IP都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。
    - 很多情况下，人们把网关就叫作路由器。其实不完全准确，而另一种比喻更加恰当：路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的IP地址都和局域网的IP地址相同的网段，每只手都是它握住的那个局域网的网关。
    - 任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下MAC头和IP头，看看，根据自己的路由算法，选择另一只手，加上IP头和MAC头，然后扔出去。
- 静态路由，其实就是在路由器上，配置一条一条规则。
- IP头和MAC头哪些变、哪些不变？
  - MAC地址是一个局域网内才有效的地址，MAC地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于IP地址是否改变。不改变IP地址的网关，我们称为转发网关；改变IP地址的网关，我们称为NAT网关
  - 现在大家每家都有家用路由器，家里的网段都是192.168.1.x，所以你肯定访问不了你邻居家的这个私网的IP地址的。所以，当我们家里的包发出去的时候，都被家用路由器NAT成为了运营商的地址了。







## 路由协议

- 如何配置路由

  - 路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为路由表。

  - > 一张路由表中会有多条路由规则。每一条规则至少包含这三项信息
    > 	目的网络
    > 	出口设备
    > 	下一跳网关
    >
    > 
    >
    > 通过route命令和ip route命令都可以进行查询或者配置
    > 	ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0
    > 	10.176.48.0/20这个目标网络，要从eth0端口出去，经过10.173.32.1。

- 如何配置策略路由

  - 除了可以根据目的ip地址配置路由外，还可以根据多个参数来配置路由，这就称为策略路由。

    - > ip rule add from 192.168.1.0/24 table 10 
      >
      > ip rule add from 192.168.2.0/24 table 20
      >
      > 表示从192.168.1.10/24这个网段来的，使用table 10中的路由表
      >
      > 而从192.168.2.0/24网段来的，使table20的路由表。
      >
      > 
      >
      > ip route add default via 60.190.27.189 dev eth3 table chao

    - > 在一条路由规则中，也可以走多条路径
      >
      > ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2
      >
      > 下一跳有两个地方，分别是100.100.100.1和200.200.200.1，权重分别为1比2。

- 动态路由算法

  - 距离矢量路由算法
    - 这种算法的基本思路是，每个路由器都保存一个路由表，包含多行，每行对应网络中的一个路由器，每一行包含两部分信息，一个是要到目标路由器，从那条线出去，另一个是到目标路由器的距离。
    - 每个路由器都知道自己和邻居之间的距离，每过几秒，每个路由器都将自己所知的到达所有的路由器的距离告知邻居，每个路由器也能从邻居那里得到相似的信息。
    - 第一个问题就是好消息传得快，坏消息传得慢。
    - 第二个问题是，每次发送的时候，要发送整个全局路由表
  - 链路状态路由算法
    - 当一个路由器启动的时候，首先是发现邻居，向邻居say hello，邻居都回复。然后计算和邻居的距离，发送一个echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用Dijkstra算法，找到两点之间的最短路径。
    - 不像距离距离矢量路由协议那样，更新时发送整个路由表。链路状态路由协议只广播更新的或改变的网络拓扑，这使得更新信息更小，节省了带宽和CPU利用率。而且一旦一个路由器挂了，它的邻居都会广播这个消息，可以使得坏消息迅速收敛。

- 动态路由协议

  - 基于距离矢量路由算法的BGP
    - 我们称为外网路由协议（BGP）
    - 每个自治系统都有边界路由器，通过它和外面的世界建立联系。
  - 基于链路状态路由算法的OSPF(开放式最短路径优先)
    - 主要用在数据中心内部，用于路由决策，因而称为内部网关协议（IGP）。
    - 有了等价路由，到一个地方去可以有相同的两个路线，可以分摊流量，还可以当一条路不通的时候，走另外一条路
      - 一般应用的接入层会有负载均衡LVS。它可以和OSPF一起，实现高吞吐量的接入层设计。
  - BGP基于TCP，OSPF基于UDP 







## UDP协议

- TCP和UDP有哪些区别？

  - TCP是面向连接的，UDP是面向无连接的
    - 所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。
  - TCP提供可靠交付,通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。
    - UDP继承了IP的特性，不保证不丢失，不保证按顺序到达。
  - TCP是面向字节流的。
    - 而UDP继承了IP的特性，基于数据报的，一个一个地发，一个一个地收。
  - TCP是可以有拥塞控制的
- MAC层定义了本地局域网的传输行为，IP层定义了整个网络端到端的传输行为
- 当我们看到UDP包头的时候，有源端口号和目标端口号,UDP除了端口号，再没有其他的了
- UDP的三大使用场景

  - 需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用
    - DHCP就是基于UDP协议的
  - 不需要一对一沟通，建立连接，而是可以广播的应用
    - UDP的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP就是一种广播的形式，就是基于UDP协议的。
  - 需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候。（当前很多应用都是要求低时延的）
- 基于UDP的“城会玩”的例子

  - 网页或者APP的访问

    - 目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。
    - QUIC（全称Quick UDP Internet Connections，快速UDP互联网连接）

  - 流媒体的协议

    - 直播

  - 实时游戏

    - 游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。

  - IoT物联网

    - 物联网领域终端资源少
    - 物联网对实时性要求也很高
- UDP 常用于以下几个方面： 1.包总量较少的通信（DNS、SNMP等）； 2.视频、音频等多媒体通信（即时通信）； 3.限定于 LAN 等特定网络中的应用通信； 4.广播通信（广播、多播）。
  - 而且以现在的应用场景，UDP协议一般用作短消息的传输，或者对结果的完整度没有太高要求的情况，比如，音频、视频等普通数据，即使丢几个包，对结果的影响也不会太大，但是UDP对网络质量要求较高，尤其是处理大文件数据时，大面积的丢包会使文件直接损坏，根本无法使用 
- **TCP和UDP可以同时监听相同的端口吗**
  - 使用netstat -an自己看看就知道了，IP数据包首部有个叫做协议的字段，指出了上层协议是TCP还是UDP还是其他P。
    协议字段（报头检验和前面那个），其值为6，则为TCP；
    其值为17，则为UDP。
  - Stack overflow:许多协议已经做到了这一点，例如DNS可在udp / 53和tcp / 53上运行。

## TCP协议

- TCP包头格式

  - 源端口号和目标端口号

  - 序号。解决乱序的问题

  - 确认序号。发出去的包应该有确认

  - 状态位。SYN是发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接等

  - 窗口大小。TCP要做流量控制

  - > 顺序问题 ，稳重不乱；
    > 丢包问题，承诺靠谱；
    > 连接维护，有始有终；
    > 流量控制，把握分寸；
    > 拥塞控制，知进知退。

- TCP的三次握手

  - 进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立TCP连接，并同步连接双方的序列号和确认号，交换TCP窗口大小信息。
  - TCP初始序列号为什么是随机的
    - 在TCP的三次握手中，采用随机产生的初始化序列号进行请求，这样做主要是出于网络安全的因素着想。如果不是随机产生初始序列号，黑客将会以很容易的方式获取到你与其他主机之间通信的初始化序列号，并且伪造序列号进行攻击，这已经成为一种很常见的网络攻击手段。
  - 第三次握手的原因
    - 避免已经失效的连接请求报文段占用服务器的连接资源。
    - 假如A发送的连接请求报文段并没有丢失，而是因为在某些网络节点长时间滞留，在收到A发送的这个已失效的请求报文段之后，没有第三次握手的确认
    - 造成一种假象：B认为和A的连接已经建立，一直等待A发送数据，而A认为自己没有发送请求连接，不理睬B的确认。
  - 过程
    - A：SYN=1 seq=x
    - B：SYN=1 ACK=1 seq=y ack=x+1
    - A：ACK=1 seq=x+1 ack=y+1
  - 过程
  - 一开始，客户端和服务端都处于CLOSED状态。先是服务端主动监听某个端口，处于LISTEN状态。
    - 客户端主动发起连接SYN，之后处于SYN-SENT状态。
    - 服务端收到发起的连接，返回SYN,ACK客户端的SYN，之后处于SYN-RCVD状态
    - 客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态，因为它一发一收成功了
    - 服务端收到ACK的ACK之后，处于ESTABLISHED状态，因为它也一发一收了。

- 状态

  > LISTEN - 侦听来自远方TCP端口的连接请求； 
  >  SYN-SENT -在发送连接请求后等待匹配的连接请求； 
  >  SYN-RECEIVED - 在收到和发送一个连接请求后等待对连接请求的确认； 
  >  ESTABLISHED- 代表一个打开的连接，数据可以传送给用户； 
  >  FIN-WAIT-1 - 等待远程TCP的连接中断请求，或先前的连接中断请求的确认；
  >  FIN-WAIT-2 - 从远程TCP等待连接中断请求；
  >  CLOSE-WAIT - 等待从本地用户发来的连接中断请求； 
  >  CLOSING -等待远程TCP对连接中断的确认； 
  >  LAST-ACK - 等待原来发向远程TCP的连接中断请求的确认； 
  >  TIME-WAIT -等待足够的时间以确保远程TCP接收到连接中断请求的确认； 
  >  CLOSED - 没有任何连接状态；

  

- TCP四次挥手

  - 因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，“你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。

  - 过程
    - A：FIN=1 seq=u
    - B：ACK=1 seq=v ack=u+1
    - B：FIN=1 ACK=1 seq=w ack=u+1
    - A：ACK=1 seq=u+1 ack=w+1
    
  - 过程
    - 刚开始双方都处于 establised 状态，假如是客户端先发起关闭请求，则：
    
      1、第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于**CLOSED_WAIT1**状态。
    
      2、第二次握手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 + 1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 **CLOSE_WAIT2**状态。
    
      3、第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 **LAST_ACK** 的状态。
    
      4、第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 + 1 作为自己 ACK 报文的序列号值，此时客户端处于 **TIME_WAIT** 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态
    
      5、服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。
    
    - 按说A可以跑路了，但是最后的这个ACK万一B收不到呢？
      - 则B会重新发一个“B不玩了”，这个时候A已经跑路了的话，B就再也收不到ACK了，因而TCP协议要求A最后等待一段时间TIME_WAIT
      - 这个时间要足够长，长到如果B没收到ACK的话，“B说不玩了”会重发的，A会重新发一个ACK并且足够时间到达B。
      - A直接跑路还有一个问题是，A的端口就直接空出来了，但是B不知道，B原来发过的很多包很可能还在路上，如果A的端口被一个新的应用占用了，这个新的应用会收到上个连接中B发过来的包
      - B超过了2MSL的时间，依然没有收到它发的FIN的ACK，怎么办呢？按照TCP的原理，B当然还会重发FIN，这个时候A再收到这个包之后，A就表示，我已经在这里等了这么长时间了，已经仁至义尽了，之后的我就都不认了，于是就直接发送RST，B就知道A早就跑了。
        - RFC 793 中虽然指出了 TCP 连接需要在 `TIME_WAIT` 中**等待 2 倍的 MSL**，但是并没有解释清楚这里的两倍是从何而来，比较合理的解释是 — 网络中可能存在来自发起方的数据段，当这些发起方的数据段被服务端处理后又会向客户端发送响应，所以一来一回需要等待 2 倍的时间[5](https://draveness.me/whys-the-design-tcp-time-wait/#fn:5)。
        - **RFC 793 文档将 MSL 的时间设置为 120 秒，即两分钟**，然而这并不是一个经过严密推断的数值，而是工程上的选择，如果根据服务历史上的经验要求我们改变操作系统的设置，也是没有任何问题的；实际上，较早版本的 Linux 就开始将 `TIME_WAIT` 的等待时间 [`TCP_TIMEWAIT_LEN`](https://github.com/torvalds/linux/blob/bd2463ac7d7ec51d432f23bf0e893fb371a908cd/include/net/tcp.h#L121) 设置成 60 秒，以便更快地复用 TCP 连接资源
    
  - **TCP 的 `TIME_WAIT` 状态有着非常重要的作用，它是保证 TCP 协议可靠性不可缺失的设计**

  - **`TIME_WAIT` 只在主动断开连接的一方出现**，被动断开连接的一方会直接进入 `CLOSED` 状态，进入 `TIME_WAIT` 的客户端需要等待 2 MSL 才可以真正关闭连接。**TCP 协议需要 `TIME_WAIT` 状态的原因和客户端需要等待两个 MSL 不能直接进入 `CLOSED` 状态的原因是一样的**：

    - 防止延迟的数据段被其他使用相同源地址、源端口、目的地址以及目的端口的 TCP 连接收到；

    - 保证 TCP 连接的远程被正确关闭，即等待被动关闭连接的一方收到 `FIN` 对应的 `ACK` 消息；

      - 从 RFC 793 对 `TIME_WAIT` 状态的定义中，我们可以发现该状态的另一个重要作用，等待足够长的时间以确定远程的 TCP 连接接收到了其发出的终止连接消息 `FIN` 对应的 `ACK`
      - 如果客户端等待的时间不够长，当服务端还没有收到 `ACK` 消息时，客户端就重新与服务端建立 TCP 连接就会造成以下问题 — 服务端因为没有收到 `ACK` 消息，所以仍然认为当前连接是合法的，客户端重新发送 `SYN` 消息请求握手时会收到服务端的 `RST` 消息，连接建立的过程就会被终止

    - **TIME-WAIT 较短导致的握手终止**

      - 在默认情况下，**如果客户端等待足够长的时间就会遇到以下两种情况**：

      1. 服务端正常收到了 `ACK` 消息并关闭当前 TCP 连接；
      2. 服务端没有收到 `ACK` 消息，重新发送 `FIN` 关闭连接并等待新的 `ACK` 消息；

      - **只要客户端等待 2 MSL 的时间，客户端和服务端之间的连接就会正常关闭，新创建的 TCP 连接收到影响的概率也微乎其微，保证了数据传输的可靠性。**
      - **TCP 的 `TIME_WAIT` 状态有着非常重要的作用，它是保证 TCP 协议可靠性不可缺失的设计**，如果能通过加机器解决的话就尽量加机器，如果不能解决的话，我们就需要理解其背后的设计原理并尽可能避免修改默认的配置，就像 Linux 手册中说的一样，在修改这些配置时应该咨询技术专家的建议；在这里，我们再重新回顾一下 TCP 协议中 `TIME_WAIT` 状态存在的原因，**如果客户端等待的时间不够长，那么使用相同端口号重新与远程建立连接时会造成以下问题**：
        - 因为数据段的网络传输时间不确定，所以可能会收到上一次 TCP 连接中未被收到的数据段；
        - 因为客户端发出的 `ACK` 可能还没有被服务端接收，服务端可能还处于 `LAST_ACK` 状态，所以它会回复 `RST` 消息终止新连接的建立；

- 顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。

  - 时间必须大于往返时间RTT，否则会引起不必要的重传
    - 超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送
  - 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？
    - 有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。
    - 例如，接收方发现6、8、9都已经接收了，就是7没来，那肯定是丢了，于是发送三个6的ACK，要求下一个是7。客户端收到3个，就会发现7的确又丢了，不等超时，马上重发。
  - 还有一种方式称为Selective Acknowledgment （SACK）。这种方式需要在TCP头里加一个SACK的东西，可以将缓存的地图发送给发送方。例如可以发送ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是7丢了

- tcp如何实现可靠性传输

  - 确认机制、重传机制、滑动窗口
  
- 流量控制问题

  - 滑动窗口
  - 发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。

- 拥塞控制问题

  - 拥塞控制的问题，也是通过窗口的大小来控制的
  - 滑动窗口rwnd是怕发送方把接收方缓存塞满，而拥塞窗口cwnd，是怕把网络塞满。
    - TCP发送包常被比喻为往一个水管里面灌水，而TCP的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽。
  - TCP的拥塞控制主要来避免两种现象，包丢失和超时重传
    - 一旦出现了这些现象就说明，发送速度太快了
  - 慢启动、指数增长、超过sshresh后线性增长、拥塞后这个时候，将sshresh设为cwnd/2，将cwnd设为1，重新开始慢启动。
    - 快速重传算法：cwnd减半为cwnd/2，然后sshthresh = cwnd，线性增长
  - TCP的拥塞控制主要来避免的两个现象都是有问题的。
    - 第一个问题是丢包并不代表着通道满了，也可能是管子本来就漏水
    - 第二个问题是TCP的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实TCP只要填满管道就可以了，不应该接着填，直到连缓存也填满。

- TCP BBR拥塞算法

  - 企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。
  - 设备缓存会导致延时
    - 假如经过设备的包都不需要进入缓存，那么得到的速度是最快的。进入缓存且等待，等待的时间就是额外的延时。BBR就是为了避免这些问题。
    - 充分利用带宽；降低buffer占用率。
  - 快速下降后重新慢启动，整个过程对于带宽来说是浪费的。
  - BBR如何解决延时
    - S1：慢启动开始时，以前期的延迟时间为延迟最小值Tmin。然后监控延迟值是否达到Tmin的n倍
      - 达到这个阀值后，判断带宽已经消耗尽并且使用了一定的缓存，进入排空阶段。
    - S2：指数降低发送速率，直至延迟不再降低
    - S3：协议进入稳定运行状态。交替探测带宽和延迟，且大多数时间下都处于带宽探测阶段。
  - BBR是如何探测最大带宽和最小延时呢？首先有一点就是最大带宽和最小延时是无法同时得到的。
    - 探测最大带宽的方法就是尽量多发数据，把网络中的buffer占满，带宽在一段时间内不会增加，这样可以得到此时的最大带宽。
    - 探测最小RTT的方法就是尽量把buffer腾空，让数据交付延时尽量低。
    - 由此，BBR就引入了基于不同探测阶段的状态机。

- **顺序问题、丢包问题、流量控制都是通过滑动窗口来解决的**

- **拥塞控制是通过拥塞窗口来解决的，**相当于往管道里面倒水，快了容易溢出，慢了浪费带宽，要摸着石头过河，找到最优值。

- 应用场景

  - ![img](https://pic4.zhimg.com/v2-06bdd52997add27938607b33edea4068_b.jpg)

- TCP 适合金融等大多数领域，UDP适合游戏和娱乐场景

  当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP（如视频传输、实时通信等）。


- **如果服务端的机器挂了，而客户端去连接会发生什么？如果是服务端的进程挂了，客户端去连接会发生什么？**

  	- 如果是机器挂了，说明操作系统也没了，那么客户端在建立连接的时候，只会发送第一次握手的包，当然收不到，然后重试几次，就不试了。
  - 如果是进程挂了，那么操作系统还在，操作系统会代替进程向客户端发送一个RESET的包，此时客户端就知道通信的那个进程没了，直接关闭连接。
  
- 长连接与短连接

   **TCP 本身并没有长短连接的区别** ，长短与否，完全取决于我们怎么用它。

   - 短连接：每次通信时，创建 Socket；一次通信结束，调用 socket.close()。这就是一般意义上的短连接，短连接的好处是管理起来比较简单，存在的连接都是可用的连接，不需要额外的控制手段。
   - 长连接：每次通信完毕后，不会关闭连接，这样可以做到连接的复用。 **长连接的好处是省去了创建连接的耗时。**

   短连接和长连接的优势，分别是对方的劣势。想要图简单，不追求高性能，使用短连接合适，这样我们就不需要操心连接状态的管理；想要追求性能，使用长连接，我们就需要担心各种问题：比如 **端对端连接的维护，连接的保活** 。

   长连接还常常被用来做数据的推送，我们大多数时候对通信的认知还是 request/response 模型，但 TCP 双工通信的性质决定了它还可以被用来做双向通信。在长连接之下，可以很方便的实现 push 模型。

- 连接的保活

   这个话题就有的聊了，会牵扯到比较多的知识点。首先需要明确一点，为什么需要连接的保活？当双方已经建立了连接，但因为网络问题，链路不通，这样长连接就不能使用了。需要明确的一点是，通过 netstat，lsof 等指令查看到连接的状态处于 `ESTABLISHED` 状态并不是一件非常靠谱的事，因为连接可能已死，但没有被系统感知到，更不用提假死这种疑难杂症了。如果保证长连接可用是一件技术活。


   - 连接的保活：KeepAlive

      首先想到的是 TCP 中的 KeepAlive 机制。KeepAlive 并不是 TCP 协议的一部分，但是大多数操作系统都实现了这个机制（所以需要在操作系统层面设置 KeepAlive 的相关参数）。KeepAlive 机制开启后，在一定时间内（一般时间为 7200s，参数 `tcp_keepalive_time`）在链路上没有数据传送的情况下，TCP 层将发送相应的 KeepAlive 探针以确定连接可用性，探测失败后重试 10（参数 `tcp_keepalive_probes`）次，每次间隔时间 75s（参数 `tcp_keepalive_intvl`），所有探测失败后，才认为当前连接已经不可用。

      在 Netty 中开启 KeepAlive：

      ```
      bootstrap.option(ChannelOption.SO_KEEPALIVE, true)
      ```

      Linux 操作系统中设置 KeepAlive 相关参数，修改 `/etc/sysctl.conf` 文件：

      ```
      net.ipv4.tcp_keepalive_time=90
      net.ipv4.tcp_keepalive_intvl=15
      net.ipv4.tcp_keepalive_probes=2
      ```

      **KeepAlive 机制是在网络层面保证了连接的可用性** ，但站在应用框架层面我们认为这还不够。主要体现在三个方面：

      - KeepAlive 的开关是在应用层开启的，但是具体参数（如重试测试，重试间隔时间）的设置却是操作系统级别的，位于操作系统的 `/etc/sysctl.conf` 配置中，这对于应用来说不够灵活。
      - KeepAlive 的保活机制只在链路空闲的情况下才会起到作用，假如此时有数据发送，且物理链路已经不通，操作系统这边的链路状态还是 `ESTABLISHED`，这时会发生什么？自然会走 TCP 重传机制，要知道默认的 TCP 超时重传，指数退避算法也是一个相当长的过程。
      - KeepAlive 本身是面向网络的，并不面向于应用，当连接不可用，可能是由于应用本身的 GC 频繁，系统 load 高等情况，但网络仍然是通的，此时，应用已经失去了活性，连接应该被认为是不可用的。

      我们已经为应用层面的连接保活做了足够的铺垫，下面就来一起看看，怎么在应用层做连接保活。

   - 连接的保活：应用层心跳

      终于点题了，文题中提到的 **心跳** 便是一个本文想要重点强调的另一个重要的知识点。上一节我们已经解释过了，网络层面的 KeepAlive 不足以支撑应用级别的连接可用性，本节就来聊聊应用层的心跳机制是实现连接保活的。

      如何理解应用层的心跳？简单来说，就是客户端会开启一个定时任务，定时对已经建立连接的对端应用发送请求（这里的请求是特殊的心跳请求），服务端则需要特殊处理该请求，返回响应。如果心跳持续多次没有收到响应，客户端会认为连接不可用，主动断开连接。不同的服务治理框架对心跳，建连，断连，拉黑的机制有不同的策略，但大多数的服务治理框架都会在应用层做心跳，Dubbo/HSF 也不例外。

   - 应用层心跳的设计细节

      以 Dubbo 为例，支持应用层的心跳，客户端和服务端都会开启一个 `HeartBeatTask`，客户端在 `HeaderExchangeClient` 中开启，服务端将在 `HeaderExchangeServer` 开启。文章开头埋了一个坑：Dubbo 为什么在服务端同时维护 `Map<String,Channel>` 呢？主要就是为了给心跳做贡献，心跳定时任务在发现连接不可用时，会根据当前是客户端还是服务端走不同的分支，客户端发现不可用，是重连；服务端发现不可用，是直接 close。

   - 注意和 HTTP 的 KeepAlive 区别对待

      - HTTP 协议的 KeepAlive 意图在于连接复用，同一个连接上串行方式传递请求 - 响应数据
      - TCP 的 KeepAlive 机制意图在于保活、心跳，检测连接错误。

   


## 套接字 Socket

- 基于TCP协议的Socket程序函数调用过程
  - TCP的服务端要先监听一个端口，一般是先调用bind函数
  - 在内核中，为每个Socket维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于established状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于syn_rcvd的状态。
  - 当服务端有了IP和端口号，就可以调用listen函数进行监听
  - 接下来，服务端调用accept函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。
  - 在服务端等待的时候，客户端可以通过connect函数发起连接，内核会给客户端分配一个临时的端口。一旦握手成功，服务端的accept就会返回另一个Socket。
    - 监听的 Socket 和真正用来传数据的 Socket 是两个，一个叫作监听 Socket，一个叫作已连接 Socket。
  - 连接建立成功之后，双方开始通过read和write函数来读写数据
  - ![img](https://static001.geekbang.org/resource/image/77/92/77d5eeb659d5347874bda5e8f711f692.jpg)
  - TCP 的 Socket 就是一个文件流，是非常准确的。因为，Socket 在 Linux 中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。
    - 在内核中，Socket 是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构 task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。
    - 这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个 inode，只不过 Socket 对应的 inode 不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个 inode 中，指向了 Socket 在内核中的 Socket 结构。
    - 在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存 sk_buff。这个缓存里面能够看到完整的包的结构
- 基于UDP协议的Socket程序函数调用过程
  - UDP是没有连接的，所以不需要三次握手，也就不需要调用listen和connect，但是，UDP的的交互仍然需要IP和端口号，因而也需要bind。
  - UDP是没有维护连接状态的，因而不需要每对连接建立一组Socket，而是只要有一个Socket，就能够和多个客户端通信
  - 每次通信的时候，都调用sendto和recvfrom，都可以传入IP地址和端口。
  - ![img](https://static001.geekbang.org/resource/image/77/ef/778687d1a02ffc0c24078c33be2ac1ef.jpg)
- 手写socket
  - http://www.52im.net/thread-1722-1-1.html
- 服务端最大并发 TCP 连接数远不能达到理论上限。首先主要是文件描述符限制，按照上面的原理，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目；另一个限制是内存，按上面的数据结构，每个 TCP 连接都要占用一定内存，操作系统是有限的。
  - 多进程方式
    - 这就相当于你是一个代理，在那里监听来的请求。一旦建立了一个连接，就会有一个已连接 Socket，这时候你可以创建一个子进程，然后将基于已连接 Socket 的交互交给这个新的子进程来做。
    - 在 Linux 下，创建子进程使用 fork 函数。通过名字可以看出，这是在父进程的基础上完全拷贝一个子进程。在 Linux 内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到了哪一行程序的进程。显然，复制的时候在调用 fork，复制完毕之后，父进程和子进程都会记录当前刚刚执行完 fork。这两个进程刚复制完的时候，几乎一模一样，只是根据 fork 的返回值来区分到底是父进程，还是子进程。如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。
    - 因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为 accept 创建的已连接 Socket 也是一个文件描述符，同样也会被子进程获得。
    - 接下来，子进程就可以通过这个已连接 Socket 和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？还记得 fork 返回的时候，如果是整数就是父进程吗？这个整数就是子进程的 ID，父进程可以通过这个 ID 查看子进程是否完成项目，是否需要退出。
  - 多线程方式
    - 在 Linux 下，通过 pthread_create 创建一个线程，也是调用 do_fork。不同的是，虽然新的线程在 task 列表会新创建一项，但是很多资源，例如文件描述符列表、进程空间，还是共享的，只不过多了一个引用而已。
    - 新的线程也可以通过已连接 Socket 处理请求，从而达到并发处理的目的。上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程。一台机器无法创建很多进程或者线程。
    - 有个 C10K，它的意思是一台机器要维护 1 万个连接，就要创建 1 万个进程或者线程，那么操作系统是无法承受的。如果维持 1 亿用户在线需要 10 万台服务器，成本也太高了。
  - IO 多路复用：一个线程维护多个 Socket
    - 由于 Socket 是文件描述符，因而某个线程盯的所有的 Socket，都放在一个文件描述符集合 fd_set 中，这就是项目进度墙，然后调用 select 函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在 fd_set 对应的位都设为 1，表示 Socket 可读或者可写，从而可以进行读写操作，然后再调用 select，接着盯着下一轮的变化。
  - IO 多路复用：从“派人盯着”到“有事通知“
    - 上面 select 函数还是有问题的，因为每次 Socket 所在的文件描述符集合中有 Socket 发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用 select，能够同时盯的项目数量由 FD_SETSIZE 限制。
    - 如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。能完成这件事情的函数叫 epoll，它在内核中的实现不是通过轮询的方式，而是通过注册 callback 函数的方式，当某个文件描述符发送变化的时候，就会主动通知。
    - 假设进程打开了 Socket m, n, x 等多个文件描述符，现在需要通过 epoll 来监听是否这些 Socket 都有事件发生。其中 epoll_create 创建一个 epoll 对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个 epoll 要监听的所有 Socket。
    - 当 epoll_ctl 添加一个 Socket 的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的 Socket 的事件列表中。当一个 Socket 来了一个事件的时候，可以从这个列表中得到 epoll 对象，并调用 call back 通知它。
    - 这种通知方式使得监听的 Socket 数据增加的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，epoll 被称为解决 C10K 问题的利器。















# servlet



## 简介

- 处理请求和发送响应的过程是由一种叫做Servlet的程序来完成的，并且Servlet是为了解决实现动态页面而衍生的东西



## tomcat和servlet的关系

- Tomcat 是Web应用服务器,是一个Servlet/JSP容器，处理客户请求,把请求传送给Servlet,并将Servlet的响应传送回给客户
  - Tomcat将http请求文本接收并解析，然后封装成HttpServletRequest类型的request对象
  - Tomcat同时会要响应的信息封装为HttpServletResponse类型的response对象
- Servlet是一种运行在支持Java语言的服务器上的组件. Servlet最常见的用途是扩展Java Web服务器功能





## servlet重要对象

- 对象
  - ServletConfig对象
  - ServletContext对象
    - 功能：tomcat为每个web项目都创建一个ServletContext实例，tomcat在启动时创建，服务器关闭时销毁
  - request对象
  - response对象





## 过滤器监听器

- Servlet、过滤器、监听器实例化对象的优先级和销毁的优先级
  - 创建（初始化）:  （ServletContext） 监听器–>过滤器–>Servlet. 
  - 调用时候：过滤器–>监听器–>Servlet. 
  - 销亡：Servlet–>过滤器–>监听器.





## Servlet3.0

- Servlet 线程不再是一直处于阻塞状态以等待业务逻辑的处理，而是启动异步线程之后可以立即返回

  - Servlet 3.0 在 <servlet>和 <filter> 标签中增加了 <async-supported> 子标签
  - @WebServlet 和 @WebFilter 进行 Servlet 或 Filter 配置的情况，这两个注解都提供了 asyncSupported 属性

- ```java
  //1.获得异步上下文对象
    		AsyncContext ac = request.startAsync();
    		//2.启动一个耗时的子线程
    		ThreadTask tt = new ThreadTask(ac);
    		//3.可设置异步超时对象，需在启动异步上下文对象前设置
    		/*
    		 * 设置超时后，在超时时间内子线程没有结束，主线程则会停止等待，继续往下执行
    		 */
    		ac.setTimeout(3000);
    		//4.开启异步上下文对象
    		ac.start(tt);
    
  //进行异步的一些处理
          HttpServletRequest requst = (HttpServletRequest) ac.getRequest();
          HttpSession session = requst.getSession();
  
  //通知主线程已经处理完成
  			/* 
  			 * 除了使用 ac.complete() 方法通知主线程已经处理外
  			 * 还可以使用 ac.dispatch() 方法重定向到一个页面
  			 */
          ac.dispatch("/show.jsp");
  
  ```

- servlet 3.0 还为异步处理提供了一个监听器，使用 AsyncListener 接口表示

  - 异步线程开始时，调用 AsyncListener 的 onStartAsync(AsyncEvent event) 方法；
  - 异步线程出错时，调用 AsyncListener 的 onError(AsyncEvent event) 方法；
  - 异步线程执行超时，则调用 AsyncListener 的 onTimeout(AsyncEvent event) 方法；
  - 异步执行完毕时，调用 AsyncListener 的 onComplete(AsyncEvent event) 方法。
  - 如果要注册一个 AsyncListener，只需将准备好的 AsyncListener 对象传递给 AsyncContext 对象的 addListener() 方法即可





# Network





## ARP

（ARP地址解析协议,  RARP反向地址解析协议），我们习惯上把它们认为是链路层的协议，实际上，从分层的角度来看，更准确的说是，（应该是一种介于网络IP层与链路层之间的一种协议）

我们知道在ISO/OSI模型中，数据在传输的过程中，有不断封装过程，到了链路层的话(以太网传输)，在以太网的帧格式中会出现目的主机的MAC地址，但是我们从一开始就只知道目的主机的IP地址，所以这里用到了ARP协议

源主机先在自己的ARP缓冲区中寻找映射，如果有（直接填充于以太网帧中），如果没有，通过路由广播请求，这时一些联网的主机就会收到这个请求，并将这个请求传回网络层，对比IP地址，检验是否可以接受，如果不行，则直接丢失这个信息，如果可以那么回复ARP请求，并且将源主机的MAC地址加入到目的ARP缓冲区中，形成映射，源主机接受到请求后，将目的的MAC地址加入到ARP缓冲区，也形成映射，并将mac地址传输至连接层。此时转化完成

![img](https://img-blog.csdn.net/20160219211855357?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**什么是ARP欺骗**

举个栗子：

| 主机 |     IP      |        MAC        |
| :--: | :---------: | :---------------: |
|  A   | 192.168.1.1 | 0A-11-22-33-44-01 |
|  B   | 192.168.1.2 | 0A-11-22-33-44-02 |
|  C   | 192.168.1.3 | 0A-11-22-33-44-03 |

> 1.主机A要和主机C通信，主机A发出ARP包询问谁是192.168.1.3?请回复192.168.1.1
>
> 2.这时主机B在疯狂的向主机A回复，我是192.168.1.3，我的地址是0A-11-22-33-44-02
>
> 3.由于ARP协议不会验证回复者的身份，造成主机A错误的将192.168.1.3的MAC映射为0A-11-22-33-44-02。

**ARP欺骗的分类**

> 1.主机欺骗，如同上面的栗子，主机B欺骗局域网中的主机A。
>
> 2.网关欺骗，局域网中的主机欺骗网关，从而获取其他主机的进流量。

**如何防御ARP欺骗**

ARP欺骗是通过重复应答实现的，那么只需要在本机添加一条静态的ARP映射，这样就不需要询问网关MAC地址了，这种方法只对主机欺骗有效。对于网关欺骗还需要在网关中也添加一条到主机的静态ARP映射。1.用管理身份运行命令提示符；输入netsh i i show in，查看一下本机有哪些网络连接

```
netsh i i show in
```

2.查看一下网关的MAC地址。注意如果正遭受ARP欺骗攻击，通过此方法查处的可能是虚假的MAC地址。输入arp -a命令查询本机的arp映射表，如果找不到网关的信息，可以先ping一下网关。

3.输入：netsh -c "i i" add neighbors 连接的Idx号 网关IP 网关MAC 添加一条静态映射,我已经添加过了，所以会显示 对象已存在

```
netsh -c "i i" add neighbors 连接的Idx号 网关IP 网关MAC
netsh -c "i i" add neighbors 9 10.60.12.1 4c-5e-0c-64-73-f5
```



## DNS

以访问知乎([http://www.zhihu.com](http://www.zhihu.com/))为例：

第一步：通过你的电脑系统的浏览器去访问[http://www.zhihu.com](http://www.zhihu.com/)，首先会去检査本机的 Hosts是否有URL映射关系，如果有，则进行映射完成域名解析！

第二步：如果没有，就从本地的DNS缓存里找映射关系，如果有，则进行映射完成解析！

第三步：如果前两者都没有找到，则将该请求发送到你的路由器（记住：路由器也有缓存)，此时你的ISP会询问根域名服务器（1.1.1.1 ): “我想访问一个.com后缀的域名”

第四步：这个时候根域名服务器上会找到.com域名服务器（比如你的域名注册商上NameCheap)，则会告诉你：试试（2.2.2.2)，接下来会通过（2.2.2.2)进一步在域名服务器上寻找[http://www.zhihu.com](http://www.zhihu.com/)，当找到域名服务器后，就到达DNS解析服务。比如例子里的域名服务商是（NameCheap)的，但是你不一定会使用NameCheap自带的DNS服务。比如图解里你的DNS可以使用其它的DNS服务商：Amazon S3、DNSMadeEasy、 NSOne 的

第五步：找到对应域名的DNS服务商后，即可最终返回给到你的电脑浏览器上，正常打开知乎（[http://www.zhihu.com](http://www.zhihu.com/))。

![img](https://pic2.zhimg.com/80/v2-ecef93d5119289285a42c295b8f0bac9_1440w.jpg?source=1940ef5c)

- 层次树状结构的联机分布式数据库系统
  - 产生于应用层上的域名系统 NDS就可以用来把互联网上的主机名转换成 IP 地址
  - 把待解析的域名放在 DNS 的请求报中，以 UDP 用户数据报方式发送给本地域名服务器。本地域名服务器在查找域名后，把对应的 IP 地址放在回答报文中返回。获得 IP 地址的后主机即可进行通信
- 域名解析过程
  - 域名解析过程
    - 本地域名服务器向根域名服务器的查询方式通常采取迭代查询
  - 递归查询
    - 主机向本地域名服务器的查询一般都采用递归查询



**DNS劫持 vs HTTP劫持**

开始正式介绍DNS劫持之前，先与HTTP劫持做一个比较，可能有助于有些同学对下文更容易理解更深入一点。

DNS劫持现象：你输入一个google.com网址，出来的是百度的页面

HTTP劫持现象：访问着github的页面，右下角出现了一个格格不入的广告弹窗

在dns解析过程中，有哪一环节出现问题的话，都可能会导致DNS解析错误，导致客户端（浏览器）得到一个假的ip地址，从而引导用户访问到这个冒名顶替，恶意的网站。

**下面大概说几种DNS劫持方法**

**1.本机DNS劫持**

攻击者通过某些手段使用户的计算机感染上木马病毒，或者恶意软件之后，恶意修改本地DNS配置，比如修改本地hosts文件，缓存等

**2. 路由DNS劫持**

很多用户默认路由器的默认密码，攻击者可以侵入到路由管理员账号中，修改路由器的默认配置

**3.攻击DNS服务器**

直接攻击DNS服务器，例如对DNS服务器进行DDOS攻击，可以是DNS服务器宕机，出现异常请求，还可以利用某些手段感染dns服务器的缓存，使给用户返回来的是恶意的ip地址

**2.DNS的防范**

> 就这上面的劫持方法，说几种方法手段

1.加强本地计算机病毒检查，开启防火墙等，防止恶意软件，木马病毒感染计算机

2.改变路由器默认密码，防止攻击者修改路由器的DNS配置指向恶意的DNS服务器

3.企业的话可以准备两个以上的域名，一旦一个域名挂掉，还可以使用另一个

4.用HTTP DNS 代替 Local DNS

> 对于DNS劫持，往往单靠个人设置很难解决，如果已经出现了劫持现象的话，对电脑进去杀毒，清理，检查hosts文件，核查网络设置的DNS配置（可以使用写公共的DNS服务器





## CDN

- 内容分发网络，解决的是如何将数据快速可靠从源站传递到用户

- 数据从服务器端交付到用户端，至少有4个地方可能会造成网络拥堵

  - 网站服务器接入互联网的链路
  - 用户接入互联网的链路
  - ISP互联，即因特网服务提供商之间的互联
  - 长距离传输时延问题

- 基本过程

  - 用户在浏览器中输入要访问的域名。 

  2. 浏览器向DNS服务器请求对域名进行解析。由于CDN对域名解析进行了调整，DNS服务器会最终将域名的解析权交给CDN专用DNS服务器。 
  3. CDN的DNS服务器将CDN的负载均衡设备IP地址返回给用户。 
  4. 用户向CDN的负载均衡设备发起内容URL访问请求。 

  - CDN负载均衡设备会为用户选择一台合适的缓存服务器提供服务。 
    5. 选择的依据包括：根据用户IP地址，判断哪一台服务器距离用户最近；根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需内容；查询各个服务器的负载情况，判断哪一台服务器的负载较小。 

  5. 用户向缓存服务器发出请求。
  6. 缓存服务器响应用户请求，将用户所需内容传送到用户。

7. CDN的工作原理：通过权威DNS服务器来实现最优节点的选择，通过缓存来减少源站的压力





## HTTP

- HTTP幂等性
  - 幂等性是数学中的一个概念，表达的是N次变换与1次变换的结果相同
  - HTTP POST和PUT，二者均可用于创建资源，更为本质的差别是在幂等性方面
    - POST所对应的URI并非创建的资源本身，而是资源的接收者
      - 两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性
    - PUT所对应的URI是要创建或更新的资源本身
      - 对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性
- HTTP协议的瓶颈及其优化技巧都是基于TCP协议本身的特性
  - 三次握手有1.5个RTT（round-trip time）的延迟
  - 不同策略的http长链接方案
    - HTTP的长连接和短连接本质上是TCP长连接和短连接
  - TCP在建立连接的初期有慢启动
- http和socket长连接和短连接区别
  - Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口
  - 门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议
- HTTP2.0
  - HTTP1.x有以下几个主要缺点：
    - 一次只允许在一个TCP连接上发起一个请求
    - HTTP/1.1使用的流水线技术也只能部分处理请求并发，仍然会存在队列头阻塞问题，因此客户端在需要发起多次请求时，通常会采用建立多连接来减少延迟。
    - 单向请求，只能由客户端发起。
    - 请求报文与响应报文首部信息冗余量大。
    - 数据未压缩，导致数据的传输量大。
  - 多路复用 (Multiplexing)
    - 所谓多路复用，即在一个TCP连接中存在多个流
    - HTTP/2 通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。在过去， HTTP 性能优化的关键并不在于高带宽，而是低延迟
  - 二进制分帧
    - 很容易的去实现多流并行而不用依赖建立多个 TCP 连接，HTTP/2 把 HTTP 协议通信的基本单位缩小为一个一个的帧
    - HTTP2.0中，有两个概念非常重要：帧（frame）和流（stream）。
      帧是最小的数据单位，每个帧会标识出该帧属于哪个流，流是多个帧组成的数据流。
  - 首部压缩（Header Compression）
  - 服务端推送（Server Push）
- 哈希算法
  - 将任意长度的信息转换为较短的固定长度的值，通常其长度要比信息小得多，且算法不可逆。







## GET/POST

- 区别
  - GET请求在URL中传送的参数是有长度限制的，而POST没有。
    - 对于一个字节流的解析，必须分配buffer来保存所有要存储的数据。而URL这种东西必须当作一个整体看待，无法一块一块处理，于是就处理一个URL请求时必须分配一整块足够大的内存
  - 最直观的区别就是GET把参数包含在URL中，POST通过request body传递参数
  - GET和POST本质上没有区别
    - HTTP是基于TCP/IP的关于数据如何在万维网中如何通信的协议
    - HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本
    - GET和POST的底层也是TCP/IP，GET/POST都是TCP链接
    - GET产生一个TCP数据包；POST产生两个TCP数据包
  - 从攻击的角度，无论是GET还是POST都不够安全
    - HTTP本身是明文协议。每个HTTP请求和返回的每个byte都会在网络上传播，不管是url，header还是body
- ElasticSearch的_search接口使用GET，却用body来表达查询，因为查询很复杂





## RestTemplate

- RestTemplate能大幅简化了提交表单数据的难度，并且附带了自动转换JSON数据的功能

  | HTTP method | RestTemplate methods |
  | :---------- | :------------------- |
  | DELETE      | delete               |
  | GET         | getForObject         |
  |             | getForEntity         |
  | HEAD        | headForHeaders       |
  | OPTIONS     | optionsForAllow      |
  | POST        | postForLocation      |
  |             | postForObject        |
  | PUT         | put                  |
  | any         | exchange             |
  |             | execute              |

- 手动指定转换器(HttpMessageConverter)

  - 调用reseful接口传递的数据内容是json格式的字符串，返回的响应也是json格式的字符串
  - restTemplate.postForObject方法的请求参数RequestBean和返回参数ResponseBean都是java类。是RestTemplate通过HttpMessageConverter自动帮我们做了转换的操作
    - StringHttpMessageConverter来处text/plain;
    - MappingJackson2HttpMessageConverter来处理application/json;
    - MappingJackson2XmlHttpMessageConverter来处理application/xml

- RestTemplate直接使用一个HttpClient作为底层实现

- 设置拦截器(ClientHttpRequestInterceptor)

  - ```java
    // 1.实现ClientHttpRequestInterceptor接口
    
    
    RestTemplate restTemplate = new RestTemplate();
    // 2.向restTemplate中添加自定义的拦截器
    restTemplate.getInterceptors().add(new TokenInterceptor());
    ```

- getForObject()其实比getForEntity()多包含了将HTTP转成POJO的功能，但是getForObject没有处理response的能力。因为它拿到手的就是成型的pojo。省略了很多response的信息

- postForEntity

  - ```java
    // httpEntity
    HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(map, headers);
    ResponseEntity<String> response = restTemplate.postForEntity( url, request , String.class );
    
    // MultiValueMap是Map的一个子类，它的一个key可以存储多个value
    
    // 为什么用MultiValueMap?因为HttpEntity接受的request类型是它
    
    // 为什么用HttpEntity是因为restTemplate.postForEntity方法虽然表面上接收的request是@Nullable Object request类型，但是你追踪下去会发现这个request是用HttpEntity来解析
    ```

- 使用exchange指定调用方式

  ```java
  HttpEntity<String> entity = new HttpEntity<>(jsonObj.toString(), headers);
  ResponseEntity<JSONObject> exchange = restTemplate.exchange(url,HttpMethod.GET, entity, JSONObject.class);
  ```







# Session&Cookie



## 概念

- HTTP协议是无状态的协议。一旦数据交换完毕，客户端与服务器端的连接就会关闭，再次交换数据需要建立新的连接。这就意味着服务器无法从连接上跟踪会话
  - 会话，指用户登录网站后的一系列动作
  - 常用的会话跟踪技术 是Cookie与Session
  - Cookie通过在客户端记录信息确定用户身份，Session通过在服务器端记录信息确定用户身份





## Cookie

- cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围
  - 如果我们想让 www.china.com能够访问bbs.china.com设置的cookies，该怎么办? 我们可以把domain属性设置成“china.com”，并把path属性设置成“/”
- Cookie具有不可跨域名性



## Session

- 用户与服务器建立连接的同时，服务器会自动为其分配一个SessionId
- HttpSession是Servlet三大域对象之一（request、session、application（ServletContext））
- 禁用cookie
  - URL重写，就是把sessionId直接附加在URL路径的后面
    - 使用Response.encodeURL则可以直接得到路径+jsessionid的全部url路径，不需要自己手动拼接字符串了。然后将这个url返回给客户端，用户通过一个链接点击(通过refresh来刷新，再次访问本页面效果如下图)
  - 表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器





## JWT

- 轻量级的认证规范，这个规范允许我们使用JWT在用户和服务器之间传递安全可靠的信息
- 整个 jwt 串会被置于 http 的 Header 或者 url 中
- 在 jwt 中以.分割三个部分
  - 一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名
  - jwt的第三部分是一个签名信息，这个签名信息由三部分组成：	
    - header (base64后的)
    - payload (base64后的)
    - secret	
    - 通过header中声明的加密方式进行加密
- jwt token泄露了怎么办
  - https
  - 返回 jwt 给客户端时设置 httpOnly=true 并且使用 cookie 而不是 LocalStorage 存储 jwt，这样可以防止 XSS 攻击和 CSRF 攻击
- 我们使用JWT的初衷就在于，我们可以不用通过读取状态来得知请求者的一些信息，因为JWT中自带了一些不敏感的信息，比如用户Id，权限列表，而且JWT不可伪造，所以我们直接通过解析JWT就能够知道一些原本需要从数据库/缓存里读取的数据。
  
  - 如果每一个请求到来的时候都要去读取状态并检测这个JWT是否有效，那么就和使用JWT的初衷相违背了，这是自相矛盾的。如果我都要去读取数据库和缓存了，那我为什么还要用JWT呢？为什么不直接给一个随机的字符串ID（比如SessionId），然后每次请求到来的时候通过这个Id取出当前的请求者信息不就完了？
- jwt 的特性天然不支持续签
  
  - 因为 payload 是参与签名的，一旦过期时间被修改，整个 jwt 串就变了
- jwt 不仅仅是作为身份认证，还在其 payload 中存储着会话信息，这是 jwt 和 session 的最大区别
- 什么场景该适合使用jwt？
  - 一次性验证，用户注册后需要发一封邮件让其激活账户
  - 单点登录系统
  - restful api 的无状态认证
    - JWT是自我校验的，所以是无状态的。JWT在客户端是不能做任何操作的，只有客户端发送请求时附带token，然后由服务端解析JWT后做自我校验
    - 服务端无需存储jwt令牌，通过特定的算法和密钥校验token，同时取出Payload中携带的用户ID，减少不必要的数据库查询

- 用户认证

  - 所谓用户认证（Authentication），就是让用户登录，并且在接下来的一段时间内让用户访问网站时可以使用其账户，而不需要再次登录的机制。

    > 小知识：可别把用户认证和用户授权（Authorization）搞混了。用户授权指的是规定并允许用户使用自己的权限，例如发布帖子、管理站点等。

    首先，服务器应用（下面简称“应用”）让用户通过 Web 表单将自己的用户名和密码发送到服务器的接口。这一过程一般是一个 HTTP POST 请求。建议的方式是通过 SSL 加密的传输（https 协议），从而避免敏感信息被嗅探。

    [![auth1](https://kirito.iocoder.cn/jwtauth1.png)](https://kirito.iocoder.cn/jwtauth1.png)auth1

    接下来，应用和数据库核对用户名和密码。

    [![auth2](https://kirito.iocoder.cn/jwtauth2.png)](https://kirito.iocoder.cn/jwtauth2.png)auth2

    核对用户名和密码成功后，应用将用户的 `id`（图中的 `user_id`）作为 JWT Payload 的一个属性，将其与头部分别进行 Base64 编码拼接后签名，形成一个 JWT。这里的 JWT 就是一个形同 `lll.zzz.xxx` 的字符串。

    [![auth3](https://kirito.iocoder.cn/jwtauth3.png)](https://kirito.iocoder.cn/jwtauth3.png)auth3

    应用将 JWT 字符串作为该请求 Cookie 的一部分返回给用户。注意，在这里必须使用 `HttpOnly` 属性来防止 Cookie 被 JavaScript 读取，从而避免 [跨站脚本攻击（XSS 攻击）](http://www.cnblogs.com/bangerlee/archive/2013/04/06/3002142.html)。

    [![auth4](https://kirito.iocoder.cn/jwtauth4.png)](https://kirito.iocoder.cn/jwtauth4.png)auth4

    在 Cookie 失效或者被删除前，用户每次访问应用，应用都会接受到含有 `jwt` 的 Cookie。从而应用就可以将 JWT 从请求中提取出来。

    [![auth5](https://kirito.iocoder.cn/jwtauth5.png)](https://kirito.iocoder.cn/jwtauth5.png)auth5

    应用通过一系列任务检查 JWT 的有效性。例如，检查签名是否正确；检查 Token 是否过期；检查 Token 的接收方是否是自己（可选）。

    [![auth6](https://kirito.iocoder.cn/jwtauth6.png)](https://kirito.iocoder.cn/jwtauth6.png)auth6

    应用在确认 JWT 有效之后，JWT 进行 Base64 解码（可能在上一步中已经完成），然后在 Payload 中读取用户的 id 值，也就是 `user_id` 属性。这里用户的 `id` 为 1025。

    应用从数据库取到 `id` 为 1025 的用户的信息，加载到内存中，进行 ORM 之类的一系列底层逻辑初始化。

    [![auth7](https://kirito.iocoder.cn/jwtauth7.png)](https://kirito.iocoder.cn/jwtauth7.png)auth7

    应用根据用户请求进行响应。

    [![auth8](https://kirito.iocoder.cn/jwtauth8.png)](https://kirito.iocoder.cn/jwtauth8.png)auth8

    ### 和 Session 方式存储 id 的差异

    Session 方式存储用户 id 的最大弊病在于要占用大量服务器内存，对于较大型应用而言可能还要保存许多的状态。一般而言，大型应用还需要借助一些 KV 数据库和一系列缓存机制来实现 Session 的存储。

    而 JWT 方式将用户状态分散到了客户端中，可以明显减轻服务端的内存压力。除了用户 id 之外，还可以存储其他的和用户相关的信息，例如该用户是否是管理员、用户所在的分桶（见 [《你所应该知道的 A/B 测试基础》一文] 等。

    虽说 JWT 方式让服务器有一些计算压力（例如加密、编码和解码），但是这些压力相比磁盘 I/O 而言或许是半斤八两。具体是否采用，需要在不同场景下用数据说话。

    ### 单点登录

    Session 方式来存储用户 id，一开始用户的 Session 只会存储在一台服务器上。对于有多个子域名的站点，每个子域名至少会对应一台不同的服务器，例如：

    - [www.taobao.com](http://www.taobao.com/)
    - nv.taobao.com
    - nz.taobao.com
    - login.taobao.com

    所以如果要实现在 `login.taobao.com` 登录后，在其他的子域名下依然可以取到 Session，这要求我们在多台服务器上同步 Session。

    使用 JWT 的方式则没有这个问题的存在，因为用户的状态已经被传送到了客户端。因此，我们只需要将含有 JWT 的 Cookie 的 `domain` 设置为顶级域名即可，例如

    ```
    Set-Cookie: jwt=lll.zzz.xxx; HttpOnly; max-age=980000; domain=.taobao.com
    ```

    注意 `domain` 必须设置为一个点加顶级域名，即 `.taobao.com`。这样，taobao.com 和 *.taobao.com 就都可以接受到这个 Cookie，并获取 JWT 了。



## jwt session

- http 无状态，所以为了实现有状态 http，才有了会话（session）的概念。
  - 会话(Session)是一个客户与服务器之间的不中断的请求响应序列。对客户的每个请求，服务器能够识别出请求来自于同一个客户。当一个未知的客户向Web应用程序发送第一个请求时就开始了一个会话。当客户明确结束会话或服务器在一个预定义的时限内不从客户接受任何请求时，会话就结束了。当会话结束后，服务器就忘记了客户以及客户的请求。
- 不管是“session id”，还是所谓“token”(如 jwt)，其实都是会话的一种实现方式。形式上“session id”和“token”都是“字符串”，这个“字符串”可以是任意的编码，本质上都是 credential（会话凭证）。
- 在各种 session 方案中，你会发现实现细节都不一样，flask session, django session, spring session, jwt 等等。但万变不离其宗， credential 的客户端保存方式，credential 的传输方式和会话信息的保存方式，都只是这几个流程的细节有改变而已，本质都是为了实现有状态的 http。

## OAuth2.0是什么

#### OAuth2.0是什么——豆瓣和QQ的故事

OAuth简单说就是一种授权的**协议**，只要授权方和被授权方遵守这个协议去写代码提供服务，那双方就是实现了OAuth模式。

举个例子，你想登录豆瓣去看看电影评论，但你丫的从来没注册过豆瓣账号，又不想新注册一个再使用豆瓣，怎么办呢？不用担心，豆瓣已经为你这种懒人做了准备，用你的qq号可以授权给豆瓣进行登录，请看。

**第一步：在豆瓣官网点击用qq登录**

**![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824150221230-104373567.png)**

**第二步：跳转到qq登录页面输入用户名密码，然后点授权并登录**

 ![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824151117089-994331290.png)

**第三步：跳回到豆瓣页面，成功登录**

![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824151220418-2113396046.png)

 这几秒钟之内发生的事情，在**无知的用户视角**看来，就是在豆瓣官网上输了个qq号和密码就登录成功了。在一些**细心的用户视角**看来，页面经历了从豆瓣到qq，再从qq到豆瓣的两次页面跳转。但作为一群专业的程序员，我们还应该从**上帝视角**来看这个过程。

#### OAuth2.0是什么——上帝视角

　　简单来说，上述例子中的豆瓣就是**客户端**，QQ就是**认证服务器**，OAuth2.0就是客户端和认证服务器之间由于相互**不信任**而产生的一个**授权协议**。呵呵，要是相互信任那QQ直接把自己数据库给豆瓣好了，你直接在豆瓣输入qq账号密码查下数据库验证就登陆呗，还跳来跳去的多麻烦。

　　先上一张图，该图描绘了只几秒钟发生的所有事情用**上帝视角**来看的流程

![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824142737402-1297004164.png)

 就这这张图，来说一下上述例子中的三个步骤在图中的表现。所用到的请求路径名称都是虚构的，所附带的请求参数忽略了一些非重点的。

如想了解每次的请求和响应的标准齐全的参数，还是去读那篇阮一峰的文章。http://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html

**第一步：在豆瓣官网点击用qq登录**

　　当你点击用qq登录的小图标时，实际上是向豆瓣的服务器发起了一个 http://www.douban.com/leadToAuthorize 的请求，豆瓣服务器会响应一个**重定向地址**，指向qq授权登录

　　浏览器接到重定向地址 http://www.qq.com/authorize?callback=www.douban.com/callback ，再次访问。并注意到这次访问带了一个参数是callback，以便qq那边授权成功再次让浏览器发起这个callback请求。不然qq怎么知道你让我授权后要返回那个页面啊，每天让我授权的像豆瓣这样的网站这么多。

　　至于访问这个地址之后，qq那边做出怎样的回应，就是第二步的事情了。总之第一步即对应了图中的这些部分。

![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824155817777-2073704717.png)

**第二步：跳转到qq登录页面输入用户名密码，然后点授权并登录**

　　上一步中浏览器接到重定向地址并访问 http://www.qq.com/authorize?callback=www.douban.com/callback

　　qq的服务器接受到了豆瓣访问的authorize，在次例中所给出的回应是跳转到qq的登录页面，用户输入账号密码点击授权并登录按钮后，一定还会访问qq服务器中校验用户名密码的方法，若校验成功，该方法会响应浏览器一个重定向地址，并附上一个**code（授权码）**。由于豆瓣只关心像qq发起authorize请求后会返回一个code，并不关心qq是如何校验用户的，并且这个过程每个授权服务器可能会做些个性化的处理，只要最终的结果是返回给浏览器一个重定向并附上code即可，所以这个过程在图中并没有详细展开。现把展开图画给大家。

![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824161339668-1419889465.png)

**第三步：跳回到豆瓣页面，成功登录**

 这一步背后的过程其实是最繁琐的，但对于用户来说是完全感知不到的。用户在QQ登录页面点击授权登陆后，就直接跳转到豆瓣首页了，但其实经历了很多隐藏的过程。

首先接上一步，QQ服务器在判断登录成功后，使页面重定向到之前豆瓣发来的callback并附上code授权码，即 callback=www.douban.com/callback 

页面接到重定向，发起 http://www.douban.com/callback 请求

豆瓣服务器收到请求后，做了两件再次与QQ沟通的事，即模拟浏览器发起了两次请求。一个是用拿到的code去换token，另一个就是用拿到的token换取用户信息。最后将用户信息储存起来，返回给浏览器其首页的视图。到此OAuth2.0授权结束。

![img](https://images2017.cnblogs.com/blog/1096103/201708/1096103-20170824162606793-113527222.png)



## 微服务设计中的API网关模式

根据 Gartner 对微服务的定义：“微服务是范围狭窄、封装紧密、松散耦合、可独立部署且可独立伸缩的应用程序组件。”

与将模块高度耦合并部署为一个大的应用程序相比，微服务的目标是将应用程序充分分解或者解耦为松散耦合的许多微服务或者模块，这样做对下面几点有很大帮助：

- 每个微服务都可以独立于应用程序中的同级服务进行部署、升级、扩展、维护和重新启动。
- 通过自治的跨职能团队进行敏捷开发和敏捷部署。
- 运用技术时具备灵活性和可扩展性

在微服务架构中，我们根据各自的特定需求部署不同的松耦合服务，其中每个服务都有其更细粒度的 API 模型，用以服务于不同的客户端（Web，移动和第三方 API）。

1客户端到微服务的连接

![img](https://oscimg.oschina.net/oscnet/044c6408-bbc4-4856-92dc-89fb49f70d03.png)

在考虑客户端与每个已部署的微服务 **直接通信** 的问题时，应考虑以下挑战：

1. 如果微服务向客户端公开了细粒度的 API，则客户端应向每个微服务发出请求。在典型的单页中，可能需要进行 **多次服务器往返**，才能满足请求。对于较差的网络条件下运行的设备（例如移动设备），这可能会更糟。
2. 微服务中存在的 **多种通信协议**（例如 gRpc、thrift、REST、AMQP 等）使客户端很难轻松采用所有这些协议。
3. 必须在每个微服务中实现 **通用网关功能**（例如身份验证、授权、日志记录）。
4. 在不中断客户端连接的情况下，**很难在微服务中进行更改**。例如，在合并或划分微服务时，可能需要重新编写客户端部分代码。

2API 网关

为了解决上述挑战，人们引入了一个附加层，该附加层位于客户端和服务器之间，充当从客户端到服务器的反向代理路由请求。与面向对象设计的模式相似，它为封装底层系统架构的 API 提供了一个单一的入口，称为 API 网关。

简而言之，它的行为就像 API 管理员一样，但重要的是不要将 API 管理与 API Gateway 混为一谈。

![img](https://oscimg.oschina.net/oscnet/3810bf35-ea8d-40ac-ad63-9388734530e3.png)

3API 网关的功能

路由

网关封装了底层系统并与客户端分离，为客户端提供了与微服务系统进行通信的单个入口点。

整合

API 网关整合了一些边缘的重复功能，无需让每个微服务都实现它们。它包括如下功能：

- 认证和授权
- 服务发现集成
- 缓存响应结果
- 重试策略、熔断器、QoS
- 限速和节流
- 负载均衡
- log 日志、链路追踪、关联
- Header、query 字符串 以及 claims 转义
- IP 白名单
- IAM
- 集中式日志管理（服务之间的 transaction ID、错误日志等）
- 身份的提供方，验证与授权

4后端服务前端模式（BFF Backend for Frontend）

它是 API 网关模式的一种变体。它提供了基于客户端的多个网关，而不是提供给客户端一个单一的入口点。目的是根据客户端的需求提供量身定制的 API，从而消除了为所有客户端制作通用 API 造成的大量的浪费。

![img](https://oscimg.oschina.net/oscnet/c18ea4ad-e32e-4f0a-8386-e284bb09926e.png)

到底需要多少 BFF

BFF 的基本概念是为每种用户体验开发利基后端。菲尔·卡尔萨多（PhilCalçado） 的指导建议是“**一种体验，一种 BFF**”。如果跨客户端（IOS 客户端、Android 客户端、Web 浏览器等）的要求有很大差异，并且单个代理或 API 的发布时间有严格要求，则 BFF 是一个很好的解决方案。还应注意，更复杂的设计需要复杂的步骤。

GraphQL 与 BFF

GraphQL 是一种 API 的查询语言。PhilCalçado 提出 BFF 和 GraphQL 的想法是相似的，但不是互斥的概念。他补充说，BFF 与你端口的形状无关，而在于赋予客户端对应用程序的自治权，您可以在其中构建与许多 BFF 或 OSFA（one-size-fits-all）的 GraphQL API。

5著名的 API 网关

Netflix API 网关：Zuul

Netflix 的流媒体服务可在 1000 多种不同类型的设备（电视、机顶盒、智能手机、游戏系统、平板电脑等）上使用，在高峰时段可以每秒处理 50,000 个请求，这种需求是 OSFA （one-size-fits-all）的 REST API 难以满足的，因此他们为每个设备量身定制了 API 网关。

Netflix 的 Zuul 2 是所有进入 Netflix 云基础架构的请求的第一步。Zuul 2 大大改进了架构和功能，使我们的网关能够处理、路由和保护 Netflix 的云系统，并帮助为我们的 1.25 亿会员提供最佳体验。



亚马逊 API 网关

AWS 提供了完备的托管服务，用于创建、发布、维护、监视以及保护 REST、HTTP 和 WebSocket，开发人员可以在其中创建用于访问 AWS 或其他 Web 服务的 API，并将数据存储在 AWS 云上面。



Kong API 网关

Kong Gateway 是一个开源的，轻量级的微服务 API 网关，可提供无与伦比的延迟性能优化和可伸缩性。如果您只需要这些基础能力，那么它就是很合适的选项。只需要增加更多节点就可以轻松横向扩展。它以非常低的延迟来支持大量可变的工作负载。

其他 API 网关

- Apigee API Gateway
- MuleSoft
- Tyk.io
- Akana
- SwaggerHub
- Azure API Gateway
- Express API Gateway
- Karken D

选择正确的网关

评估标准里面，一些常见的指标包括简便性、开源还是专有、可伸缩性和灵活性、安全性、后续功能、社区、管理（支持情况、监控和部署）、环境配置（安装、配置、是否支持托管）、定价和文档等。

6API 组合与聚合

API 网关中的一些 API 请求直接映射到单个服务的 API 上，可以通过将请求路由到相应的微服务来提供服务。但是，在需要从多个微服务获得结果的复杂 API 操作的情况下，可以通过 **API 组合 / 聚合**（分散 - 收集机制）来提供服务。在需要同步通信的情况下，如果服务彼此依赖，则必须遵循链式组合模式。组合层必须支持很大一部分的 ESB / 集成功能，例如转换、编排、弹性和稳定性模式。

根容器的部署必须配备特殊的分发器和聚合器功能（或微服务）。分发者负责分解成细粒度的任务，并将这些任务分发给微服务实例。聚合器负责聚合业务工作流从组合微服务中得出的结果。

API 网关和聚合

具备复杂功能的网关会增大测试和部署的难度。强烈建议大家避免在 API 网关中进行聚合和数据转换。领域专属的功能更应该遵循软件开发实践的定义，在应用程序的代码中完成。Netflix API Gateway Zuul 2 从他们在 Zuul 到原始系统的网关中，删除了许多业务逻辑。 

![img](https://oscimg.oschina.net/oscnet/3ad4e5c6-a6c4-454f-a754-f5b041a23f45.png)

Service Mesh 与 API 网关

微服务中的 Service Mesh 是处理进程间通信的可配置网络基础结构层。这和通常称为 Sidecar 代理或 Sidecar 网关的东西很像。它提供了许多功能，例如：

- 负载均衡
- 服务发现
- 健康检查
- 安全性

从表面上看，**API 网关和 Service Mesh 似乎解决了相同的问题，因此好像是多余的。它们确实解决了相同的问题，但是应用在不同的场景**。API 网关被部署为业务解决方案的一部分，被外部的服务发现，处理纵向的流量（面对外部客户端），但是，Service Mesh 是用来处理横向流量（在不同的微服务之间）。

实现 Service Mesh 可避免在您自己的代码中出现一些弹性交互，例如熔断器、服务发现、健康检查以及服务观察。对于少量的微服务，应考虑使用其他替代方法来进行故障管理，因为 Service Mesh 集成可能代价太大了。但对于大量的微服务，它的收益是显著的。

结合这两种技术可能是确保应用程序正常运行时间和弹性伸缩能力的一种有效方法，同时又可以确保您的应用程序易于使用。将两者视为同样的产品是不对的，最好将两者视为在涉及微服务和 API 的部署中相辅相成的工具。

API 网关实现的注意事项：

- 可能产生的单点故障或者瓶颈
- 由于通过 API 网关进行了额外的网络跳转以及复杂性风险，响应时间增长了。





# Web安全





## XSS

- 有很多用户发送了同样类型的内容，而且这些内容都是一个带有诱惑性的问题和一个可以点击的链接。
- 简单来说，XSS 就是利用 Web 漏洞，在用户的浏览器中执行黑客定义的 JavaScript 脚本，这样一种攻击方式。
- 如何进行 XSS 防护？
  - 验证输入 OR 验证输出
  - 编码
  - 检测和过滤
  - CSP
    - CSP（Content Security Policy，内容安
      全策略）来提升 Web 的安全性。所谓 CSP，就是在服务端返回的 HTTP header 里面添加
      一个 Content-Security-Policy 选项，然后定义资源的白名单域名。浏览器就会识别这个字
      段，并限制对非白名单资源的访问。
    - 那我们为什么要限制外域资源的访问呢？这是因为 XSS 通常会受到长度的限制，导致黑客无法提交一段完整的 JavaScript 代码。为了解决这个问题，黑客会采取引用一个外域JavaScript 资源的方式来进行注入。



## SQL注入

- 通常来说，我们会将应用的用户信息存储在数据库中。每次用户登录时，都会执行一个相应的 SQL 语句。这时，黑客会通过构造一些恶意的输入参数，在应用拼接 SQL 语句的时候，去篡改正常的 SQL 语意，从而执行黑客所控制的 SQL 查询功能。这个过程，就相当于黑客“注入”了一段 SQL 代码到应用中。这就是我们常说的 SQL 注入。

- SELECT * FROM Users WHERE Username ="" AND Password ="" or ""=""

- 使用 PreparedStatement

  - 通过合理地使用 PreparedStatement，我们就能够避免 99.99% 的 SQL 注入问题。

  - 当数据库在处理一个 SQL 命令的时候，大致可以分为两个步骤：

    - 将 SQL 语句解析成数据库可使用的指令集。我们在使用 EXPLAIN 关键字分析 SQL 语句，就是干的这个事情；
    - 将变量代入指令集，开始实际执行。之所以在批量处理 SQL 的时候能够提升性能，就是因为这样做避免了重复解析 SQL 的过程。

  - SQL 注入是在解析的过程中生效的，用户的输入会影响 SQL 解析的结果。因此，我们可以通过使用 PreparedStatement，将 SQL 语句的解析和实际执行过程分开，只在执行的过程中代入用户的操作。这样一来，无论黑客提交的参数怎么变化，数据库都不会去执行额外的逻辑，也就避免了 SQL 注入的发生。

  - ```java
    1 String sql = "SELECT * FROM Users WHERE UserId = ?"; 
    2 PreparedStatement statement = connection.prepareStatement(sql); 
    3 statement.setInt(1, userId); 
    4 ResultSet results = statement.executeQuery();
    
    
    // 如果你在使用 PreparedStatement 的时候，还是通过字符串拼接来构造 SQL语句，那仍然是将解析和执行放在了一块，也就不会产生相应的防护效果了。
    
    1 String sql = "SELECT * FROM Users WHERE UserId = " + userId; 
    2 PreparedStatement statement = connection.prepareStatement(sql); 
    3 ResultSet results = statement.executeQuery();
    ```







## CSRF/SSRF

- 在平常使用浏览器访问各种网页的时候，是否遇到过，自己的银行应用突然发起了一笔转账，又或者，你的微博突然发送了一条内容？

- 为了能够准确地代表你的身份，浏览器通常会在 Cookie 中存储一些必要的身份信息。所以，在我们使用一个网页的时候，只需要在首次访问的时候登录就可以了。

  - 黑客正是利用这一点，来编写带有恶意JavaScript 脚本的网页，通过“钓鱼”的方式诱导你访问。然后，黑客会通过这些JavaScript 脚本窃取你保存在网页中的身份信息，通过仿冒你，让你的浏览器发起伪造的请求，最终执行黑客定义的操作。而这一切对于你自己而言都是无感知的。这就是CSRF（Cross-Site Request Forgery，跨站请求伪造）攻击。

- 和 XSS 一样，CSRF 也可以仿冒用户去进行一些功能操作的请求，比如修改密码、转账等等，相当于绕过身份认证，进行未授权的操作。

- 行业内标准的 CSRF 防护方法是CSRFToken

  - CSRF 是通过自动提交表单的形式来发起攻击的。所以，在前面转账的例子中，黑客可以通过抓包分析出 http://bank.com/transfer 这个接口所需要的参数，从而构造对应的 form 表单。因此，我们只需要在这个接口中，加入一个黑客无法猜到的参数，就可以有效防止 CSRF 了。这就是 CSRF Token 的工作原理。
  - 因为 CSRF Token 是每次用户正常访问页面时，服务端随机生成返回给浏览器的。所以，每一次正常的转账接口调用，都会携带不同的 CSRF Token。黑客没有办法进行提前猜测，也就没有办法构造出正确的表单了。

- SSRF：同样的原理，发生在服务端又会发生什么？

  - 我们知道，服务端也有代理请求的功能：用户在浏览器中输入一个 URL（比如某个图片资源），然后服务端会向这个 URL 发起请求，通过访问其他的服务端资源来完成正常的页面展示。
  - 这个时候，只要黑客在输入中提交一个内网 URL，就能让服务端发起一个黑客定义的内网
    请求，从而获取到内网数据。这就是SSRF（Server Side Request Forgery，服务端请求伪造）的原理。而服务端作为内网设备，通常具备很高的权限，所以，这个伪造的请求往往
    因为能绕过大部分的认证和授权机制，而产生很严重的后果。
  - 比方说，当我们在百度中搜索图片时，会涉及图片的跨域加载保护，百度不会直接在页面中加载图片的源地址，而是将地址通过 GET 参数提交到百度服务器，然后百度服务器请求到对应的图片，再返回到页面展示出来。
  - 这个过程中，百度服务器实际上会向另外一个 URL 地址发起请求。利用这个代理发起请求的功能，黑客可以通过提交一个内网的地址，实现对内网任意服务的访问。这就是 SSRF 攻击的实现过程，也就是我们常说的“内网穿透”。

- 因为 SSRF 最终的结果，是接受代理请求的服务端发生数据泄漏。所以，SSRF防护不仅仅涉及接收 URL 的服务端检测，也需要接受代理请求的服务端进行配合。在这种情况下，我们就需要用到请求端限制，它的防护措施主要包括两个方面。

  - 第一，为其他业务提供的服务接口尽量使用 POST，避免 GET 的使用。因为，在 SSRF 中
    （以及大部分的 Web 攻击中），发起一个 POST 请求的难度是远远大于 GET 请求的。
  - 第二，为其他业务提供的服务接口，最好每次都进行验证。通过 SSRF，黑客只能发起请求，并不能获取到服务端存储的验证信息（如认证的 key 和 secret 等）。因此，只要接受代理请求的端对每次请求都进行完整的验证，黑客无法成功通过验证，也就无法完成请求了。

  





















































































